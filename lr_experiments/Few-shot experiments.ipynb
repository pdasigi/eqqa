{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb651c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_datasets import read_json_dataset\n",
    "from dict_utils import unfold_to_list, fold_from_list\n",
    "from pipeline import Pipeline, FewShotPipeline, FineTuningFewShotPipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a026931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./experiments_20220605/data\"\n",
    "TRAIN_FILEPATH = f\"{DATA_DIR}/train.csv\"\n",
    "DEV_DILEPATH = f\"{DATA_DIR}/dev.csv\"\n",
    "TEST_FILEPATH = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "# Read datasets\n",
    "train_df = pd.read_csv(TRAIN_FILEPATH, index_col=0)\n",
    "dev_df = pd.read_csv(DEV_DILEPATH, index_col=0)\n",
    "test_df = pd.read_csv(TEST_FILEPATH, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99bfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64890d5d",
   "metadata": {},
   "source": [
    "Let us create the list of datasets to train and evaluate. For zero-shot leave-one-out experiments, we will create a list of tuples `(train_dataset: 1, finetune: 1, eval: 1+)`, a tuple w/ dimension `n_evals + 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8be6f995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval:\n",
      "- dataset: cosmosqa\n",
      "  filepath: ./experiments_20220605/data/dev.csv\n",
      "- dataset: cosmosqa\n",
      "  filepath: ./experiments_20220605/data/test.csv\n",
      "finetune:\n",
      "  dataset: cosmosqa\n",
      "  filepath: ./experiments_20220605/data/train.csv\n",
      "models:\n",
      "- data_features:\n",
      "  - bleu1\n",
      "  - bleu2\n",
      "  - bleu3\n",
      "  - bleu4\n",
      "  - rougeL\n",
      "  - hf_rouge1\n",
      "  - hf_rouge2\n",
      "  - meteor\n",
      "  - recall\n",
      "  - precision\n",
      "  - f1_score\n",
      "  - sari_context\n",
      "  - sari_question\n",
      "  - precision_at_err1\n",
      "  - recall_at_err1\n",
      "  - tp\n",
      "  - fn\n",
      "  - fp\n",
      "  - char_edit_score\n",
      "  - word_edit_score\n",
      "  - bertscore\n",
      "  - bleurt\n",
      "  - LERC\n",
      "  - candidatelength_word\n",
      "  - candidatelength_char\n",
      "  - candidatenunique_words\n",
      "  - referencelength_word\n",
      "  - referencelength_char\n",
      "  - referencenunique_words\n",
      "  - contextlength_word\n",
      "  - contextlength_char\n",
      "  - contextnunique_words\n",
      "  - questionlength_word\n",
      "  - questionlength_char\n",
      "  - questionnunique_words\n",
      "  data_target: score_scaled\n",
      "  model_classpath: sklearn.linear_model.LinearRegression\n",
      "  model_hyperparams: {}\n",
      "  n_runs: 10\n",
      "  preprocessing_hyperparams:\n",
      "    with_std: true\n",
      "seed: 1123\n",
      "serialization_dir: ./experiments_20220605/results/\n",
      "train:\n",
      "  dataset: cosmosqa\n",
      "  filepath: ./experiments_20220605/data/train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_config = {\n",
    "    \"serialization_dir\": \"./experiments_20220605/results/\",\n",
    "    \"train\": {\n",
    "        \"filepath\": \"./experiments_20220605/data/train.csv\",\n",
    "        \"dataset\": \"cosmosqa\",\n",
    "    },\n",
    "    # optional\n",
    "    \"finetune\": { \n",
    "        \"filepath\": \"./experiments_20220605/data/train.csv\",\n",
    "        \"dataset\": \"cosmosqa\",\n",
    "    },\n",
    "    \"eval\": [\n",
    "      {\"filepath\": \"./experiments_20220605/data/dev.csv\", \"dataset\": \"cosmosqa\"},\n",
    "      {\"filepath\": \"./experiments_20220605/data/test.csv\", \"dataset\": \"cosmosqa\"},\n",
    "    ],\n",
    "    \"models\": [{\n",
    "        # Preprocessing\n",
    "        \"preprocessing_hyperparams\": {\"with_std\": True},\n",
    "        \n",
    "        # Model specific\n",
    "        \"model_classpath\": \"sklearn.linear_model.LinearRegression\", \n",
    "        \"model_hyperparams\": {},\n",
    "        \"n_runs\": 10,\n",
    "        \n",
    "        # Dataset specific\n",
    "        \"data_features\": [\n",
    "            # Bleu\n",
    "            'bleu1', 'bleu2', 'bleu3', 'bleu4', \n",
    "            # 'hf_bleu1', 'hf_bleu2', 'hf_bleu3', 'hf_bleu4', \n",
    "            'rougeL', \n",
    "            # 'hf_rougeL', 'hf_rougeLsum',\n",
    "            'hf_rouge1', 'hf_rouge2',\n",
    "            'meteor',\n",
    "            'recall', 'precision', 'f1_score',\n",
    "            'sari_context', 'sari_question',\n",
    "            # Token overlap when 1st error occurred\n",
    "            'precision_at_err1', 'recall_at_err1',\n",
    "            # Confusion matrix\n",
    "            'tp', 'fn', 'fp',\n",
    "            # Edit scores ------\n",
    "            'char_edit_score',\n",
    "            'word_edit_score',\n",
    "            # Learned metrics -------\n",
    "            'bertscore',\n",
    "            'bleurt',\n",
    "            # \n",
    "            \"LERC\",\n",
    "            # Input statistics ------\n",
    "            'candidatelength_word', 'candidatelength_char',\n",
    "            'candidatenunique_words', 'referencelength_word',\n",
    "            'referencelength_char', 'referencenunique_words',\n",
    "            'contextlength_word', 'contextlength_char',\n",
    "            'contextnunique_words', 'questionlength_word',\n",
    "            'questionlength_char', 'questionnunique_words',\n",
    "        ],\n",
    "        \"data_target\": \"score_scaled\", \n",
    "    }],\n",
    "    \"seed\": 1123,\n",
    "}\n",
    "\n",
    "import yaml\n",
    "print(yaml.dump(experiment_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54943fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_generic import generate_uuid, filter_params_by_prefix, filter_params, import_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acc89728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(filepath: str, dataset: str, dataset_col: str=\"dataset\"):\n",
    "    data = pd.read_csv(filepath, index_col=0)\n",
    "    if dataset == \"*\":\n",
    "        return data\n",
    "    else:\n",
    "        return data[data[dataset_col] == dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12728a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'cosmosqa': ['cosmosqa']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "serialization_dir = experiment_config[\"serialization_dir\"]\n",
    "\n",
    "## EXPERIMENT \n",
    "experiment_uuid = generate_uuid(experiment_config)\n",
    "experiment_name = experiment_config.get(\"name\", \"default\") # TODO - create automatic name\n",
    "\n",
    "seed = experiment_config[\"seed\"]\n",
    "\n",
    "# Unroll the config\n",
    "train_config = experiment_config[\"train\"]\n",
    "eval_configs = experiment_config[\"eval\"]\n",
    "\n",
    "rand = np.random.default_rng(seed)\n",
    "\n",
    "# Read training dataset\n",
    "get_dataset_kwargs = filter_params(train_config, get_dataset)\n",
    "train_df = get_dataset(**get_dataset_kwargs)\n",
    "\n",
    "finetune_config = experiment_config.get(\"finetune\")\n",
    "if finetune_config:\n",
    "    get_dataset_kwargs = filter_params(finetune_config, get_dataset)\n",
    "    ft_df = get_dataset(**get_dataset_kwargs)\n",
    "\n",
    "\n",
    "for config in experiment_config[\"models\"]:\n",
    "    seed = rand.integers(10**6)\n",
    "    n_runs = config[\"n_runs\"]\n",
    "    metadata = filter_params_by_prefix(config, \"data_\", True)\n",
    "    model_config  = filter_params_by_prefix(config, \"model_\", trim=True)\n",
    "    \n",
    "    model_class = import_method(model_config[\"classpath\"])\n",
    "    model_hparams = model_config[\"hyperparams\"].copy()\n",
    "    model_hparams.update(random_state=seed)\n",
    "    model_hparams = filter_params(model_hparams, model_class)\n",
    "    \n",
    "    # TODO - Determine pipeline\n",
    "    features, target = metadata[\"features\"], metadata[\"target\"]\n",
    "    pipeline = Pipeline(model_class, model_hparams, train_config[\"dataset\"], features, target, seed)\n",
    "    \n",
    "    # Data (abstract into a preprocessing method)\n",
    "    pipeline.load_data(train_df.copy())\n",
    "    \n",
    "    # TODO - Add splitting (for downsampling)\n",
    "    if False:\n",
    "        pipeline.split(holdout_fraction=0.2)\n",
    "    \n",
    "    preproc_configs = filter_params_by_prefix(config, \"preprocessing_\", True)\n",
    "    preproc_configs = filter_params(preproc_configs[\"hyperparams\"], pipeline.preprocess)\n",
    "    pipeline.preprocess(**preproc_configs)\n",
    "    \n",
    "    # Fit \n",
    "    pipeline.fit()\n",
    "    \n",
    "    if train_config.get(\"save_predict\"):\n",
    "        y_pred = pipeline.predict(train_df)\n",
    "        y_train_pred = pipeline.evaluate(train_df)\n",
    " \n",
    "        # TODO PERSIST\n",
    "        train_filename = train_config[\"filepath\"].rpartition(\"/\")[-1]\n",
    "        train_dataset = train_config[\"dataset\"]\n",
    "        \n",
    "        train_df[\"preds\"] = y_pred\n",
    "        \n",
    "        \n",
    "    # TODO - Evals:\n",
    "    for eval_config in  experiment_config[\"eval\"]:\n",
    "        dataset = eval_config[\"dataset\"]\n",
    "        eval_df = get_dataset(**eval_config)\n",
    "        \n",
    "        y_test_pred = pipeline.predict(eval_df)\n",
    "        results = pipeline.evaluate(eval_df)\n",
    "        results[\"evaluated_on\"] = dataset\n",
    "        \n",
    "        # TODO - PERSIST\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0535afc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train.csv'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config[\"filepath\"].rpartition(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748e8cf",
   "metadata": {},
   "source": [
    "# Zero shot experiments\n",
    "\n",
    "In this section, we perform the zero-shot experiments for different classes of models, including LinearRegression, RandomForests, and MLP Regressors. We use `sklearn` implementation.\n",
    "\n",
    "\n",
    "We also experiment with different settings, including different subsampling of the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e9a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f0c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8936786",
   "metadata": {},
   "source": [
    "# Few-shot experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8b739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6344c8e8",
   "metadata": {},
   "source": [
    "## Baseline experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "        model_class,\n",
    "        model_hparams,\n",
    "        features,\n",
    "        target,\n",
    "        train_datasets,\n",
    "        split_frac=None,\n",
    "        with_std=True,\n",
    "        with_pca=False,\n",
    "        seed=817237,\n",
    "        pipeline_class=Pipeline,\n",
    "    ) -> dict:\n",
    "    pipelines = {}\n",
    "    \n",
    "    for train_name, train_data in train_datasets.items():\n",
    "        pipeline = pipeline_class(model_class, model_hparams, train_name, features, target, seed=seed)\n",
    "        pipeline.load_data(train_data)\n",
    "        if split_frac and isinstance(split_frac, float):\n",
    "            pipeline.split(holdout_fraction=split_frac)\n",
    "\n",
    "        pipeline.preprocess(with_std=with_std, with_pca=with_pca)\n",
    "        pipeline.fit()\n",
    "        pipelines[train_name] = pipeline\n",
    "\n",
    "    return pipelines\n",
    "\n",
    "\n",
    "def evaluate(pipelines, eval_datasets):\n",
    "    results = []\n",
    "    for train_name, pipeline in pipelines.items():\n",
    "        result = pipeline.evaluate_multiple(eval_datasets)\n",
    "        results.extend(result)\n",
    "    return results\n",
    "\n",
    "def evaluate_loo(pipelines, eval_datasets):\n",
    "    results = []\n",
    "    for train_name, pipeline in pipelines.items():\n",
    "        loo_dataset = train_name.rpartition(\"_\")[-1]\n",
    "        result = pipeline.evaluate_multiple({loo_dataset: eval_datasets[loo_dataset]})\n",
    "        results.extend(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f30803",
   "metadata": {},
   "source": [
    "### Individual metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ca0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndividualMetric:\n",
    "    def __init__(self, feature=None):\n",
    "        self.feature = feature\n",
    "                \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.feature is None:\n",
    "            raise RuntimeError(\"No feature specified\")\n",
    "\n",
    "        y_pred = X[self.feature]\n",
    "        \n",
    "        assert len(y_pred) == X.shape[0]\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce5628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "individual_results = []\n",
    "for metric in [\"LERC\"] + METRICS: \n",
    "    ps = fit(IndividualMetric, {\"feature\": metric}, [metric], TARGET, {\"all\": TRAIN_DATASETS_DUMMY_LERC[\"all\"]}, with_pca=False, with_std=False)\n",
    "    results = evaluate(ps, DEV_DATASETS)\n",
    "    for r in results:\n",
    "        r[\"model_classpath\"] = metric\n",
    "    \n",
    "    individual_results.extend(results)\n",
    "\n",
    "individual_results = pd.DataFrame(individual_results)\n",
    "individual_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbae8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_results.to_csv(\"results/baseline_individual.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f3759",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageBaseline:\n",
    "    def __init__(self, features=None, subset=None):\n",
    "        if features is None or subset is None:\n",
    "            self.features = None\n",
    "            self.subset = None\n",
    "            self.subset_feat_ids = None\n",
    "        else:\n",
    "            self.features = features\n",
    "            self.subset = subset        \n",
    "            self.subset_feat_ids = [i for i, f in enumerate(features) if f in subset]\n",
    "                \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.features is not None:\n",
    "            X = X[:, self.subset_feat_ids]\n",
    "        else:\n",
    "            print(\"Since no features were specified, using all input to make prediction\")\n",
    "\n",
    "        y_pred = np.mean(X, axis=1)\n",
    "        assert len(y_pred) == X.shape[0]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b19646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All datasets experiment\n",
    "ad_avg_all_pipelines = fit(AverageBaseline, {}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "ad_avg_all_results = evaluate(ad_avg_all_pipelines, DEV_DATASETS)\n",
    "ad_avg_all_results = pd.DataFrame(ad_avg_all_results)\n",
    "ad_avg_all_results.to_csv(\"results/baseline_avg_all.csv\")\n",
    "ad_avg_all_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Stronger avgs\n",
    "# (per dataset)\n",
    "# Learn avg in training and predict that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91694d9",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LERC\" in METRICS:\n",
    "    name = \"_w_lerc\"\n",
    "else:\n",
    "    name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All datasets experiment\n",
    "ad_lr_pipelines = fit(LinearRegression, {}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "ad_lr_results = evaluate(ad_lr_pipelines, DEV_DATASETS)\n",
    "ad_lr_results = pd.DataFrame(ad_lr_results)\n",
    "ad_lr_results.to_csv(f\"results/ad_lr{name}.csv\")\n",
    "\n",
    "loo_lr_pipelines = fit(LinearRegression, {}, METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "loo_lr_results = evaluate(loo_lr_pipelines, DEV_DATASETS)\n",
    "loo_lr_results = pd.DataFrame(loo_lr_results)\n",
    "loo_lr_results.to_csv(f\"results/loo_lr{name}.csv\")\n",
    "\n",
    "# All (baseline) LR results\n",
    "lr_results = pd.concat((ad_lr_results, loo_lr_results)).reset_index(drop=True)\n",
    "# lr_results.to_csv(\"results/lr.csv\")\n",
    "\n",
    "lr_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in DATASETS:\n",
    "    eval_on_mask = lr_results[\"evaluated_on\"] == dataset\n",
    "    train_on_mask = lr_results[\"trained_on\"] == f\"all\"\n",
    "    print(\"all\", dataset, lr_results.loc[train_on_mask & eval_on_mask, \"pearson\"])\n",
    "    \n",
    "print(\"\\n\", \"#\" * 20)\n",
    "print(\"LOO\")\n",
    "for dataset in DATASETS:\n",
    "    train_on_mask = lr_results[\"trained_on\"] == f\"except_{dataset}\"\n",
    "    print(dataset, \":\", lr_results.loc[train_on_mask, [\"evaluated_on\", \"pearson\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a3feb",
   "metadata": {},
   "source": [
    "## L1 Regression (Lasso Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats import expon\n",
    "\n",
    "\n",
    "def get_alpha(args):\n",
    "    return eval(args)[\"alpha\"]\n",
    "\n",
    "\n",
    "def plot_metric_by_alpha(data, metric, **kwargs):\n",
    "    n_plots = data.trained_on.nunique()\n",
    "    n_cols = 3\n",
    "\n",
    "    n_rows = n_plots // n_cols\n",
    "    n_rows += n_plots % n_cols\n",
    "\n",
    "    position = range(1, n_plots+1)\n",
    "\n",
    "    fig = plt.figure(1, figsize=(10, 10), dpi=150)\n",
    "\n",
    "    for k, trained_on in enumerate(data.trained_on.unique()):\n",
    "        d = data[(data[\"trained_on\"] == trained_on)]\n",
    "        ax = fig.add_subplot(n_rows, n_cols, position[k])\n",
    "        sns.lineplot(data=d, x=\"alpha\", y=metric, ax=ax, **kwargs)\n",
    "        ax.set_title(f\"Trained_on={trained_on}\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d72601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform model selection using L1 regression\n",
    "# since it is known to enforce sparsity of the solution!\n",
    "N_L1_MODELS = 100\n",
    "\n",
    "L1_GRID = {'alpha': expon(loc=0, scale=0.20)}\n",
    "L1_PARAMS = list(ParameterSampler(L1_GRID, n_iter=N_L1_MODELS, random_state=81723))\n",
    "\n",
    "plt.figure(figsize=(5, 3), dpi=150)\n",
    "plt.hist([p[\"alpha\"] for p in L1_PARAMS])\n",
    "plt.title(\"Distribution of the sampled alpha values for Lasso\")\n",
    "plt.savefig(\"results/l1_alphas_dist.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_pipelines = {}\n",
    "ad_results = []\n",
    "loo_results = []\n",
    "for i, l1_hparams in enumerate(L1_PARAMS):\n",
    "    if l1_hparams[\"alpha\"] > 2:\n",
    "        continue\n",
    "\n",
    "    # All datasets experiment\n",
    "    ad_l1_pipelines = fit(Lasso, l1_hparams, METRICS, TARGET, TRAIN_DATASETS)\n",
    "    ad_l1_results = evaluate(ad_l1_pipelines, DEV_DATASETS)\n",
    "    ad_l1_results = pd.DataFrame(ad_l1_results)\n",
    "    ad_l1_results[\"i\"] = i\n",
    "    ad_results.append(ad_l1_results)\n",
    "\n",
    "    loo_l1_pipelines = fit(Lasso, l1_hparams, METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "    # loo_l1_results = evaluate(loo_l1_pipelines, DEV_DATASETS)\n",
    "    loo_l1_results = evaluate_loo(loo_l1_pipelines, DEV_DATASETS)\n",
    "    loo_l1_results = pd.DataFrame(loo_l1_results)\n",
    "    loo_l1_results[\"i\"] = i\n",
    "    loo_results.append(loo_l1_results)\n",
    "\n",
    "    # All (baseline) LR results\n",
    "    l1_pipelines[i] = {\"AD\": ad_l1_pipelines, \"LOO\": loo_l1_pipelines}\n",
    "    \n",
    "l1_ad_results = pd.concat(ad_results).reset_index(drop=True)\n",
    "l1_ad_results[\"alpha\"] = l1_ad_results[\"model_hparams\"].apply(get_alpha)\n",
    "\n",
    "l1_loo_results = pd.concat(loo_results).reset_index(drop=True)\n",
    "l1_loo_results[\"alpha\"] = l1_loo_results[\"model_hparams\"].apply(get_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat_information(pipelines, results, experiment_type, metrics):\n",
    "    results = results.copy()\n",
    "    # For every set of experiments\n",
    "    for i, experiments in pipelines.items():\n",
    "        # Get the experiment_type pipeline (AD or LOO)\n",
    "        for trained_on, pipeline in experiments[experiment_type].items():\n",
    "            # Determine the important features and their importance\n",
    "            _feat_importance = pipeline.model.coef_\n",
    "            _mask = np.abs(_feat_importance) > 1e-6\n",
    "            \n",
    "            trained_on_mask = results[\"trained_on\"] == trained_on\n",
    "            i_mask = results[\"i\"] == i\n",
    "\n",
    "            _feats = np.argsort(np.abs(_feat_importance))[::-1]\n",
    "            _featnames = tuple(metrics[ix] for ix in _feats if _mask[ix])\n",
    "            _feats = {metrics[ix]: _feat_importance[ix] for ix in _feats if _mask[ix]}\n",
    "            _feats[\"intercept_\"] = pipeline.model.intercept_\n",
    "\n",
    "            results.loc[trained_on_mask & i_mask, \"n_features\"] =  sum(_mask)\n",
    "            results.loc[trained_on_mask & i_mask, \"feat_names\"] = str(_featnames)\n",
    "            results.loc[trained_on_mask & i_mask, \"feat_importance\"] = str(_feats)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "l1_ad_results = get_feat_information(l1_pipelines, l1_ad_results, \"AD\", METRICS)\n",
    "l1_ad_results.to_csv(\"results/l1_ad.csv\")\n",
    "\n",
    "l1_loo_results = get_feat_information(l1_pipelines, l1_loo_results, \"LOO\", METRICS)\n",
    "l1_loo_results.to_csv(\"results/l1_loo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204529d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_by_alpha(l1_ad_results, \"mse\")\n",
    "plt.savefig(f\"results/l1_ad_avg_mse_by_alpha{name}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d929fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_by_alpha(l1_ad_results, \"mse\", hue=\"evaluated_on\")\n",
    "plt.savefig(f\"results/l1_ad_mse_by_alpha_discriminated_by_evaluation_set{name}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686525c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_by_alpha(l1_loo_results, \"mse\", hue=\"evaluated_on\")\n",
    "plt.savefig(f\"results/l1_loo_mse_by_alpha_discriminated_by_evaluation_set{name}.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c5ae2",
   "metadata": {},
   "source": [
    "## Random Forest experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df20925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All datasets experiment\n",
    "ad_rf_def_pipelines = fit(RandomForestRegressor, {}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "ad_rf_def_results = evaluate(ad_rf_def_pipelines, DEV_DATASETS)\n",
    "ad_rf_def_results = pd.DataFrame(ad_rf_def_results)\n",
    "ad_rf_def_results.to_csv(f\"results/ad_rf{name}.csv\")\n",
    "\n",
    "loo_rf_def_pipelines = fit(RandomForestRegressor, {}, METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "loo_rf_def_results = evaluate(loo_rf_def_pipelines, DEV_DATASETS)\n",
    "loo_rf_def_results = pd.DataFrame(loo_rf_def_results)\n",
    "loo_rf_def_results.to_csv(f\"results/loo_rf{name}.csv\")\n",
    "\n",
    "# All (baseline) LR results\n",
    "rf_def_results = pd.concat((ad_rf_def_results, loo_rf_def_results)).reset_index(drop=True)\n",
    "rf_def_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3d7d67",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "ad_mlp_pipelines1 = fit(MLPRegressor, {\"random_state\": 42, \"early_stopping\": True}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "ad_mlp_results1 = evaluate(ad_mlp_pipelines1, DEV_DATASETS)\n",
    "ad_mlp_results1 = pd.DataFrame(ad_mlp_results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499106c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_mlp_pipelines2 = fit(MLPRegressor, {\"hidden_layer_sizes\": (128, 64, 32), \"random_state\": 42, \"early_stopping\": True}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "ad_mlp_results2 = evaluate(ad_mlp_pipelines2, DEV_DATASETS)\n",
    "ad_mlp_results2 = pd.DataFrame(ad_mlp_results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_mlp_results = pd.concat((ad_mlp_results1, ad_mlp_results2))\n",
    "ad_mlp_results.to_csv(f\"results/ad_mlp_default{name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1975cd5",
   "metadata": {},
   "source": [
    "## Few shot experiment \n",
    "\n",
    "We can perform this experiment in multiple ways. It considers the LOO experiment. \n",
    "We can use weight the training data differently, and we can use different number of examples in the LOO experiment.\n",
    "\n",
    "For the first experiment, we will consider using all the available training data ($100\\%$) and use different number of points in the LOO. In order to ensure comparable results, we will restrict our _few shot_ examples to the ones available in the training split (that weren't used in the first place) and we evaluate on the same development set. Future experiments may consider enlarging it and using more examples from the dev set.\n",
    "\n",
    "\n",
    "In general, we devise the following steps for a few-shot experiment:\n",
    "1. create dataset of $D_{PT}=(D_1, ..., D_5)$;\n",
    "2. train __model__ $m$ in $D_{PT}$;\n",
    "3. assign weight $w_{PT}$ to examples used in pre-training according to ratio $\\tau$;\n",
    "3. select a fraction of the examples $f$ from $D_6$;\n",
    "4. assign weight $w_{FS}$ to the fraction of $D_6$ examples according to ratio $\\tau$;\n",
    "5. train __model__\n",
    "6. evaluate in dev set for $D_6$\n",
    "5. repeat evaluation for 20 seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewsho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a62648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting scheme proof of concept\n",
    "n1, n2 = 24_000, 1000\n",
    "n = n1 + n2\n",
    "\n",
    "# If we want n1 examples to be equivalent to a\n",
    "# of the total dataset, then:\n",
    "a = 0.2\n",
    "target_n1, target_n2 = n * a, n * (1-a)\n",
    "n1_w = target_n1 / n1 \n",
    "n2_w = target_n2 / n2\n",
    "print(n1_w, n2_w)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49113ee4",
   "metadata": {},
   "source": [
    "fs_pipeline = FewShotPipeline(\n",
    "    fewshot_dataset=\"cosmosqa\",\n",
    "    fewshot_weight=0.6,\n",
    "    model_class=LinearRegression,\n",
    "    model_hparams={},\n",
    "    dataset=\"except_cosmosqa\",\n",
    "    features=METRICS,\n",
    "    target=TARGET,\n",
    ")\n",
    "\n",
    "fs_pipeline.load_data(TRAIN_LOO_DATASETS[\"except_cosmosqa\"], fewshot_data=TRAIN_DATASETS[\"cosmosqa\"])\n",
    "fs_pipeline.fewshot_fit()\n",
    "# TRAIN_LOO_DATASETS\n",
    "fs_pipeline.evaluate_multiple(DEV_DATASETS);\n",
    "del fs_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc97a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_few_shot_experiment(\n",
    "        train_datasets,\n",
    "        dataset_name,\n",
    "        fewshot_datasets,\n",
    "        fewshot_dataset_name,\n",
    "        eval_datasets,\n",
    "        fewshot_weights,\n",
    "        fewshot_pct_examples,\n",
    "        features,\n",
    "        target,\n",
    "        nruns=5,\n",
    "        seed=81723,\n",
    "        model_class=LinearRegression,\n",
    "        model_hparams={},\n",
    "        pipeline=None,\n",
    "    ):\n",
    "    from itertools import product\n",
    "    rand = np.random.default_rng(seed)\n",
    "    \n",
    "    fewshot_data = fewshot_datasets[fewshot_dataset_name]\n",
    "    \n",
    "    all_results = []\n",
    "    all_pipelines = []\n",
    "    for i, (fewshot_pct, fewshot_weight) in enumerate(product(fewshot_pct_examples, fewshot_weights)):\n",
    "        for j in range(nruns):\n",
    "            seed = rand.integers(10**6)\n",
    "            fewshot_fraction =  fewshot_data.sample(frac=fewshot_pct, replace=False, random_state=seed)\n",
    "            print(len(fewshot_fraction))\n",
    "            # Get subset of few shot data:\n",
    "            if pipeline is None:\n",
    "                pipeline = FewShotPipeline\n",
    "\n",
    "            fs_pipeline = pipeline(\n",
    "                fewshot_dataset=fewshot_dataset_name,\n",
    "                fewshot_weight=fewshot_weight,\n",
    "                model_class=model_class,\n",
    "                model_hparams=model_hparams,\n",
    "                dataset=dataset_name,\n",
    "                features=features,\n",
    "                target=target,\n",
    "                seed=seed,\n",
    "            )\n",
    "\n",
    "            fs_pipeline.load_data(train_datasets[dataset_name], fewshot_data=fewshot_fraction)\n",
    "            fs_pipeline.fewshot_fit()\n",
    "            results = fs_pipeline.evaluate_multiple(eval_datasets)\n",
    "            \n",
    "            for r in results:\n",
    "                r[\"i\"] = i\n",
    "                r[\"seed\"] = seed\n",
    "                r[\"fewshot_weight\"] = fewshot_weight\n",
    "                r[\"fewshot_pct\"] = fewshot_pct\n",
    "                \n",
    "            all_results.extend(results)\n",
    "            all_pipelines.append(fs_pipeline)\n",
    "            \n",
    "    return all_results, all_pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43705a50",
   "metadata": {},
   "source": [
    "### Experiments \n",
    "\n",
    "\n",
    "- [ ] GMMs\n",
    "- [x] Feature engineering: logs\n",
    "- [ ] Ordinal regression\n",
    "- [ ] Create self-contained script to launch fewshot experiment for individual dataset and model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "RESULTS_DIR = \"results_20220602\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd69c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEWSHOT_PCTS = np.linspace(0.01, 1, 15, endpoint=True)\n",
    "FEWSHOT_WEIGHTS = [0.25, 0.5, 0.75, 0.9, 1, None]\n",
    "\n",
    "print(len(FEWSHOT_PCTS), len(FEWSHOT_WEIGHTS))\n",
    "print(\"Fewshot pcts:\", FEWSHOT_PCTS)\n",
    "print(\"Fewshot weights:\", FEWSHOT_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dad927",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.01"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f832195",
   "metadata": {},
   "source": [
    "config = {\n",
    "    \"model_classpath\": \"sklearn.tree.RandomForestRegressor\",\n",
    "    \"model_hyperparams\": {},\n",
    "    \n",
    "    \"fewshot_pct\": [],\n",
    "    \"fewshot_weights\": [],\n",
    "    \"seed\": \n",
    "    \"nruns\": 1,\n",
    "    \n",
    "    \"training\": {\n",
    "        \"metrics\": METRICS,\n",
    "        \"target\": TARGET,\n",
    "        \"filepath\": ,\n",
    "        \"dataset\": ,\n",
    "    }\n",
    "    \"evaluation\": {\n",
    "        \"filepath\": ,\n",
    "        \"dataset\": ,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da778a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model: \n",
    "    def __init__(self):\n",
    "        self.name = None\n",
    "        self.classpath = None\n",
    "        self.pipeline = None\n",
    "        self.nruns = None\n",
    "\n",
    "model = Model()\n",
    "# model.name, model.classpath, model.hparams, model.nruns = \"lr\", LinearRegression, {}, 10\n",
    "# model.name, model.classpath, model.hparams, model.nruns = \"rf\", RandomForestRegressor, {\"n_jobs\": 15}, 2\n",
    "model.name, model.classpath, model.hparams, model.nruns, model.pipeline = \"mlp\", MLPRegressor, {\"learning_rate\": \"adaptive\", \"random_state\": 42, \"early_stopping\": True}, 5, FineTuningFewShotPipeline\n",
    "\n",
    "USE_LOG_METRICS = False\n",
    "\n",
    "METRICS = [\n",
    "    # Bleu\n",
    "    'bleu1', 'bleu2', 'bleu3', 'bleu4', \n",
    "    # 'hf_bleu1', 'hf_bleu2', 'hf_bleu3', 'hf_bleu4', \n",
    "    'rougeL', \n",
    "    # 'hf_rougeL', 'hf_rougeLsum',\n",
    "    'hf_rouge1', 'hf_rouge2',\n",
    "    'meteor',\n",
    "    'recall', 'precision', 'f1_score',\n",
    "    'sari_context', 'sari_question',\n",
    "    # Token overlap when 1st error occurred\n",
    "    'precision_at_err1', 'recall_at_err1',\n",
    "    # Confusion matrix\n",
    "    'tp', 'fn', 'fp',\n",
    "    # Edit scores ------\n",
    "    'char_edit_score', 'word_edit_score',\n",
    "    # Learned metrics -------\n",
    "    'bertscore', \n",
    "    'bleurt',\n",
    "    \"LERC\",\n",
    "    # Input statistics ------\n",
    "    'candidatelength_word', 'candidatelength_char',\n",
    "    'candidatenunique_words', 'referencelength_word',\n",
    "    'referencelength_char', 'referencenunique_words',\n",
    "    'contextlength_word', 'contextlength_char',\n",
    "    'contextnunique_words', 'questionlength_word',\n",
    "    'questionlength_char', 'questionnunique_words',\n",
    "]\n",
    "\n",
    "if USE_LOG_METRICS and len(LOG_METRICS) > 0:\n",
    "    METRICS += LOG_METRICS_NAMES\n",
    "    model.name += \"_w_log_metrics\"\n",
    "\n",
    "if \"bleurt\" not in METRICS:\n",
    "    model.name += '_no_bleurt'\n",
    "if \"bertscore\" not in METRICS:\n",
    "    model.name += \"_no_bertscore\"\n",
    "if \"LERC\" in METRICS:\n",
    "    model.name += \"_w_LERC\"\n",
    "    \n",
    "print(model.name)\n",
    "DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9bbbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for dataset in [\"quoref\", \"socialiqa\"]:\n",
    "#for dataset in ['narrativeqa', 'quoref', 'socialiqa']:\n",
    "for dataset in DATASETS:\n",
    "    print(\"Experiment for dataset\", dataset)\n",
    "    loo_fewshot, loo_ps =  run_few_shot_experiment(\n",
    "        train_datasets=TRAIN_LOO_DATASETS,\n",
    "        dataset_name=f\"except_{dataset}\",\n",
    "        fewshot_datasets=TRAIN_DATASETS,\n",
    "        fewshot_dataset_name=dataset,\n",
    "        eval_datasets=DEV_DATASETS,\n",
    "        fewshot_weights=FEWSHOT_WEIGHTS,\n",
    "        fewshot_pct_examples=FEWSHOT_PCTS,\n",
    "        features=METRICS,\n",
    "        target=TARGET,\n",
    "        nruns=model.nruns,\n",
    "        seed=81723,\n",
    "        model_class=model.classpath,\n",
    "        model_hparams=model.hparams,\n",
    "        pipeline=model.pipeline,\n",
    "    )\n",
    "\n",
    "    loo_results = pd.DataFrame(loo_fewshot)\n",
    "    loo_results.fewshot_weight = loo_results.fewshot_weight.fillna(\"default\")\n",
    "    \n",
    "    dataset_dir = f\"{RESULTS_DIR}/{dataset}\"\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    loo_results.to_csv(f\"{dataset_dir}/fewshot_loo_{model.name}_{model.nruns}.csv\")\n",
    "    joblib.dump(loo_ps, f\"{dataset_dir}/fewshot_loo_{model.name}_{model.nruns}.pipelines\")\n",
    "    del loo_fewshot\n",
    "    del loo_ps\n",
    "    del loo_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278f958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c80b2e871d42739fd0f1d2ac9c8f31a15028627c2324c1cda8de503e9f52a70d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
