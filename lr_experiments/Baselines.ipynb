{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb651c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_datasets import read_json_dataset\n",
    "from dict_utils import unfold_to_list, fold_from_list\n",
    "from pipeline import Pipeline, FewShotPipeline, FineTuningFewShotPipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "def add_lerc_preds(data, lerc_preds_dir, split):\n",
    "    lerc_preds = read_json_dataset(lerc_preds_dir, split)\n",
    "        \n",
    "    for dataset, d in lerc_preds.items():\n",
    "        for example_id, score in d.items():\n",
    "            data[dataset][example_id][\"LERC\"] = (score[\"pred_score\"] - 1) / (5-1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_log(data, metrics):\n",
    "    for m in metrics:\n",
    "        data[f\"{m}_log\"] = data[m].apply(lambda s: np.log(s+1e-15)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc9671c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "\n",
    "kendalltau(np.arange(5), np.arange(5)*np.arange(5))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"../data/lr_experiments\"\n",
    "LERC_PREDS_DIR = f\"{DATASET_DIR}/lerc_preds\"\n",
    "\n",
    "RESULTS_DIR = \"./experiments_20220614/results/all-data\"\n",
    "IMAGES_DIR = \"./experiments_20220614/images/all-data\"\n",
    "\n",
    "train = read_json_dataset(DATASET_DIR, \"train_metrics\")\n",
    "dev = read_json_dataset(DATASET_DIR, \"dev_metrics\")\n",
    "test = read_json_dataset(DATASET_DIR, \"test_metrics\")\n",
    "print(len(train), len(dev), len(test))\n",
    "\n",
    "add_lerc_preds(train, LERC_PREDS_DIR, \"train\")\n",
    "add_lerc_preds(dev, LERC_PREDS_DIR, \"dev\")\n",
    "add_lerc_preds(test, LERC_PREDS_DIR, \"test\")\n",
    "\n",
    "train_df = pd.DataFrame(unfold_to_list(train, \"dataset\", \"example_id\"))\n",
    "dev_df   = pd.DataFrame(unfold_to_list(dev, \"dataset\", \"example_id\"))\n",
    "test_df  = pd.DataFrame(unfold_to_list(test, \"dataset\", \"example_id\"))\n",
    "print(train_df.shape, dev_df.shape, test_df.shape)\n",
    "\n",
    "train_df[\"score_scaled\"] = train_df.score.apply(lambda s: (s-1)/(5-1))\n",
    "dev_df[\"score_scaled\"] = dev_df.score.apply(lambda s: (s-1)/(5-1))\n",
    "test_df[\"score_scaled\"] = test_df.score.apply(lambda s: (s-1)/(5-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "LERC_PREDS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26564e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = list(train.keys())\n",
    "\n",
    "METRICS = [\n",
    "    # Bleu\n",
    "    'bleu1', 'bleu2', 'bleu3', 'bleu4', \n",
    "    # 'hf_bleu1', 'hf_bleu2', 'hf_bleu3', 'hf_bleu4', \n",
    "    'rougeL', \n",
    "    # 'hf_rougeL', 'hf_rougeLsum',\n",
    "    'hf_rouge1', 'hf_rouge2',\n",
    "    'meteor',\n",
    "    'recall', 'precision', 'f1_score',\n",
    "    'sari_context', 'sari_question',\n",
    "    # Token overlap when 1st error occurred\n",
    "    'precision_at_err1', 'recall_at_err1',\n",
    "    # Confusion matrix\n",
    "    'tp', 'fn', 'fp',\n",
    "    # Edit scores ------\n",
    "    'char_edit_score',\n",
    "    'word_edit_score',\n",
    "    # Learned metrics -------\n",
    "    'bertscore', \n",
    "    'bleurt',\n",
    "    # Input statistics ------\n",
    "    'candidatelength_word',\n",
    "    'candidatelength_char',\n",
    "    'candidatenunique_words',\n",
    "    'referencelength_word',\n",
    "    'referencelength_char',\n",
    "    'referencenunique_words',\n",
    "    'contextlength_word',\n",
    "    'contextlength_char',\n",
    "    'contextnunique_words',\n",
    "    'questionlength_word',\n",
    "    'questionlength_char',\n",
    "    'questionnunique_words',\n",
    "]\n",
    "\n",
    "TARGET = \"score_scaled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f93496",
   "metadata": {},
   "source": [
    "**Validate numbers reported in original MOCHA paper**\n",
    "\n",
    "Most of the values are close to the numbers reported in the paper. The ones that are not, are consistently higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce19408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    print(); print(\"---- DEV SET ----\")\n",
    "    _df = dev_df[dev_df.dataset == dataset]\n",
    "    print(dataset, \"bleu1\", round(pearsonr(_df[\"score_scaled\"], _df[\"bleu1\"])[0], 3))\n",
    "    print(dataset, \"meteor\", round(pearsonr(_df[\"score_scaled\"], _df[\"meteor\"])[0], 3))\n",
    "    print(dataset, \"rougeL\", round(pearsonr(_df[\"score_scaled\"], _df[\"rougeL\"])[0], 3))\n",
    "    print(dataset, \"bert-score\", round(pearsonr(_df[\"score_scaled\"], _df[\"bertscore\"])[0], 3))\n",
    "    print()\n",
    "    \n",
    "    print(\"TEST SET\")\n",
    "    _df = test_df[test_df.dataset == dataset]\n",
    "    print(dataset, \"bleu1\", round(pearsonr(_df[\"score_scaled\"], _df[\"bleu1\"])[0], 3))\n",
    "    print(dataset, \"meteor\", round(pearsonr(_df[\"score_scaled\"], _df[\"meteor\"])[0], 3))\n",
    "    print(dataset, \"rougeL\", round(pearsonr(_df[\"score_scaled\"], _df[\"rougeL\"])[0], 3))\n",
    "    print(dataset, \"bert-score\", round(pearsonr(_df[\"score_scaled\"], _df[\"bertscore\"])[0], 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8936786",
   "metadata": {},
   "source": [
    "# Regression Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(df, dataset = None, col=\"dataset\"):\n",
    "    return df[df[col] == dataset].copy() if dataset else df\n",
    "    \n",
    "def get_all_datasets(df, datasets, include_all=True):\n",
    "    result = {} if not include_all else {\"all\": df.copy()}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        result.update({dataset: get_subset(df, dataset)})\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_loov_datasets(df, datasets):\n",
    "    result = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        loo_datasets = [get_subset(df, d) for d in datasets if d != dataset]\n",
    "        loo_dataset = pd.concat(loo_datasets)\n",
    "        \n",
    "        result.update({f\"except_{dataset}\": loo_dataset})\n",
    "        \n",
    "    return result\n",
    "\n",
    "TRAIN_DATASETS = get_all_datasets(train_df, DATASETS)\n",
    "DEV_DATASETS   = get_all_datasets(dev_df, DATASETS)\n",
    "TEST_DATASETS  = get_all_datasets(test_df, DATASETS)\n",
    "\n",
    "\n",
    "TRAIN_LOO_WRONG_LERC_DATASETS = get_loov_datasets(train_df, DATASETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d2eea",
   "metadata": {},
   "source": [
    "###### Add QASPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae56d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add QASPER\n",
    "qasper = read_json_dataset(DATASET_DIR, \"qasper_metrics\")\n",
    "print(\"Qasper:\", len(qasper))\n",
    "\n",
    "qasper_df = pd.DataFrame(unfold_to_list(qasper, \"dataset\", \"example_id\"))\n",
    "print(\"Qasper:\", qasper_df.shape)\n",
    "\n",
    "DEV_DATASETS.update({f\"qasper\": qasper_df.copy()})\n",
    "\n",
    "# Let us measure the values per annotation type\n",
    "for annot_type in sorted(qasper_df.annotation_type.unique()):\n",
    "    DEV_DATASETS.update({f\"qasper_{annot_type}\": qasper_df[qasper_df.annotation_type == annot_type].copy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344c8e8",
   "metadata": {},
   "source": [
    "# Baselines (ALL DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "        model_class,\n",
    "        model_hparams,\n",
    "        features,\n",
    "        target,\n",
    "        train_datasets,\n",
    "        split_frac=None,\n",
    "        with_std=True,\n",
    "        with_pca=False,\n",
    "        seed=817237,\n",
    "        pipeline_class=Pipeline,\n",
    "    ) -> dict:\n",
    "    pipelines = {}\n",
    "    \n",
    "    for train_name, train_data in train_datasets.items():\n",
    "        pipeline = pipeline_class(model_class, model_hparams, train_name, features, target, seed=seed)\n",
    "        pipeline.load_data(train_data)\n",
    "        if split_frac and isinstance(split_frac, float):\n",
    "            pipeline.split(holdout_fraction=split_frac)\n",
    "\n",
    "        pipeline.preprocess(with_std=with_std, with_pca=with_pca)\n",
    "        pipeline.fit()\n",
    "        pipelines[train_name] = pipeline\n",
    "\n",
    "    return pipelines\n",
    "\n",
    "\n",
    "def evaluate(pipelines, eval_datasets):\n",
    "    results = []\n",
    "    for train_name, pipeline in pipelines.items():\n",
    "        result = pipeline.evaluate_multiple(eval_datasets)\n",
    "        results.extend(result)\n",
    "    return results\n",
    "\n",
    "def evaluate_loo(pipelines, eval_datasets):\n",
    "    results = []\n",
    "    for train_name, pipeline in pipelines.items():\n",
    "        loo_dataset = train_name.rpartition(\"_\")[-1]\n",
    "        result = pipeline.evaluate_multiple({loo_dataset: eval_datasets[loo_dataset]})\n",
    "        results.extend(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f30803",
   "metadata": {},
   "source": [
    "## Individual metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ca0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndividualMetric:\n",
    "    def __init__(self, feature=None):\n",
    "        self.feature = feature\n",
    "                \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.feature is None:\n",
    "            raise RuntimeError(\"No feature specified\")\n",
    "\n",
    "        y_pred = X[self.feature]\n",
    "        \n",
    "        assert len(y_pred) == X.shape[0]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce5628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "individual_results = []\n",
    "for metric in [\"LERC\"] + METRICS: \n",
    "    ps = fit(IndividualMetric, {\"feature\": metric}, [metric], TARGET, {\"all\": TRAIN_DATASETS[\"all\"]}, with_pca=False, with_std=False)\n",
    "    results = evaluate(ps, DEV_DATASETS)\n",
    "    for r in results:\n",
    "        r[\"model_classpath\"] = metric\n",
    "    \n",
    "    individual_results.extend(results)\n",
    "\n",
    "individual_results = pd.DataFrame(individual_results)\n",
    "individual_results.to_csv(f\"{RESULTS_DIR}/individual_metrics.csv\")\n",
    "individual_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffccaf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f3759",
   "metadata": {},
   "source": [
    "## Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageBaseline:                \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.mean(X, axis=1)\n",
    "        assert len(y_pred) == X.shape[0]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b19646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top3_features  = {\n",
    "    \"cosmosqa\": [\"bleurt\", \"bertscore\", \"meteor\"],\n",
    "    \"drop\": [\"hf_rouge1\", \"meteor\", \"f1_score\"],\n",
    "    \"mcscript\": [\"bleurt\", \"meteor\", \"hf_rouge1\"],\n",
    "    \"narrativeqa\": [\"bleurt\", \"bertscore\", \"meteor\"],\n",
    "    \"quoref\": [\"hf_rouge1\", \"meteor\", \"bleurt\"],\n",
    "    \"socialiqa\": [\"bleurt\", \"meteor\", \"precision\"],\n",
    "}\n",
    "\n",
    "ad_avg_results = []\n",
    "# All datasets experiment\n",
    "print(\"AVG Baseline\")\n",
    "ad_avg_pipelines = fit(AverageBaseline, {}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "ad_avg_results.extend(evaluate(ad_avg_pipelines, DEV_DATASETS))\n",
    "\n",
    "ad_avg_pipelines = fit(AverageBaseline, {}, [\"LERC\"] + METRICS, TARGET, TRAIN_DATASETS)\n",
    "ad_avg_results.extend(evaluate(ad_avg_pipelines, DEV_DATASETS))\n",
    "\n",
    "feature_set = set()\n",
    "for dataset in DATASETS:\n",
    "    features = tuple(top3_features[dataset])\n",
    "\n",
    "    if features not in feature_set:\n",
    "        ad_avg_pipelines = fit(AverageBaseline, {}, list(features), TARGET, TRAIN_DATASETS)\n",
    "        results = evaluate(ad_avg_pipelines, DEV_DATASETS)\n",
    "        \n",
    "        for r in results:\n",
    "            r[\"model_classpath\"] = str(features)\n",
    "        ad_avg_results.extend(results)\n",
    "        feature_set.add(features)\n",
    "\n",
    "        ad_avg_pipelines = fit(AverageBaseline, {}, [\"LERC\"] + list(features), TARGET, TRAIN_DATASETS)\n",
    "        results = evaluate(ad_avg_pipelines, DEV_DATASETS)\n",
    "        \n",
    "        for r in results:\n",
    "            r[\"model_classpath\"] = str([\"LERC\"]+ list(features))\n",
    "        ad_avg_results.extend(results)\n",
    "        feature_set.add(tuple([\"LERC\"]+ list(features)))\n",
    "    \n",
    "ad_avg_results = pd.DataFrame(ad_avg_results).reset_index(drop=True)\n",
    "ad_avg_results.to_csv(f\"{RESULTS_DIR}/avg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91694d9",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4019ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All datasets experiment\n",
    "lr_results = []\n",
    "\n",
    "lr_pipelines = fit(LinearRegression, {}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "lr_results.extend(evaluate(lr_pipelines, DEV_DATASETS))\n",
    "\n",
    "lr_pipelines = fit(LinearRegression, {}, METRICS + [\"LERC\"], TARGET, TRAIN_DATASETS)\n",
    "lr_results.extend(evaluate(lr_pipelines, DEV_DATASETS))\n",
    "\n",
    "lr_results = pd.DataFrame(lr_results)\n",
    "lr_results.to_csv(f\"{RESULTS_DIR}/linear_regression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a3feb",
   "metadata": {},
   "source": [
    "## L1 Regression (Lasso Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats import expon\n",
    "\n",
    "\n",
    "def get_alpha(args):\n",
    "    return eval(args)[\"alpha\"]\n",
    "\n",
    "\n",
    "def plot_metric_by_alpha(data, metric, **kwargs):\n",
    "    n_plots = data.trained_on.nunique()\n",
    "    n_cols = 3\n",
    "\n",
    "    n_rows = n_plots // n_cols\n",
    "    n_rows += n_plots % n_cols\n",
    "\n",
    "    position = range(1, n_plots+1)\n",
    "\n",
    "    fig = plt.figure(1, figsize=(10, 10), dpi=150)\n",
    "\n",
    "    for k, trained_on in enumerate(data.trained_on.unique()):\n",
    "        d = data[(data[\"trained_on\"] == trained_on)]\n",
    "        d = d.sort_values(by=\"alpha\")\n",
    "        ax = fig.add_subplot(n_rows, n_cols, position[k])\n",
    "        sns.lineplot(data=d, x=\"alpha\", y=metric, ax=ax, **kwargs)\n",
    "        ax.set_title(f\"Trained_on={trained_on}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "def get_feat_information(pipelines, results, experiment_type, metrics):\n",
    "    results = results.copy()\n",
    "    # For every set of experiments\n",
    "    for i, experiments in pipelines.items():\n",
    "        # Get the experiment_type pipeline (AD or LOO)\n",
    "        for trained_on, pipeline in experiments[experiment_type].items():\n",
    "            # Determine the important features and their importance\n",
    "            _feat_importance = pipeline.model.coef_\n",
    "            _mask = np.abs(_feat_importance) > 1e-6\n",
    "            \n",
    "            trained_on_mask = results[\"trained_on\"] == trained_on\n",
    "            i_mask = results[\"i\"] == i\n",
    "\n",
    "            _feats = np.argsort(np.abs(_feat_importance))[::-1]\n",
    "            _featnames = tuple(metrics[ix] for ix in _feats if _mask[ix])\n",
    "            _feats = {metrics[ix]: _feat_importance[ix] for ix in _feats if _mask[ix]}\n",
    "            _feats[\"intercept_\"] = pipeline.model.intercept_\n",
    "\n",
    "            results.loc[trained_on_mask & i_mask, \"n_features\"] =  sum(_mask)\n",
    "            results.loc[trained_on_mask & i_mask, \"feat_names\"] = str(_featnames)\n",
    "            results.loc[trained_on_mask & i_mask, \"feat_importance\"] = str(_feats)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def group_by(results):\n",
    "    results = results.copy()\n",
    "    results[\"model_hparams\"] = results[\"model_hparams\"].apply(eval)\n",
    "    results[\"model_seed\"] = results[\"model_hparams\"].apply(lambda d: d.pop(\"random_state\"))\n",
    "    results[\"model_hparams\"] = results[\"model_hparams\"].apply(str)\n",
    "    return results\\\n",
    "        .groupby(['model_classpath', 'model_hparams', 'features', 'target', 'trained_on', 'evaluated_on'])\\\n",
    "        .mean()\\\n",
    "        .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d72601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform model selection using L1 regression\n",
    "# since it is known to enforce sparsity of the solution!\n",
    "N_L1_MODELS = 100\n",
    "\n",
    "L1_GRID = {'alpha': expon(loc=0, scale=0.20)}\n",
    "L1_PARAMS = list(ParameterSampler(L1_GRID, n_iter=N_L1_MODELS, random_state=81723))\n",
    "\n",
    "plt.figure(figsize=(5, 3), dpi=150)\n",
    "plt.hist([p[\"alpha\"] for p in L1_PARAMS])\n",
    "plt.title(\"Distribution of L1 regularization coefficients (α)\")\n",
    "plt.xlabel(\"α\")\n",
    "plt.savefig(f\"{IMAGES_DIR}/l1_alphas_dist.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_pipelines = {}\n",
    "ad_results = []\n",
    "loo_results = []\n",
    "for i, l1_hparams in enumerate(L1_PARAMS):\n",
    "    if l1_hparams[\"alpha\"] > 2:\n",
    "        continue\n",
    "\n",
    "    # All datasets experiment\n",
    "    ad_l1_pipelines = fit(Lasso, l1_hparams, METRICS, TARGET, TRAIN_DATASETS)\n",
    "    ad_l1_results = evaluate(ad_l1_pipelines, DEV_DATASETS)\n",
    "    ad_l1_results = pd.DataFrame(ad_l1_results)\n",
    "    ad_l1_results[\"i\"] = i\n",
    "    ad_results.append(ad_l1_results)\n",
    "\n",
    "    # All (baseline) LR results\n",
    "    l1_pipelines[i] = {\"AD\": ad_l1_pipelines}\n",
    "    \n",
    "l1_ad_results = pd.concat(ad_results).reset_index(drop=True)\n",
    "l1_ad_results[\"alpha\"] = l1_ad_results[\"model_hparams\"].apply(get_alpha)\n",
    "\n",
    "l1_ad_results = get_feat_information(l1_pipelines, l1_ad_results, \"AD\", METRICS)\n",
    "l1_ad_results.to_csv(f\"{RESULTS_DIR}/lasso_regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb480e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_metric_by_alpha(l1_ad_results, \"mse\")\n",
    "plt.savefig(f\"{IMAGES_DIR}/l1_alldata_avg_mse_by_alpha.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "plot_metric_by_alpha(l1_ad_results, \"mse\", hue=\"evaluated_on\")\n",
    "plt.savefig(f\"{IMAGES_DIR}/l1_alldata_mse_by_alpha_discriminated_by_evaluation_set.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d948e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_pipelines = {}\n",
    "ad_results = []\n",
    "for i, l1_hparams in enumerate(L1_PARAMS):\n",
    "    if l1_hparams[\"alpha\"] > 2:\n",
    "        continue\n",
    "\n",
    "    # All datasets experiment\n",
    "    ad_l1_pipelines = fit(Lasso, l1_hparams, METRICS + [\"LERC\"], TARGET, TRAIN_DATASETS)\n",
    "    ad_l1_results = evaluate(ad_l1_pipelines, DEV_DATASETS)\n",
    "    ad_l1_results = pd.DataFrame(ad_l1_results)\n",
    "    ad_l1_results[\"i\"] = i\n",
    "    ad_results.append(ad_l1_results)\n",
    "\n",
    "    # All (baseline) LR results\n",
    "    l1_pipelines[i] = {\"AD\": ad_l1_pipelines}\n",
    "    \n",
    "l1_ad_results = pd.concat(ad_results).reset_index(drop=True)\n",
    "l1_ad_results[\"alpha\"] = l1_ad_results[\"model_hparams\"].apply(get_alpha)\n",
    "\n",
    "l1_ad_results = get_feat_information(l1_pipelines, l1_ad_results, \"AD\", METRICS + [\"LERC\"])\n",
    "l1_ad_results.to_csv(f\"{RESULTS_DIR}/lasso_regression_with_lerc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b73191",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_by_alpha(l1_ad_results, \"mse\")\n",
    "plt.savefig(f\"{IMAGES_DIR}/l1_alldata_w_lerc_avg_mse_by_alpha.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "plot_metric_by_alpha(l1_ad_results, \"mse\", hue=\"evaluated_on\")\n",
    "plt.savefig(f\"{IMAGES_DIR}/l1_alldata_w_lerc_mse_by_alpha_discriminated_by_evaluation_set.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf206663",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a62924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rnd = np.random.default_rng(13)\n",
    "\n",
    "ad_rf_def_results = []\n",
    "for i in range(3):\n",
    "    seed = rnd.integers(10**6)\n",
    "    \n",
    "    # All datasets experiment\n",
    "    ad_rf_def_pipelines = fit(RandomForestRegressor, {\"random_state\": seed}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "    ad_rf_def_results.extend(evaluate(ad_rf_def_pipelines, DEV_DATASETS))\n",
    "    \n",
    "ad_rf_def_results = pd.DataFrame(ad_rf_def_results)\n",
    "ad_rf_def_results[\"features\"] = ad_rf_def_results[\"features\"].astype(str)\n",
    "ad_rf_def_results_gb = group_by(ad_rf_def_results)\n",
    "ad_rf_def_results_gb.to_csv(f\"{RESULTS_DIR}/random_forest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe1873",
   "metadata": {},
   "source": [
    "### With LERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eef3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All datasets experiment\n",
    "\n",
    "rnd = np.random.default_rng(13)\n",
    "\n",
    "ad_rf_def_results = []\n",
    "for i in range(3):\n",
    "    seed = rnd.integers(10**6)\n",
    "    \n",
    "    # All datasets experiment\n",
    "    ad_rf_def_pipelines = fit(RandomForestRegressor, {\"random_state\": seed}, METRICS + [\"LERC\"], TARGET, TRAIN_DATASETS)\n",
    "    ad_rf_def_results.extend(evaluate(ad_rf_def_pipelines, DEV_DATASETS))\n",
    "    \n",
    "ad_rf_def_results = pd.DataFrame(ad_rf_def_results)\n",
    "ad_rf_def_results[\"features\"] = ad_rf_def_results[\"features\"].astype(str)\n",
    "ad_rf_def_results_gb = group_by(ad_rf_def_results)\n",
    "ad_rf_def_results_gb.to_csv(f\"{RESULTS_DIR}/random_forest_with_lerc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2026a",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "rnd = np.random.default_rng(13)\n",
    "\n",
    "lgbm_results = []\n",
    "for i in range(10):\n",
    "    seed = rnd.integers(10**6)\n",
    "    lgbm_def_pipelines = fit(lgb.LGBMRegressor, {\"random_state\": seed}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "    lgbm_results.extend(evaluate(lgbm_def_pipelines, DEV_DATASETS))\n",
    "\n",
    "lgbm_results = pd.DataFrame(lgbm_results)\n",
    "lgbm_results[\"features\"] = lgbm_results[\"features\"].astype(str)\n",
    "lgbm_results_gb = group_by(lgbm_results)\n",
    "lgbm_results_gb.to_csv(f\"{RESULTS_DIR}/lgbm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b80e0",
   "metadata": {},
   "source": [
    "### With LERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12acfccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "rnd = np.random.default_rng(13)\n",
    "\n",
    "lgbm_results = []\n",
    "for i in range(10):\n",
    "    seed = rnd.integers(10**6)\n",
    "    lgbm_def_pipelines = fit(lgb.LGBMRegressor, {\"random_state\": seed}, METRICS + [\"LERC\"], TARGET, TRAIN_DATASETS)\n",
    "    lgbm_results.extend(evaluate(lgbm_def_pipelines, DEV_DATASETS))\n",
    "\n",
    "lgbm_results = pd.DataFrame(lgbm_results)\n",
    "lgbm_results[\"features\"] = lgbm_results[\"features\"].astype(str)\n",
    "lgbm_results_gb = group_by(lgbm_results)\n",
    "lgbm_results_gb.to_csv(f\"{RESULTS_DIR}/lgbm_with_lerc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c210f2",
   "metadata": {},
   "source": [
    "## MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_results = []\n",
    "rnd = np.random.default_rng(13)\n",
    "\n",
    "for i in range(10):\n",
    "    seed = rnd.integers(10**6)\n",
    "\n",
    "    mlp_pipelines = fit(MLPRegressor, {\"random_state\": seed, \"early_stopping\": True}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "    mlp_results.extend(evaluate(mlp_pipelines, DEV_DATASETS))\n",
    "\n",
    "    mlp_pipelines = fit(MLPRegressor, {\"hidden_layer_sizes\": (128, 64, 32), \"random_state\": seed, \"early_stopping\": True}, METRICS, TARGET, TRAIN_DATASETS)\n",
    "    mlp_results.extend(evaluate(mlp_pipelines, DEV_DATASETS))\n",
    "\n",
    "mlp_results = pd.DataFrame(mlp_results)\n",
    "mlp_results[\"features\"] = mlp_results[\"features\"].astype(str)\n",
    "mlp_results_gb = group_by(mlp_results)\n",
    "mlp_results_gb.to_csv(f\"{RESULTS_DIR}/mlp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90698e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.default_rng(13)\n",
    "\n",
    "mlp_results = []\n",
    "for i in range(10):\n",
    "    seed = rnd.integers(10**6)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # WITH LERC \n",
    "    # -------------------------------------------------------\n",
    "    mlp_pipelines = fit(MLPRegressor, {\"random_state\": seed, \"early_stopping\": True}, METRICS + [\"LERC\"], TARGET, TRAIN_DATASETS)\n",
    "    mlp_results.extend(evaluate(mlp_pipelines, DEV_DATASETS))\n",
    "    \n",
    "    mlp_pipelines = fit(MLPRegressor, {\"hidden_layer_sizes\": (128, 64, 32), \"random_state\": seed, \"early_stopping\": True}, METRICS + [\"LERC\"], TARGET, TRAIN_DATASETS)\n",
    "    mlp_results.extend(evaluate(mlp_pipelines, DEV_DATASETS))\n",
    "\n",
    "mlp_results = pd.DataFrame(mlp_results)\n",
    "mlp_results[\"features\"] = mlp_results[\"features\"].astype(str)\n",
    "mlp_results_gb = group_by(mlp_results)\n",
    "mlp_results_gb.to_csv(f\"{RESULTS_DIR}/mlp_with_lerc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf1197",
   "metadata": {},
   "source": [
    "# Baselines (Zero shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LERC_PREDS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a86a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute zero shot LERC\n",
    "DATASET_DIR = \"../data/lr_experiments/\"\n",
    "\n",
    "RESULTS_DIR = \"./experiments_20220614/results/zero-shot\"\n",
    "IMAGES_DIR = \"./experiments_20220614/images/zero-shot\"\n",
    "\n",
    "TRAIN_LOO_DATASETS = {}\n",
    "FEW_SHOT_LOO_DATASETS = {}\n",
    "DEV_LOO_DATASETS = {}\n",
    "\n",
    "for dataset in [\"narrativeqa\", \"mcscript\"]:\n",
    "    train = read_json_dataset(DATASET_DIR, \"train_metrics\")\n",
    "    dev = read_json_dataset(DATASET_DIR, \"dev_metrics\")\n",
    "    print(len(train), len(dev))\n",
    "    \n",
    "    LERC_PREDS_DIR = f\"{DATASET_DIR}/lerc_wo_{dataset}\"\n",
    "    add_lerc_preds(train, LERC_PREDS_DIR, \"train_preds\")\n",
    "    add_lerc_preds(dev, LERC_PREDS_DIR, \"dev_preds\")\n",
    " \n",
    "    train_df = pd.DataFrame(unfold_to_list(train, \"dataset\", \"example_id\"))\n",
    "    dev_df   = pd.DataFrame(unfold_to_list(dev, \"dataset\", \"example_id\"))\n",
    "    \n",
    "    # Scale the scores (we will have negatives and above 1)\n",
    "    train_df[\"score_scaled\"] = train_df.score.apply(lambda s: (s-1)/(5-1))\n",
    "    dev_df[\"score_scaled\"] = dev_df.score.apply(lambda s: (s-1)/(5-1))\n",
    "    \n",
    "    \n",
    "    # Compute the training leave-one-out (all datasets except the LOO)\n",
    "    train_loo_df = train_df[train_df[\"dataset\"] != dataset]  \n",
    "    TRAIN_LOO_DATASETS[f\"except_{dataset}\"] = train_loo_df\n",
    "    \n",
    "    # Compute the few shot dataset (the LOO training split)\n",
    "    fewshot_loo_df = train_df[train_df[\"dataset\"] == dataset]\n",
    "    FEW_SHOT_LOO_DATASETS[f\"{dataset}\"] = fewshot_loo_df\n",
    "\n",
    "    # Development set, we'll evaluate on the narrativeQA directly\n",
    "    dev_df   = dev_df[dev_df[\"dataset\"] == dataset]\n",
    "    DEV_LOO_DATASETS[dataset] = dev_df\n",
    "    \n",
    "    print(train_loo_df.shape, fewshot_loo_df.shape, dev_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a7113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30977b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LOO_DATASETS.keys(), FEW_SHOT_LOO_DATASETS.keys(), DEV_LOO_DATASETS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(DEV_LOO_DATASETS[\"narrativeqa\"][\"LERC\"], DEV_LOO_DATASETS[\"narrativeqa\"][\"score_scaled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ed6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(DEV_LOO_DATASETS[\"mcscript\"][\"LERC\"], DEV_LOO_DATASETS[\"mcscript\"][\"score_scaled\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f829713",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# All datasets experiment\n",
    "lr_results = []\n",
    "\n",
    "lr_pipelines = fit(LinearRegression, {}, METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "lr_results.extend(evaluate(lr_pipelines, DEV_LOO_DATASETS))\n",
    "\n",
    "# With LERC\n",
    "lr_pipelines = fit(LinearRegression, {}, METRICS + [\"LERC\"], TARGET, TRAIN_LOO_DATASETS)\n",
    "lr_results.extend(evaluate(lr_pipelines, DEV_LOO_DATASETS))\n",
    "\n",
    "lr_results = pd.DataFrame(lr_results)\n",
    "lr_results.to_csv(f\"{RESULTS_DIR}/linear_regression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c7861",
   "metadata": {},
   "source": [
    "## L1 Regression (LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_pipelines = {}\n",
    "loo_results = []\n",
    "for i, l1_hparams in enumerate(L1_PARAMS):\n",
    "    if l1_hparams[\"alpha\"] > 2:\n",
    "        continue\n",
    "\n",
    "    loo_l1_pipelines = fit(Lasso, l1_hparams, METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "    # loo_l1_results = evaluate(loo_l1_pipelines, DEV_DATASETS)\n",
    "    loo_l1_results = evaluate_loo(loo_l1_pipelines, DEV_LOO_DATASETS)\n",
    "    loo_l1_results = pd.DataFrame(loo_l1_results)\n",
    "    loo_l1_results[\"i\"] = i\n",
    "    loo_results.append(loo_l1_results)\n",
    "    \n",
    "    l1_pipelines[(i,)] = {\"LOO\": loo_l1_pipelines}\n",
    "\n",
    "    loo_l1_pipelines = fit(Lasso, l1_hparams, METRICS + [\"LERC\"], TARGET, TRAIN_LOO_DATASETS)\n",
    "    # loo_l1_results = evaluate(loo_l1_pipelines, DEV_DATASETS)\n",
    "    loo_l1_results = evaluate_loo(loo_l1_pipelines, DEV_LOO_DATASETS)\n",
    "    loo_l1_results = pd.DataFrame(loo_l1_results)\n",
    "    loo_l1_results[\"i\"] = i\n",
    "    loo_results.append(loo_l1_results)\n",
    "    \n",
    "    l1_pipelines[(i, \"LERC\")] = {\"LOO\": loo_l1_pipelines}\n",
    "\n",
    "\n",
    "l1_loo_results = pd.concat(loo_results).reset_index(drop=True)\n",
    "l1_loo_results[\"alpha\"] = l1_loo_results[\"model_hparams\"].apply(get_alpha)\n",
    "\n",
    "l1_loo_results = get_feat_information(l1_pipelines, l1_loo_results, \"LOO\", METRICS + [\"LERC\"])\n",
    "l1_loo_results.to_csv(f\"{RESULTS_DIR}/lasso_regression.csv\")\n",
    "l1_loo_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_by_alpha(l1_loo_results, \"mse\", hue=\"evaluated_on\")\n",
    "plt.savefig(f\"{IMAGES_DIR}/l1_mse_by_alpha_discriminated_by_evaluation_set.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70393b1",
   "metadata": {},
   "source": [
    "### with LERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce84438",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_by_alpha(l1_loo_results, \"mse\", hue=\"evaluated_on\")\n",
    "plt.savefig(f\"{IMAGES_DIR}/l1_z_mse_by_alpha_discriminated_by_evaluation_set_lerc.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c5ae2",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34922d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_rf_def_pipelines = fit(RandomForestRegressor, {}, METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "loo_rf_def_results = evaluate(loo_rf_def_pipelines, DEV_LOO_DATASETS)\n",
    "loo_rf_def_results = pd.DataFrame(loo_rf_def_results)\n",
    "loo_rf_def_results.to_csv(f\"{RESULTS_DIR}/random_forest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4146a",
   "metadata": {},
   "source": [
    "### With LERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_rf_def_pipelines = fit(RandomForestRegressor, {}, METRICS+[\"LERC\"], TARGET, TRAIN_LOO_DATASETS)\n",
    "loo_rf_def_results = evaluate(loo_rf_def_pipelines, DEV_LOO_DATASETS)\n",
    "loo_rf_def_results = pd.DataFrame(loo_rf_def_results)\n",
    "\n",
    "loo_rf_def_results.to_csv(f\"{RESULTS_DIR}/random_forest_with_lerc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7b922",
   "metadata": {},
   "source": [
    "## LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e638ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81345aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_def_results = []\n",
    "\n",
    "lgbm_def_pipelines = fit(lgb.LGBMRegressor, {\"random_state\": 113}, METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "lgbm_def_results.extend(evaluate(lgbm_def_pipelines, DEV_LOO_DATASETS))\n",
    "\n",
    "# LERC\n",
    "lgbm_def_pipelines = fit(lgb.LGBMRegressor, {\"random_state\": 113}, METRICS + [\"LERC\"], TARGET, TRAIN_LOO_DATASETS)\n",
    "lgbm_def_results.extend(evaluate(lgbm_def_pipelines, DEV_LOO_DATASETS))\n",
    "\n",
    "lgbm_def_results = pd.DataFrame(lgbm_def_results)\n",
    "lgbm_def_results.to_csv(f\"{RESULTS_DIR}/lgbm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_DATASETS[\"narrativeqa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ae46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_def_results = []\n",
    "\n",
    "lgbm_def_pipelines = fit(lgb.LGBMRegressor, {\"random_state\": 113}, METRICS, TARGET, TRAIN_LOO_WRONG_LERC_DATASETS)\n",
    "lgbm_def_results.extend(evaluate(lgbm_def_pipelines, DEV_DATASETS))\n",
    "\n",
    "lgbm_def_results = pd.DataFrame(lgbm_def_results)\n",
    "lgbm_def_results.pivot(index=[\"model_classpath\", \"model_hparams\", 'trained_on'], columns='evaluated_on', values='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = lgbm_def_results[\"trained_on\"] == \"except_mcscript\"\n",
    "mask2 = lgbm_def_results[\"evaluated_on\"] == \"mcscript\"\n",
    "\n",
    "lgbm_def_results[mask1 & mask2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3d7d67",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_results = []\n",
    "\n",
    "mlp_pipelines = fit(MLPRegressor, {\"random_state\": 42, \"early_stopping\": True}, METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "mlp_results.extend(evaluate(mlp_pipelines, DEV_LOO_DATASETS))\n",
    "\n",
    "mlp_pipelines = fit(MLPRegressor, {\"hidden_layer_sizes\": (128, 64, 32), \"random_state\": 42, \"early_stopping\": True}, METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "mlp_results.extend(evaluate(mlp_pipelines, DEV_LOO_DATASETS))\n",
    "\n",
    "mlp_pipelines = fit(MLPRegressor, {\"random_state\": 42, \"early_stopping\": True}, [\"LERC\"] + METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "mlp_results.extend(evaluate(mlp_pipelines, DEV_LOO_DATASETS))\n",
    "\n",
    "mlp_pipelines = fit(MLPRegressor, {\"hidden_layer_sizes\": (128, 64, 32), \"random_state\": 42, \"early_stopping\": True}, [\"LERC\"] + METRICS, TARGET, TRAIN_LOO_DATASETS)\n",
    "mlp_results.extend(evaluate(mlp_pipelines, DEV_LOO_DATASETS))\n",
    "\n",
    "mlp_results = pd.DataFrame(mlp_results)\n",
    "mlp_results.to_csv(f\"{RESULTS_DIR}/mlp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_results = []\n",
    "\n",
    "mlp_pipelines = fit(MLPRegressor, {\"hidden_layer_sizes\": (128, 64, 32), \"random_state\": 42, \"early_stopping\": True}, METRICS, TARGET, TRAIN_LOO_WRONG_LERC_DATASETS)\n",
    "mlp_results.extend(evaluate(mlp_pipelines, DEV_DATASETS))\n",
    "\n",
    "mlp_results = pd.DataFrame(mlp_results)\n",
    "mlp_results.pivot(index=[\"model_classpath\", \"model_hparams\", 'trained_on'], columns='evaluated_on', values='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1975cd5",
   "metadata": {},
   "source": [
    "## Few shot experiment \n",
    "\n",
    "\n",
    "TODO \n",
    "- [ ] Random Downsample\n",
    "- [ ] Measure of uncertainty to pick downsampling? \n",
    "\n",
    "We can perform this experiment in multiple ways. It considers the LOO experiment. \n",
    "We can use weight the training data differently, and we can use different number of examples in the LOO experiment.\n",
    "\n",
    "For the first experiment, we will consider using all the available training data ($100\\%$) and use different number of points in the LOO. In order to ensure comparable results, we will restrict our _few shot_ examples to the ones available in the training split (that weren't used in the first place) and we evaluate on the same development set. Future experiments may consider enlarging it and using more examples from the dev set.\n",
    "\n",
    "\n",
    "In general, we devise the following steps for a few-shot experiment:\n",
    "1. create dataset of $D_{PT}=(D_1, ..., D_5)$;\n",
    "2. train __model__ $m$ in $D_{PT}$;\n",
    "3. assign weight $w_{PT}$ to examples used in pre-training according to ratio $\\tau$;\n",
    "3. select a fraction of the examples $f$ from $D_6$;\n",
    "4. assign weight $w_{FS}$ to the fraction of $D_6$ examples according to ratio $\\tau$;\n",
    "5. train __model__\n",
    "6. evaluate in dev set for $D_6$\n",
    "5. repeat evaluation for 20 seeds.\n",
    "\n",
    "\n",
    "$MSE(Y, \\hat{Y}) = \\frac{\\alpha}{|D_o|} \\sum_{(y, \\hat{y}) \\in D_o} SSE(y, \\hat{y}) + \\frac{1-\\alpha}{|D_x|} \\sum_{(y, \\hat{y}) \\in D_x} SSE(y, \\hat{y}) $\n",
    "\n",
    "\n",
    "_default_ implies $\\alpha = \\frac{|D_o|}{|D_o| + |D_x|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f0c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"./experiments_20220614/results/few-shot\"\n",
    "IMAGES_DIR = \"./experiments_20220614/images/few-shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a62648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Weighting scheme proof of concept\n",
    "# ---------------------------------------------\n",
    "n1, n2 = 24_000, 1000\n",
    "n = n1 + n2\n",
    "\n",
    "# If we want n1 examples to be equivalent to a\n",
    "# of the total dataset, then:\n",
    "a = 0.2\n",
    "target_n1, target_n2 = n * a, n * (1-a)\n",
    "n1_w = target_n1 / n1 \n",
    "n2_w = target_n2 / n2\n",
    "print(n1_w, n2_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4dcbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7edece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_few_shot_experiment(\n",
    "        train_datasets,\n",
    "        dataset_name,\n",
    "        fewshot_datasets,\n",
    "        fewshot_dataset_name,\n",
    "        eval_datasets,\n",
    "        features,\n",
    "        target,\n",
    "        nruns=5,\n",
    "        seed=81723,\n",
    "        model_class=LinearRegression,\n",
    "        model_hparams={},\n",
    "        pipeline=None,\n",
    "        pretrain_pct: List[float] = [1],\n",
    "        fewshot_n_examples: List[float] = None,\n",
    "        fewshot_weights: List[float]=[None],\n",
    "    ):\n",
    "    from itertools import product\n",
    "    rand = np.random.default_rng(seed)\n",
    "    seed_fn = lambda rand: int(rand.integers(10**6))\n",
    "    \n",
    "    pretrain_data = train_datasets[dataset_name]\n",
    "    pretrain_n_examples = [len(pretrain_data) * n for n in pretrain_pct]\n",
    "    pretrain_n_examples = [int(round(n, 0)) for n in pretrain_n_examples]\n",
    "\n",
    "    fewshot_data = fewshot_datasets[fewshot_dataset_name]\n",
    "    print(len(fewshot_data))\n",
    "    fewshot_n_examples = [n for n in fewshot_n_examples if n <= len(fewshot_data)]\n",
    "    \n",
    "    print(\"#PT:\", pretrain_n_examples)\n",
    "    print(\"#FS:\", fewshot_n_examples)\n",
    "    print(\"FS Weights:\", fewshot_weights)\n",
    "\n",
    "    all_results = []\n",
    "    all_pipelines = []\n",
    "    combinations = list(product(pretrain_n_examples, fewshot_n_examples, fewshot_weights))\n",
    "    \n",
    "    print(f\"About to run {len(combinations)}...\")\n",
    "    for i, (pretrain_n, fewshot_n, fewshot_w) in enumerate(combinations):\n",
    "        for j in range(nruns):\n",
    "            try:\n",
    "                # Compute the pretrain pct\n",
    "                pretrain_fraction = pretrain_data.sample(n=pretrain_n, replace=False, random_state=seed_fn(rand))\n",
    "\n",
    "                # Compute pct of fewshot\n",
    "                fewshot_fraction =  fewshot_data.sample(n=fewshot_n, replace=False, random_state=seed_fn(rand))\n",
    "\n",
    "                # Get subset of few shot data:\n",
    "                if pipeline is None:\n",
    "                    pipeline = FewShotPipeline\n",
    "\n",
    "                # Create pipeline\n",
    "                fs_pipeline = pipeline(\n",
    "                    fewshot_dataset=fewshot_dataset_name,\n",
    "                    fewshot_weight=fewshot_w,\n",
    "                    model_class=model_class,\n",
    "                    model_hparams=model_hparams,\n",
    "                    dataset=dataset_name,\n",
    "                    features=features,\n",
    "                    target=target,\n",
    "                    seed=seed_fn(rand),\n",
    "                )\n",
    "\n",
    "                # -------------------------------------------------------\n",
    "                # Load data + Fit BASE TRAIN + FEWSHOT\n",
    "                # -------------------------------------------------------\n",
    "                fs_pipeline.load_data(pretrain_fraction, fewshot_data=fewshot_fraction)\n",
    "                fs_pipeline.fewshot_fit()\n",
    "\n",
    "                # Evaluate results\n",
    "                results = fs_pipeline.evaluate_multiple(eval_datasets)\n",
    "\n",
    "                for r in results:\n",
    "                    r[\"i\"] = i\n",
    "                    r[\"seed\"] = seed_fn(rand)\n",
    "                    r[\"pretrain_n\"] = pretrain_n\n",
    "                    r[\"pretrain_pct\"] = round(pretrain_n / len(pretrain_data), 2)\n",
    "                    r[\"fewshot_n\"] = fewshot_n\n",
    "                    r[\"fewshot_weight\"] = fewshot_w if fewshot_w is not None else \"uniform\"\n",
    "\n",
    "                all_results.extend(results)\n",
    "                all_pipelines.append(fs_pipeline)\n",
    "            except:\n",
    "                print(\"Failed for combination:\", pretrain_n, fewshot_n, fewshot_w)\n",
    "            \n",
    "    return all_results, all_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd69c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model: \n",
    "    def __init__(self):\n",
    "        self.name = None\n",
    "        self.classpath = None\n",
    "        self.pipeline = None\n",
    "        self.nruns = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.name}_{self.nruns}\"\n",
    "        \n",
    "\n",
    "model = Model()\n",
    "# model.name, model.classpath, model.hparams, model.nruns = \"lr\", LinearRegression, {}, 10\n",
    "# model.name, model.classpath, model.hparams, model.nruns = \"lgbm\", LGBMRegressor, {\"random_state\": 113}, 10\n",
    "model.name, model.classpath, model.hparams, model.nruns = \"rf\", RandomForestRegressor, {\"n_jobs\": 10}, 3\n",
    "# model.name, model.classpath, model.hparams, model.nruns, model.pipeline = \"mlp\", MLPRegressor, {\"learning_rate\": \"adaptive\", \"random_state\": 42, \"early_stopping\": True}, 5, FineTuningFewShotPipeline\n",
    "\n",
    "\n",
    "METRICS = [\n",
    "    # Bleu\n",
    "    'bleu1', 'bleu2', 'bleu3', 'bleu4', \n",
    "    # 'hf_bleu1', 'hf_bleu2', 'hf_bleu3', 'hf_bleu4', \n",
    "    'rougeL', \n",
    "    # 'hf_rougeL', 'hf_rougeLsum',\n",
    "    'hf_rouge1', 'hf_rouge2',\n",
    "    'meteor',\n",
    "    'recall', 'precision', 'f1_score',\n",
    "    'sari_context', 'sari_question',\n",
    "    # Token overlap when 1st error occurred\n",
    "    'precision_at_err1', 'recall_at_err1',\n",
    "    # Confusion matrix\n",
    "    'tp', 'fn', 'fp',\n",
    "    # Edit scores ------\n",
    "    'char_edit_score', 'word_edit_score',\n",
    "    # Learned metrics -------\n",
    "    'bertscore', \n",
    "    'bleurt',\n",
    "    \"LERC\",\n",
    "    # Input statistics ------\n",
    "    'candidatelength_word', 'candidatelength_char',\n",
    "    'candidatenunique_words', 'referencelength_word',\n",
    "    'referencelength_char', 'referencenunique_words',\n",
    "    'contextlength_word', 'contextlength_char',\n",
    "    'contextnunique_words', 'questionlength_word',\n",
    "    'questionlength_char', 'questionnunique_words',\n",
    "]\n",
    "\n",
    "if \"bleurt\" not in METRICS:\n",
    "    model.name += '_no_bleurt'\n",
    "if \"bertscore\" not in METRICS:\n",
    "    model.name += \"_no_bertscore\"\n",
    "if \"LERC\" in METRICS:\n",
    "    model.name += \"_with_LERC\"\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ee868",
   "metadata": {},
   "source": [
    "### Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHAS = [None, 0.0, 0.10, 0.5, 0.90]\n",
    "FEWSHOT_WEIGHTS = [round(1-alpha, 2) if isinstance(alpha, (int, float)) else alpha for alpha in ALPHAS]\n",
    "PRETRAIN_PCTS = [1]\n",
    "FEWSHOT_N = [0, 1, 2, 3, 4, 5, 8, 12, 16, 24, 32, 48, 64, 96, 128, 200, 256, 320, 512, 1024]\n",
    "\n",
    "print(FEWSHOT_WEIGHTS, PRETRAIN_PCTS, FEWSHOT_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9bbbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in [\"narrativeqa\", \"mcscript\"]:\n",
    "    loo_fewshot, loo_ps =  run_few_shot_experiment(\n",
    "        train_datasets=TRAIN_LOO_DATASETS,\n",
    "        dataset_name=f\"except_{dataset}\",\n",
    "        fewshot_datasets=FEW_SHOT_LOO_DATASETS,\n",
    "        fewshot_dataset_name=dataset,\n",
    "        eval_datasets=DEV_LOO_DATASETS,\n",
    "        features=METRICS,\n",
    "        target=TARGET,\n",
    "        nruns=model.nruns,\n",
    "        seed=81723,\n",
    "        model_class=model.classpath,\n",
    "        model_hparams=model.hparams,\n",
    "        pipeline=model.pipeline,\n",
    "        # ----------------------------------------\n",
    "        # few shot parameters\n",
    "        # ----------------------------------------\n",
    "        pretrain_pct=PRETRAIN_PCTS,\n",
    "        fewshot_n_examples=FEWSHOT_N,\n",
    "        fewshot_weights=FEWSHOT_WEIGHTS,\n",
    "    )\n",
    "    loo_results = pd.DataFrame(loo_fewshot)\n",
    "    loo_results.fewshot_weight = loo_results.fewshot_weight.fillna(\"default\")\n",
    "    \n",
    "    loo_results.to_csv(f\"{RESULTS_DIR}/{dataset}_{model}_weighting.csv\")\n",
    "    joblib.dump(loo_ps, f\"{RESULTS_DIR}/{dataset}_{model}_weighting.pipelines\")\n",
    "    \n",
    "    del loo_fewshot\n",
    "    del loo_ps\n",
    "    del loo_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e2026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f07ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./experiments_20220614/results/few-shot/narrativeqa_lr_with_LERC_10_weighting.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df[(df.fewshot_weight == \"1.0\") & (df.evaluated_on == \"narrativeqa\")]\n",
    "\n",
    "sns.lineplot(x=d.fewshot_n, y=d[\"pearson\"])\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug - few-shot only\n",
    "data = FEW_SHOT_LOO_DATASETS[\"narrativeqa\"]\n",
    "X, y = data[METRICS], data[TARGET]\n",
    "\n",
    "dev = DEV_LOO_DATASETS[\"narrativeqa\"]\n",
    "X_dev, y_dev = dev[METRICS], dev[TARGET]\n",
    "\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    for n in FEWSHOT_N[1:]:\n",
    "        # Model 1. Fit two instances\n",
    "        ix_n = np.random.choice(np.arange(len(y)), size=n, replace=False)\n",
    "        X_n, y_n = X.iloc[ix_n].copy(), y.iloc[ix_n].copy()\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_n, y_n)\n",
    "\n",
    "        preds = model.predict(X_dev)\n",
    "\n",
    "        import scipy.stats as st\n",
    "        results.append(st.pearsonr(preds, y_dev)[0])\n",
    "        \n",
    "        \n",
    "sns.lineplot(x=FEWSHOT_N[1:] * 10, y=results)\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31017533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "train_data = TRAIN_LOO_DATASETS[\"except_narrativeqa\"]\n",
    "X_train, y_train = train_data[[\"LERC\"] + METRICS], train_data[TARGET]\n",
    "\n",
    "data = FEW_SHOT_LOO_DATASETS[\"narrativeqa\"]\n",
    "X, y = data[[\"LERC\"] + METRICS], data[TARGET]\n",
    "\n",
    "dev = DEV_LOO_DATASETS[\"narrativeqa\"]\n",
    "X_dev, y_dev = dev[[\"LERC\"] + METRICS], dev[TARGET]\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    for n in FEWSHOT_N[0:]:\n",
    "        # Model 1. Fit two instances\n",
    "        ix_n = np.random.choice(np.arange(len(y)), size=n, replace=False)\n",
    "        X_n, y_n = X.iloc[ix_n].copy(), y.iloc[ix_n].copy()\n",
    "        \n",
    "        X_n = pd.concat((X_train, X_n))\n",
    "        y_n = pd.concat((y_train, y_n))\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_n, y_n)\n",
    "\n",
    "        preds = model.predict(X_dev)\n",
    "\n",
    "        import scipy.stats as st\n",
    "        results.append(st.pearsonr(preds, y_dev)[0])\n",
    "  \n",
    "    \n",
    "sns.lineplot(x=FEWSHOT_N[0:] * 10, y=results)\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "train_data = TRAIN_LOO_DATASETS[\"except_narrativeqa\"]\n",
    "X_train, y_train = train_data[[\"LERC\"] + METRICS], train_data[TARGET]\n",
    "\n",
    "data = FEW_SHOT_LOO_DATASETS[\"narrativeqa\"]\n",
    "X, y = data[[\"LERC\"] + METRICS], data[TARGET]\n",
    "\n",
    "dev = DEV_LOO_DATASETS[\"narrativeqa\"]\n",
    "X_dev, y_dev = dev[[\"LERC\"] + METRICS], dev[TARGET]\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    for n in FEWSHOT_N[1:]:\n",
    "        # Model 1. Fit two instances\n",
    "        ix_n = np.random.choice(np.arange(len(y)), size=n, replace=False)\n",
    "        X_n, y_n = X.iloc[ix_n].copy(), y.iloc[ix_n].copy()\n",
    "        \n",
    "        X_n = pd.concat((X_train, X_n))\n",
    "        y_n = pd.concat((y_train, y_n))\n",
    "        \n",
    "        weights = np.ones(len(X_n))\n",
    "        weights[:len(y_train)] *= 0.10\n",
    "        weights[len(y_train):] *= 0.9\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_n, y_n, sample_weight=weights)\n",
    "\n",
    "        preds = model.predict(X_dev)\n",
    "\n",
    "        import scipy.stats as st\n",
    "        results.append(st.pearsonr(preds, y_dev)[0])\n",
    "  \n",
    "    \n",
    "sns.lineplot(x=FEWSHOT_N[1:] * 10, y=results)\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "train_data = TRAIN_LOO_DATASETS[\"except_narrativeqa\"]\n",
    "X_train, y_train = train_data[[\"LERC\"] + METRICS], train_data[TARGET]\n",
    "\n",
    "data = FEW_SHOT_LOO_DATASETS[\"narrativeqa\"]\n",
    "X, y = data[[\"LERC\"] + METRICS], data[TARGET]\n",
    "\n",
    "dev = DEV_LOO_DATASETS[\"narrativeqa\"]\n",
    "X_dev, y_dev = dev[[\"LERC\"] + METRICS], dev[TARGET]\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    for n in FEWSHOT_N[1:]:\n",
    "        # Model 1. Fit two instances\n",
    "        ix_n = np.random.choice(np.arange(len(y)), size=n, replace=False)\n",
    "        X_n, y_n = X.iloc[ix_n].copy(), y.iloc[ix_n].copy()\n",
    "        \n",
    "        X_n = pd.concat((X_train, X_n))\n",
    "        y_n = pd.concat((y_train, y_n))\n",
    "        \n",
    "        weights = np.ones(len(X_n))\n",
    "        weights[:len(y_train)] = len(X_n) * 0.10 / len(y_train)\n",
    "        weights[len(y_train):] = len(X_n) * 0.9 / len(n)\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_n, y_n, sample_weight=weights)\n",
    "\n",
    "        preds = model.predict(X_dev)\n",
    "\n",
    "        import scipy.stats as st\n",
    "        results.append(st.pearsonr(preds, y_dev)[0])\n",
    "  \n",
    "    \n",
    "sns.lineplot(x=FEWSHOT_N[1:] * 10, y=results)\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9bcc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "919566e5",
   "metadata": {},
   "source": [
    "### Debug Few shot pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230af75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e309939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = lgb.LGBMRegressor(random_state=113)\n",
    "model.fit(X_2, y_2)\n",
    "\n",
    "preds = model.predict(X_dev)\n",
    "\n",
    "import scipy.stats as st\n",
    "st.pearsonr(preds, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2. Fit all instances but assign them a weight of 0\n",
    "weights = np.zeros(len(y))\n",
    "weights[ix_2] = len(y)/len(ix_2)\n",
    "\n",
    "model = lgb.LGBMRegressor(random_state=113)\n",
    "model.fit(X, y, sample_weight=weights)\n",
    "\n",
    "preds = model.predict(X_dev)\n",
    "\n",
    "import scipy.stats as st\n",
    "st.pearsonr(preds, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665f58d",
   "metadata": {},
   "source": [
    "### Sampling only (default weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHAS = [None]\n",
    "FEWSHOT_WEIGHTS = [1-alpha if isinstance(alpha, float) else alpha for alpha in ALPHAS]\n",
    "PRETRAIN_PCTS = [0.10, 0.20, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "FEWSHOT_N = [0, 1, 2, 3, 4, 5, 8, 12, 16, 24, 36, 48, 64, 96, 128, 200, 256, 512, 1024]\n",
    "\n",
    "for dataset in [\"narrativeqa\", \"mcscript\"]:\n",
    "# for dataset in DATASETS:\n",
    "    print(\"Experiment for dataset\", dataset)\n",
    "    loo_fewshot, loo_ps =  run_few_shot_experiment(\n",
    "        train_datasets=TRAIN_LOO_DATASETS,\n",
    "        dataset_name=f\"except_{dataset}\",\n",
    "        fewshot_datasets=TRAIN_DATASETS,\n",
    "        fewshot_dataset_name=dataset,\n",
    "        eval_datasets=DEV_DATASETS,\n",
    "        features=METRICS,\n",
    "        target=TARGET,\n",
    "        nruns=model.nruns,\n",
    "        seed=81723,\n",
    "        model_class=model.classpath,\n",
    "        model_hparams=model.hparams,\n",
    "        pipeline=model.pipeline,\n",
    "        # ----------------------------------------\n",
    "        # few shot parameters\n",
    "        # ----------------------------------------\n",
    "        pretrain_pct=PRETRAIN_PCTS,\n",
    "        fewshot_n_examples=FEWSHOT_N,\n",
    "        fewshot_weights=FEWSHOT_WEIGHTS,\n",
    "    )\n",
    "    loo_results = pd.DataFrame(loo_fewshot)\n",
    "    loo_results.fewshot_weight = loo_results.fewshot_weight.fillna(\"default\")\n",
    "    \n",
    "    _results_dir = f\"{RESULTS_DIR}/few-shot\"\n",
    "    os.makedirs(_results_dir, exist_ok=True)\n",
    "    loo_results.to_csv(f\"{_results_dir}/{dataset}_{model}_sampling.csv\")\n",
    "    joblib.dump(loo_ps, f\"{_results_dir}/{dataset}_{model}_sampling.pipelines\")\n",
    "    \n",
    "    del loo_fewshot\n",
    "    del loo_ps\n",
    "    del loo_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822051a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b594b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec1d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c80b2e871d42739fd0f1d2ac9c8f31a15028627c2324c1cda8de503e9f52a70d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
