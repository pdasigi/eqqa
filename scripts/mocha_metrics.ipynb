{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdeca9b1",
   "metadata": {},
   "source": [
    "To run this script, download the [MOCHA.tar.gz](https://github.com/anthonywchen/MOCHA/blob/main/data/mocha.tar.gz) and extract it to `data/mocha`. This script will use the following frameworks:\n",
    "- `SPACY` for tokenization;\n",
    "- HuggingFace `datasets` for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04aa7630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "import json\n",
    "\n",
    "# Tokenization\n",
    "import spacy\n",
    "\n",
    "# Evaluation\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efda148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the (lowercase) name of the dataset to run the analysis for\n",
    "DATASET = \"narrativeqa\"\n",
    "\n",
    "# Data directory: where to look for a model\n",
    "DATA_DIR = \"../data/mocha/\"\n",
    "\n",
    "# Full filepath to load\n",
    "FILEPATH = f\"{DATA_DIR}/dev.json\"\n",
    "\n",
    "\n",
    "# ouput directory\n",
    "OUTPUT_DIR = \"../output/predictions\"\n",
    "\n",
    "# Evaluation tokenizer\n",
    "EVALUATION_TOKENIZER = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7caefbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0005c7718ff653683df879622efb02d1',\n",
       " {'candidate': 'his distant relative pascal rougon',\n",
       "  'context': \"The plot centres on the neurotic young priest Serge Mouret, first seen in La ConquĂŞte de Plassans, as he takes his orders and becomes the parish priest for the uninterested village of Artauds. The inbred villagers have no interest in religion and Serge is portrayed giving several wildly enthusiastic Masses to his completely empty, near-derelict church. Serge not only seems unperturbed by this state of affairs but actually appears to have positively sought it out especially, for it gives him time to contemplate religious affairs and to fully experience the fervour of his faith. Eventually he has a complete nervous breakdown and collapses into a near-comatose state, whereupon his distant relative, the unconventional doctor Pascal Rougon (the central character of the last novel in the series, 1893's Le Docteur Pascal), places him in the care of the inhabitants of a nearby derelict stately home, Le Paradou. The novel then takes a complete new direction in terms of both tone and style, as Serge - suffering from amnesia and total long-term memory loss, with no idea who or where he is beyond his first name - is doted upon by Albine, the whimsical, innocent and entirely uneducated girl who has been left to grow up practically alone and wild in the vast, sprawling, overgrown grounds of Le Paradou. The two of them live a life of idyllic bliss with many Biblical parallels, and over the course of a number of months, they fall deeply in love with one another; however, at the moment they consummate their relationship, they are discovered by Serge's monstrous former monseignor and his memory is instantly returned to him. Wracked with guilt at his unwitting sins, Serge is plunged into a deeper religious fervour than ever before, and poor Albine is left bewildered at the loss of her soulmate. As with many of Zola's earlier works, the novel then builds to a horrible climax.\",\n",
       "  'metadata': {'scores': [5, 5, 4], 'source': 'mhpg'},\n",
       "  'question': 'Who put Serge Mouret in a home care?',\n",
       "  'reference': 'Le Docteur Pascal',\n",
       "  'score': 4.666666666666667})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(FILEPATH))\n",
    "data = data[DATASET] if DATASET is not None else data\n",
    "print(\"Number of examples:\", len(data))\n",
    "next(iter(data.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37f5b7",
   "metadata": {},
   "source": [
    "### Token overlap metrics\n",
    "\n",
    "Current QA evaluation relies on string matching or token overlap metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20bd04a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/kat/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Couldn't find a directory or a metric named 'exact_match' in this version. It was picked from the master branch on github instead.\n"
     ]
    }
   ],
   "source": [
    "BLEU = datasets.load_metric(\"bleu\")\n",
    "ROUGE = datasets.load_metric(\"rouge\") \n",
    "#^Note: requires installing rouge-score (!pip install rouge-score)\n",
    "\n",
    "METEOR = datasets.load_metric(\"meteor\")\n",
    "EXACT_MATCH = datasets.load_metric(\"exact_match\")\n",
    "\n",
    "BERT_SCORE = datasets.load_metric(\"bertscore\")\n",
    "#^Note: requires installing bert-score: https://pypi.org/project/bert-score/\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Union\n",
    "import logging\n",
    "\n",
    "\n",
    "Tokens = List[str]\n",
    "Text = Union[str, Tokens]\n",
    "\n",
    "\n",
    "def exact_match(y_true: Text, y_pred: Text) -> int:\n",
    "    \"\"\"Determine whether two texts (or sequences of tokens) are equal.\"\"\"\n",
    "    if isinstance(y_true, str) and isinstance(y_pred, str):\n",
    "        return int(y_true == y_pred)\n",
    "\n",
    "    elif isinstance(y_true, (list, tuple)) and isinstance(y_pred, (list, tuple)):\n",
    "        if len(y_true) != len(y_pred):\n",
    "            logging.debug(\n",
    "                f\"Dimension mismatch (default value is 0): {y_true} vs {y_pred}\"\n",
    "            )\n",
    "            return 0\n",
    "        return int(all(map(lambda t1, t2: t1 == t2, y_true, y_pred)))\n",
    "    else:\n",
    "        error_msg = f\"y_true ({type(y_true)}) and y_pred ({type(y_pred)})\"\n",
    "        raise ValueError(\n",
    "            f\"Cannot compare `exact_match` for argument types: {error_msg}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def first_error_position(y_true: Tokens, y_pred: Tokens, no_err_val: int = None) -> int:\n",
    "    \"\"\"Determine the position in the predicted sequence of the first error.\n",
    "    Notes\n",
    "    -----\n",
    "    If both text sequences are equivalent we return ``no_err_val`` as the position.\n",
    "    Otherwise, we iterate for each token in ``y_pred`` and look for the first\n",
    "    mismatch between ``y_pred`` and ``y_true`` tokens returning that position.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> y_true = [\"The\", \"sky\", \"is\", \"blue\"]\n",
    "    >>> y_pred = [\"A\", \"sky\", \"is\", \"blue\"]\n",
    "    >>> first_error_position(y_true, y_pred)\n",
    "    1\n",
    "    >>> y_pred = [\"The\", \"sky\", \"IS\", \"blue\"]\n",
    "    >>> first_error_position(y_true, y_pred)\n",
    "    3\n",
    "    >>> first_error_position(y_true, y_true, no_err_val=-1)\n",
    "    -1\n",
    "    \"\"\"\n",
    "    assert isinstance(y_true, (list, tuple)) and len(y_true) != 0\n",
    "    assert isinstance(y_pred, (list, tuple)) and len(y_pred) != 0\n",
    "\n",
    "    # When no error occurs return the `no_err_val`\n",
    "    if exact_match(y_true, y_pred):\n",
    "        return no_err_val\n",
    "\n",
    "    # If there are differences then they are one of two types:\n",
    "    # 1. Token mismatch: which will occur in the common length of\n",
    "    # the two sequences. Values can vary between 0 and min(lengths)\n",
    "    # 2. Misnumber of tokens: one of the sequences is longer than the\n",
    "    # other, causing them to be wrong.\n",
    "    max_mismatch_ix = min(len(y_true), len(y_pred))\n",
    "\n",
    "    for i in range(max_mismatch_ix):\n",
    "        if y_true[i] != y_pred[i]:\n",
    "            return i\n",
    "    return max_mismatch_ix\n",
    "\n",
    "\n",
    "def _precision(tp, fp, tn, fn) -> float:\n",
    "    return 0 if tp == 0 else tp / (tp + fp)\n",
    "\n",
    "\n",
    "def _recall(tp, fp, tn, fn) -> float:\n",
    "    return 0 if tp == 0 else tp / (tp + fn)\n",
    "\n",
    "\n",
    "def _critical_success_index(tp, fp, tn, fn):\n",
    "    \"Ratio of positives w.r.t. number of errors (also dubbed threat score).\"\n",
    "    return 0 if tp == 0 else tp / (tp + fn + fp)\n",
    "\n",
    "\n",
    "def _f1_score(precision=None, recall=None, **kwargs) -> float:\n",
    "    if precision is not None and recall is not None:\n",
    "        p = precision\n",
    "        r = recall\n",
    "        # return if precision or recall are 0\n",
    "        if p == 0 or r == 0:\n",
    "            return 0\n",
    "    else:\n",
    "        p = _precision(**kwargs)\n",
    "        r = _recall(**kwargs)\n",
    "\n",
    "    return (2 * p * r) / (p + r)\n",
    "\n",
    "\n",
    "def f_metrics(references: Tokens, predictions: Tokens) -> Dict[str, float]:\n",
    "    true_tokens, pred_tokens = Counter(references), Counter(predictions)\n",
    "    tp = sum((true_tokens & pred_tokens).values())\n",
    "    fp = len(predictions) - tp\n",
    "    fn = len(references) - tp\n",
    "    tn = 0\n",
    "    assert tp + fp + fn == sum((true_tokens | pred_tokens).values())\n",
    "\n",
    "    prec = _precision(tp=tp, fp=fp, tn=tn, fn=fn)\n",
    "    rec = _recall(tp=tp, fp=fp, tn=tn, fn=fn)\n",
    "    return {\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1_score\": _f1_score(precision=prec, recall=rec),\n",
    "        \"csi\": _critical_success_index(tp=tp, fp=fp, tn=tn, fn=fn),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a93a4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts, tokens=True, sep: str = \" \", tokenizer=EVALUATION_TOKENIZER) -> list:\n",
    "    arg_is_str = isinstance(texts, str)\n",
    "    if arg_is_str:\n",
    "        texts = [texts]\n",
    "\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        text_tokens = tokenizer(text)\n",
    "        text_tokens = [str(t) for t in text_tokens]\n",
    "        results.append(text_tokens if tokens else [sep.join(text_tokens)])\n",
    "\n",
    "    return results[0] if arg_is_str else results\n",
    "\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    import re, string\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "assert tokenize(\"Hello, world!\") == ['Hello', ',', 'world', '!']\n",
    "assert tokenize([\"Hello, world!\"]) == [['Hello', ',', 'world', '!']]\n",
    "assert tokenize([\"Hello, world!\", \"Ola, mundo!\"]) == [['Hello', ',', 'world', '!'], [\"Ola\", \",\", \"mundo\", \"!\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84c02bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(data):\n",
    "    references = {example_id: (example[\"reference\"]) for example_id, example in data.items()}\n",
    "    candidates = {example_id: (example[\"candidate\"]) for example_id, example in data.items()}\n",
    "    \n",
    "    return references, candidates\n",
    "\n",
    "\n",
    "def apply(d: dict, fn):\n",
    "    return {k: fn(elem) for k, elem in d.items()}\n",
    "\n",
    "\n",
    "# Separate data in references and candidates\n",
    "references, candidates = get_examples(data)\n",
    "assert len(references) == len(candidates)\n",
    "\n",
    "# Compute metrics that rely on strings ()\n",
    "references_norm = apply(references, fn=normalize_answer)\n",
    "candidates_norm = apply(candidates, fn=normalize_answer)\n",
    "\n",
    "# Tokenize data\n",
    "references_tokens = apply(references_norm, fn=tokenize)\n",
    "candidates_tokens = apply(candidates_norm, fn=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22109a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(**kwargs):\n",
    "    results = ROUGE.compute(**kwargs)\n",
    "    return {rouge_type: rouge.mid.fmeasure for rouge_type, rouge in results.items()}\n",
    "\n",
    "def compute_bleu(max_order=4, **kwargs):\n",
    "    bleu = {}\n",
    "    for i in range(1, max_order+1):\n",
    "        results = BLEU.compute(**kwargs, max_order=i)[\"bleu\"]\n",
    "        bleu[f\"bleu{i}\"] = results\n",
    "        \n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb71c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results = []\n",
    "\n",
    "for k in references.keys():\n",
    "    reference = references_norm[k]\n",
    "    candidate = candidates_norm[k]\n",
    "\n",
    "    reference_tks = references_tokens[k]\n",
    "    candidate_tks = candidates_tokens[k]    \n",
    "\n",
    "    metrics = {\n",
    "        \"example_id\": k, \n",
    "        \"reference\": reference,\n",
    "        \"candidate\": candidate,\n",
    "        \n",
    "        \"reference_tokens\": reference_tks,\n",
    "        \"candidate_tokens\": candidate_tks,\n",
    "    }\n",
    "    \n",
    "    text_args = {\"predictions\": [candidate], \"references\": [reference]}\n",
    "    \n",
    "    metrics.update(text_args)\n",
    "    metrics.update(EXACT_MATCH.compute(**text_args))\n",
    "    metrics.update(METEOR.compute(**text_args))\n",
    "    metrics.update(compute_rouge(**text_args))\n",
    "    \n",
    "    # keyword arguments for BLEU-like metrics\n",
    "    token_args = {\"predictions\": [candidate_tks], \"references\": [[reference_tks]]}\n",
    "    metrics.update(compute_bleu(**token_args))\n",
    "    \n",
    "    token_args = {\"predictions\": candidate_tks, \"references\": reference_tks}\n",
    "    metrics.update(f_metrics(**token_args))\n",
    "    \n",
    "    \n",
    "    break\n",
    "    metric_results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2df93e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_id': '0005c7718ff653683df879622efb02d1',\n",
       " 'reference': 'le docteur pascal',\n",
       " 'candidate': 'his distant relative pascal rougon',\n",
       " 'reference_tokens': ['le', 'docteur', 'pascal'],\n",
       " 'candidate_tokens': ['his', 'distant', 'relative', 'pascal', 'rougon'],\n",
       " 'predictions': ['his distant relative pascal rougon'],\n",
       " 'references': ['le docteur pascal'],\n",
       " 'exact_match': 0.0,\n",
       " 'meteor': 0.15625,\n",
       " 'rouge1': 0.25,\n",
       " 'rouge2': 0.0,\n",
       " 'rougeL': 0.25,\n",
       " 'rougeLsum': 0.25,\n",
       " 'bleu1': 0.2,\n",
       " 'bleu2': 0.0,\n",
       " 'bleu3': 0.0,\n",
       " 'bleu4': 0.0,\n",
       " 'precision': 0.2,\n",
       " 'recall': 0.3333333333333333,\n",
       " 'f1_score': 0.25,\n",
       " 'csi': 0.14285714285714285}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
