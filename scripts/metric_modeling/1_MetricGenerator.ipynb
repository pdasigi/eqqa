{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31148be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_datasets import read_json_dataset, write_json_dataset\n",
    "from dict_utils import update_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b255dd6",
   "metadata": {},
   "source": [
    "## Preprocess datasets for metric modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beda30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT_DIR = \"../..\"\n",
    "DATASET_RAW_DIR = f\"{ROOT_DIR}/data/raw_splits\"\n",
    "\n",
    "DATASET_PREPROC_DIR = f\"{ROOT_DIR}/data/preprocessing\"\n",
    "os.makedirs(DATASET_PREPROC_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "FEATURES = [\n",
    "    'char_edit_score',\n",
    "    'word_edit_score',\n",
    "    'recall', \n",
    "    'tp', \n",
    "    'fn',\n",
    "    'precision',\n",
    "    'fp',\n",
    "    'f1_score',\n",
    "    'sari_context',\n",
    "    'sari_question',\n",
    "    'bleu1',\n",
    "    'bleu2',\n",
    "    'bleu3',\n",
    "    'bleu4',\n",
    "    'hf_bleu1', \n",
    "    'hf_bleu2',\n",
    "    'hf_bleu3',\n",
    "    'hf_bleu4',\n",
    "    'rougeL',\n",
    "    'hf_rouge1',\n",
    "    'hf_rouge2',\n",
    "    'hf_rougeL',\n",
    "    'hf_rougeLsum',\n",
    "    'precision_at_err1',\n",
    "    'recall_at_err1',\n",
    "    'meteor',\n",
    "    'bertscore',\n",
    "    'bleurt',\n",
    "    'wmd',\n",
    "]\n",
    "\n",
    "TARGET = 'score'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90803845",
   "metadata": {},
   "source": [
    "### Preprocess AD datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d13d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a16c5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(data: dict) -> pd.DataFrame:\n",
    "    return pd.DataFrame.from_dict(data).T\n",
    "\n",
    "\n",
    "def cast_types(df: pd.DataFrame, features: list, label: str) -> pd.DataFrame:\n",
    "    print(\"Loaded\", len(df), \"examples\")\n",
    "\n",
    "    for feat in features + [label]:\n",
    "        df[feat] = df[feat].astype(float)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def imputation(df: pd.DataFrame, features: list) -> pd.DataFrame:\n",
    "    \"\"\"Currently only accounts for inf.\"\"\"    \n",
    "    \n",
    "    for feature in features:\n",
    "        # Currently, we do not support missing data imputation\n",
    "        assert not df[feature].isna().any(), f\"NaN found for feature: {feature}\"\n",
    "        \n",
    "        # For now, we will drop the infinite instances\n",
    "        valid_mask = df[feature].abs() != np.inf\n",
    "        df = df[valid_mask]\n",
    "\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def standardize(data, mean, std):\n",
    "    return (data - mean) / std\n",
    "\n",
    "\n",
    "def min_max_scaling(data, min, max):\n",
    "    return (data - min) / (max-min)\n",
    "\n",
    "\n",
    "def whitening(df: pd.DataFrame, features, params=None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    transform_params = params if params is not None else {}\n",
    "    \n",
    "    for feature in features:\n",
    "        if params is None:\n",
    "            transform_params[feature] = {\n",
    "                \"mean\": df[feature].mean(),\n",
    "                \"std\": df[feature].std(),\n",
    "            }\n",
    "        \n",
    "        df[feature] = standardize(df[feature], **transform_params[feature])\n",
    "        \n",
    "    return df, transform_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1a272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31068 examples\n",
      "dev 4007\n",
      "Loaded 4007 examples\n",
      "dev 3897\n",
      "test 6321\n",
      "Loaded 6321 examples\n",
      "test 6161\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "      <th>hf_bleu1</th>\n",
       "      <th>hf_bleu2</th>\n",
       "      <th>hf_bleu3</th>\n",
       "      <th>hf_bleu4</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>...</th>\n",
       "      <th>precision_at_err1</th>\n",
       "      <th>recall_at_err1</th>\n",
       "      <th>char_edit_score</th>\n",
       "      <th>word_edit_score</th>\n",
       "      <th>sari_context</th>\n",
       "      <th>sari_question</th>\n",
       "      <th>bertscore</th>\n",
       "      <th>bleurt</th>\n",
       "      <th>wmd</th>\n",
       "      <th>score_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30274.000000</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>30274.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.439354</td>\n",
       "      <td>-4.130790e-17</td>\n",
       "      <td>1.595987e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.510526e-18</td>\n",
       "      <td>9.388158e-19</td>\n",
       "      <td>-2.534803e-17</td>\n",
       "      <td>2.816447e-18</td>\n",
       "      <td>-5.632895e-18</td>\n",
       "      <td>-4.694079e-18</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.065395e-17</td>\n",
       "      <td>-2.910329e-17</td>\n",
       "      <td>6.102303e-18</td>\n",
       "      <td>-2.769507e-17</td>\n",
       "      <td>-6.792332e-16</td>\n",
       "      <td>-3.924250e-16</td>\n",
       "      <td>-5.149405e-16</td>\n",
       "      <td>1.288055e-15</td>\n",
       "      <td>-1.032697e-17</td>\n",
       "      <td>0.359838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.593652</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.398413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.618692e-01</td>\n",
       "      <td>-4.802753e-01</td>\n",
       "      <td>-0.304062</td>\n",
       "      <td>-2.028705e-01</td>\n",
       "      <td>-9.322605e-01</td>\n",
       "      <td>-5.203978e-01</td>\n",
       "      <td>-3.319088e-01</td>\n",
       "      <td>-2.232683e-01</td>\n",
       "      <td>-9.405319e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.347742e-01</td>\n",
       "      <td>-5.403622e-01</td>\n",
       "      <td>-4.489977e-01</td>\n",
       "      <td>-5.690573e-01</td>\n",
       "      <td>-2.815802e+00</td>\n",
       "      <td>-4.987661e+00</td>\n",
       "      <td>-3.390892e+00</td>\n",
       "      <td>-2.655677e+01</td>\n",
       "      <td>-1.936443e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.618692e-01</td>\n",
       "      <td>-4.802753e-01</td>\n",
       "      <td>-0.304062</td>\n",
       "      <td>-2.028705e-01</td>\n",
       "      <td>-9.322605e-01</td>\n",
       "      <td>-5.203978e-01</td>\n",
       "      <td>-3.319088e-01</td>\n",
       "      <td>-2.232683e-01</td>\n",
       "      <td>-9.405319e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.347742e-01</td>\n",
       "      <td>-5.403622e-01</td>\n",
       "      <td>-2.798836e-01</td>\n",
       "      <td>-3.329889e-01</td>\n",
       "      <td>-7.592215e-01</td>\n",
       "      <td>-7.699267e-01</td>\n",
       "      <td>-6.751251e-01</td>\n",
       "      <td>-3.899368e-01</td>\n",
       "      <td>-6.843684e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>-3.327049e-01</td>\n",
       "      <td>-4.802753e-01</td>\n",
       "      <td>-0.304062</td>\n",
       "      <td>-2.028705e-01</td>\n",
       "      <td>-2.778152e-01</td>\n",
       "      <td>-5.203978e-01</td>\n",
       "      <td>-3.319088e-01</td>\n",
       "      <td>-2.232683e-01</td>\n",
       "      <td>-2.521664e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.347742e-01</td>\n",
       "      <td>-5.403622e-01</td>\n",
       "      <td>-2.080101e-01</td>\n",
       "      <td>-2.149547e-01</td>\n",
       "      <td>-1.164469e-02</td>\n",
       "      <td>-1.075420e-01</td>\n",
       "      <td>-1.670626e-01</td>\n",
       "      <td>3.764280e-02</td>\n",
       "      <td>-1.122080e-02</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.624568e-01</td>\n",
       "      <td>-4.802752e-01</td>\n",
       "      <td>-0.304062</td>\n",
       "      <td>-2.028705e-01</td>\n",
       "      <td>6.630428e-01</td>\n",
       "      <td>-1.497835e-02</td>\n",
       "      <td>-3.319088e-01</td>\n",
       "      <td>-2.232683e-01</td>\n",
       "      <td>6.925558e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>7.921652e-02</td>\n",
       "      <td>7.075278e-02</td>\n",
       "      <td>-1.065416e-01</td>\n",
       "      <td>-9.692046e-02</td>\n",
       "      <td>6.970322e-01</td>\n",
       "      <td>7.073919e-01</td>\n",
       "      <td>5.913318e-01</td>\n",
       "      <td>4.743118e-01</td>\n",
       "      <td>6.716581e-01</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.411109e+00</td>\n",
       "      <td>4.568934e+00</td>\n",
       "      <td>6.360420</td>\n",
       "      <td>8.896545e+00</td>\n",
       "      <td>3.076227e+00</td>\n",
       "      <td>4.061226e+00</td>\n",
       "      <td>5.594109e+00</td>\n",
       "      <td>7.761579e+00</td>\n",
       "      <td>2.941398e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.535179e+00</td>\n",
       "      <td>2.515213e+00</td>\n",
       "      <td>4.744412e+01</td>\n",
       "      <td>3.838223e+01</td>\n",
       "      <td>2.377325e+00</td>\n",
       "      <td>2.475758e+00</td>\n",
       "      <td>2.289381e+00</td>\n",
       "      <td>3.336418e+00</td>\n",
       "      <td>3.388624e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              score         bleu1         bleu2         bleu3         bleu4  \\\n",
       "count  30274.000000  3.027400e+04  3.027400e+04  30274.000000  3.027400e+04   \n",
       "mean       2.439354 -4.130790e-17  1.595987e-17      0.000000 -7.510526e-18   \n",
       "std        1.593652  1.000000e+00  1.000000e+00      1.000000  1.000000e+00   \n",
       "min        1.000000 -8.618692e-01 -4.802753e-01     -0.304062 -2.028705e-01   \n",
       "25%        1.000000 -8.618692e-01 -4.802753e-01     -0.304062 -2.028705e-01   \n",
       "50%        2.000000 -3.327049e-01 -4.802753e-01     -0.304062 -2.028705e-01   \n",
       "75%        4.000000  5.624568e-01 -4.802752e-01     -0.304062 -2.028705e-01   \n",
       "max        5.000000  3.411109e+00  4.568934e+00      6.360420  8.896545e+00   \n",
       "\n",
       "           hf_bleu1      hf_bleu2      hf_bleu3      hf_bleu4        rougeL  \\\n",
       "count  3.027400e+04  3.027400e+04  3.027400e+04  3.027400e+04  3.027400e+04   \n",
       "mean   9.388158e-19 -2.534803e-17  2.816447e-18 -5.632895e-18 -4.694079e-18   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -9.322605e-01 -5.203978e-01 -3.319088e-01 -2.232683e-01 -9.405319e-01   \n",
       "25%   -9.322605e-01 -5.203978e-01 -3.319088e-01 -2.232683e-01 -9.405319e-01   \n",
       "50%   -2.778152e-01 -5.203978e-01 -3.319088e-01 -2.232683e-01 -2.521664e-01   \n",
       "75%    6.630428e-01 -1.497835e-02 -3.319088e-01 -2.232683e-01  6.925558e-01   \n",
       "max    3.076227e+00  4.061226e+00  5.594109e+00  7.761579e+00  2.941398e+00   \n",
       "\n",
       "       ...  precision_at_err1  recall_at_err1  char_edit_score  \\\n",
       "count  ...       3.027400e+04    3.027400e+04     3.027400e+04   \n",
       "mean   ...      -2.065395e-17   -2.910329e-17     6.102303e-18   \n",
       "std    ...       1.000000e+00    1.000000e+00     1.000000e+00   \n",
       "min    ...      -5.347742e-01   -5.403622e-01    -4.489977e-01   \n",
       "25%    ...      -5.347742e-01   -5.403622e-01    -2.798836e-01   \n",
       "50%    ...      -5.347742e-01   -5.403622e-01    -2.080101e-01   \n",
       "75%    ...       7.921652e-02    7.075278e-02    -1.065416e-01   \n",
       "max    ...       2.535179e+00    2.515213e+00     4.744412e+01   \n",
       "\n",
       "       word_edit_score  sari_context  sari_question     bertscore  \\\n",
       "count     3.027400e+04  3.027400e+04   3.027400e+04  3.027400e+04   \n",
       "mean     -2.769507e-17 -6.792332e-16  -3.924250e-16 -5.149405e-16   \n",
       "std       1.000000e+00  1.000000e+00   1.000000e+00  1.000000e+00   \n",
       "min      -5.690573e-01 -2.815802e+00  -4.987661e+00 -3.390892e+00   \n",
       "25%      -3.329889e-01 -7.592215e-01  -7.699267e-01 -6.751251e-01   \n",
       "50%      -2.149547e-01 -1.164469e-02  -1.075420e-01 -1.670626e-01   \n",
       "75%      -9.692046e-02  6.970322e-01   7.073919e-01  5.913318e-01   \n",
       "max       3.838223e+01  2.377325e+00   2.475758e+00  2.289381e+00   \n",
       "\n",
       "             bleurt           wmd  score_scaled  \n",
       "count  3.027400e+04  3.027400e+04  30274.000000  \n",
       "mean   1.288055e-15 -1.032697e-17      0.359838  \n",
       "std    1.000000e+00  1.000000e+00      0.398413  \n",
       "min   -2.655677e+01 -1.936443e+00      0.000000  \n",
       "25%   -3.899368e-01 -6.843684e-01      0.000000  \n",
       "50%    3.764280e-02 -1.122080e-02      0.250000  \n",
       "75%    4.743118e-01  6.716581e-01      0.750000  \n",
       "max    3.336418e+00  3.388624e+00      1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_all_datasets_experiment(raw_dir, output_dir, features, target):\n",
    "    train = read_json_dataset(raw_dir, \"train\")\n",
    "    update_examples(train.values(), \"original_filepath\", [f\"{raw_dir}/train.json\"] * len(train.values()))\n",
    "\n",
    "    df_train = to_dataframe(train)\n",
    "    df_train = cast_types(df_train, features, target)\n",
    "    df_train = imputation(df_train, features)\n",
    "    df_train, df_params = whitening(df_train, features)\n",
    "    df_train[target + \"_scaled\"] = min_max_scaling(df_train[target], 1, 5)\n",
    "    train_json = df_train.T.to_dict()\n",
    "\n",
    "    write_json_dataset(train_json, output_dir, \"train\")\n",
    "    write_json_dataset(df_params, output_dir, \"preproc_params\")\n",
    "\n",
    "    for split in (\"dev\", \"test\"):\n",
    "        split_data = read_json_dataset(raw_dir, split)\n",
    "        update_examples(split_data.values(), \"original_filepath\", [f\"{raw_dir}/{split}.json\"] * len(split_data.values()))\n",
    "\n",
    "        df = to_dataframe(split_data)\n",
    "        print(split, len(df))\n",
    "        df = cast_types(df, features, target)\n",
    "        df = imputation(df, features)\n",
    "        df, _ = whitening(df, features, params=df_params)\n",
    "        df[target + \"_scaled\"] = min_max_scaling(df[target], 1, 5)\n",
    "\n",
    "        print(split, len(df))\n",
    "        split_json = df.T.to_dict()\n",
    "        write_json_dataset(split_json, output_dir, split)\n",
    "\n",
    "    return df_train\n",
    "\n",
    "preprocess_all_datasets_experiment(\n",
    "    DATASET_RAW_DIR + \"/all_datasets\",\n",
    "    DATASET_PREPROC_DIR + \"/all_datasets\",\n",
    "    features=FEATURES,\n",
    "    target=TARGET,\n",
    "    \n",
    ").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841d1ef",
   "metadata": {},
   "source": [
    "### LOOV experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "428e3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_loov(raw_dir, output_dir, features, target, train_filename=\"train\", splits=(\"dev\", \"test\")):\n",
    "    train = read_json_dataset(raw_dir, train_filename)\n",
    "    update_examples(train.values(), \"original_filepath\", [f\"{raw_dir}/{train_filename}.json\"] * len(train.values()))\n",
    "\n",
    "    df_train = to_dataframe(train)\n",
    "    df_train = cast_types(df_train, features, target)\n",
    "    df_train = imputation(df_train, features)\n",
    "    df_train, df_params = whitening(df_train, features)\n",
    "    df_train[target + \"_scaled\"] = min_max_scaling(df_train[target], 1, 5)\n",
    "    train_json = df_train.T.to_dict()\n",
    "\n",
    "    write_json_dataset(train_json, output_dir, train_filename)\n",
    "    write_json_dataset(df_params, output_dir, f\"{train_filename}_preproc_params\")\n",
    "\n",
    "    for split in splits:\n",
    "        split_data = read_json_dataset(raw_dir, split)\n",
    "        update_examples(split_data.values(), \"original_filepath\", [f\"{raw_dir}/{split}.json\"] * len(split_data.values()))\n",
    "\n",
    "        df = to_dataframe(split_data)\n",
    "        print(\"Before imputation\", split, len(df))\n",
    "        df = cast_types(df, features, target)\n",
    "        df = imputation(df, features)\n",
    "        df, _ = whitening(df, features, params=df_params)\n",
    "        print(\"After imputation\", split, len(df))\n",
    "        df[target + \"_scaled\"] = min_max_scaling(df[target], 1, 5)\n",
    "\n",
    "        split_json = df.T.to_dict()\n",
    "        write_json_dataset(split_json, output_dir, split)\n",
    "\n",
    "    return df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef1aab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26035 examples\n",
      "Before imputation except_cosmosqa_dev 3324\n",
      "Loaded 3324 examples\n",
      "After imputation except_cosmosqa_dev 3215\n",
      "Before imputation except_cosmosqa_test 5304\n",
      "Loaded 5304 examples\n",
      "After imputation except_cosmosqa_test 5145\n",
      "Before imputation cosmosqa_test 1017\n",
      "Loaded 1017 examples\n",
      "After imputation cosmosqa_test 1016\n",
      "Loaded 30381 examples\n",
      "Before imputation except_drop_dev 3910\n",
      "Loaded 3910 examples\n",
      "After imputation except_drop_dev 3801\n",
      "Before imputation except_drop_test 6169\n",
      "Loaded 6169 examples\n",
      "After imputation except_drop_test 6009\n",
      "Before imputation drop_test 152\n",
      "Loaded 152 examples\n",
      "After imputation drop_test 152\n",
      "Loaded 23858 examples\n",
      "Before imputation except_mcscript_dev 3029\n",
      "Loaded 3029 examples\n",
      "After imputation except_mcscript_dev 3024\n",
      "Before imputation except_mcscript_test 4912\n",
      "Loaded 4912 examples\n",
      "After imputation except_mcscript_test 4896\n",
      "Before imputation mcscript_test 1409\n",
      "Loaded 1409 examples\n",
      "After imputation mcscript_test 1265\n",
      "Loaded 23598 examples\n",
      "Before imputation except_narrativeqa_dev 3118\n",
      "Loaded 3118 examples\n",
      "After imputation except_narrativeqa_dev 3011\n",
      "Before imputation except_narrativeqa_test 4614\n",
      "Loaded 4614 examples\n",
      "After imputation except_narrativeqa_test 4466\n",
      "Before imputation narrativeqa_test 1707\n",
      "Loaded 1707 examples\n",
      "After imputation narrativeqa_test 1695\n",
      "Loaded 27809 examples\n",
      "Before imputation except_quoref_dev 3664\n",
      "Loaded 3664 examples\n",
      "After imputation except_quoref_dev 3554\n",
      "Before imputation except_quoref_test 5812\n",
      "Loaded 5812 examples\n",
      "After imputation except_quoref_test 5654\n",
      "Before imputation quoref_test 509\n",
      "Loaded 509 examples\n",
      "After imputation quoref_test 507\n",
      "Loaded 23659 examples\n",
      "Before imputation except_socialiqa_dev 2990\n",
      "Loaded 2990 examples\n",
      "After imputation except_socialiqa_dev 2880\n",
      "Before imputation except_socialiqa_test 4794\n",
      "Loaded 4794 examples\n",
      "After imputation except_socialiqa_test 4635\n",
      "Before imputation socialiqa_test 1527\n",
      "Loaded 1527 examples\n",
      "After imputation socialiqa_test 1526\n"
     ]
    }
   ],
   "source": [
    "for dataset in (\"cosmosqa\", \"drop\", \"mcscript\", \"narrativeqa\", \"quoref\", \"socialiqa\"):\n",
    "    # The train for the loov experiment will be \"except_{dataset}_(train|dev)\"\n",
    "    # The evaluation for the loov experiment will be \"{dataset}_test\"\n",
    "    preprocess_loov(\n",
    "        raw_dir= DATASET_RAW_DIR + \"/loov_datasets\",\n",
    "        output_dir = DATASET_PREPROC_DIR + \"/loov_datasets\",    \n",
    "        features=FEATURES,\n",
    "        target=TARGET,\n",
    "        train_filename=f\"except_{dataset}_train\",\n",
    "        splits=(f\"except_{dataset}_dev\", f\"except_{dataset}_test\", f\"{dataset}_test\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d3b8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
