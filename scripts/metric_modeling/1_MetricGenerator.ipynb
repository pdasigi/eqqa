{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d434e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import datasets\n",
    "\n",
    "\n",
    "ROOT_DIR = \"../..\"\n",
    "ORIGINAL_MOCHA_DIR = f\"{ROOT_DIR}/data/metric-modeling/mocha\"\n",
    "SPLITS = (\"train\", \"dev\", \"test\")\n",
    "DATASETS = ('cosmosqa', 'drop', 'mcscript', 'narrativeqa', 'quoref', 'socialiqa')\n",
    "\n",
    "PREPROC_DIR = f\"{ROOT_DIR}/data/raw_splits\"\n",
    "os.makedirs(PREPROC_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a349dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"{ORIGINAL_MOCHA_DIR}/{SPLITS[0]}.json\"\n",
    "\n",
    "data = json.load(open(filepath))\n",
    "datasets = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfa99a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_dataset(parent_dir, filename, dataset=None) -> dict:\n",
    "    \"\"\"Loads the dataset from the specified path. \n",
    "    \n",
    "    It assumes the dataset is in JSON format and that is\n",
    "    represented as {tag1: {examples}, tag2: {...}, ...}\n",
    "    where tag1 and tag2 are dataset tags that the user\n",
    "    can specify. If none are specified all the datasets\n",
    "    will be returned.\n",
    "    \"\"\"\n",
    "    data = json.load(open(f\"{parent_dir}/{filename}.json\"))\n",
    "    \n",
    "    if dataset is None:\n",
    "        datasets = list(data.keys())\n",
    "    else:\n",
    "        datasets = dataset if isinstance(dataset, list) else [dataset]\n",
    "    \n",
    "    data = {d: datum for d, datum in data.items() if d in datasets}\n",
    "    return data\n",
    "\n",
    "\n",
    "# Sanity check (:\n",
    "data = read_json_dataset(ORIGINAL_MOCHA_DIR, \"dev\", \"narrativeqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94f1aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as BERT_SCORE\n",
    "from datasets import load_metric\n",
    "from pycocoevalcap.meteor.meteor import Meteor as pccMeteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge as pccRouge\n",
    "from pycocoevalcap.bleu.bleu import Bleu as pccBleu\n",
    "\n",
    "\n",
    "def remove_punc(s):\n",
    "    return s.replace('?', '').replace('.', '').replace('!', '')\n",
    "\n",
    "def update_examples(examples: dict, key, values):\n",
    "    assert len(examples) == len(values)\n",
    "\n",
    "    for example, value in zip(examples, values):\n",
    "        example[key] = value\n",
    "\n",
    "\n",
    "def add_bleu(mocha_dataset, order: int=4):\n",
    "    BLEU = pccBleu(order)\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = {i: [remove_punc(instance['reference'])] for i, instance in\n",
    "                enumerate(examples.values())}\n",
    "        cands = {i: [remove_punc(instance['candidate'])] for i, instance in\n",
    "                 enumerate(examples.values())}\n",
    "        \n",
    "        # compute_scores return (aggregate-bleu, instance-wise bleu)\n",
    "        # -- by accessing the first index, we get the bleu per instance\n",
    "        bleu_scores = BLEU.compute_score(refs, cands, verbose=0)[1]\n",
    "        \n",
    "        for i in range(order):\n",
    "            update_examples(examples.values(), f\"bleu{i+1}\", bleu_scores[i])\n",
    "\n",
    "\n",
    "def add_meteor(mocha_dataset):\n",
    "    METEOR = pccMeteor()\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = {i: [remove_punc(instance['reference'])] for i, instance in\n",
    "                enumerate(examples.values())}\n",
    "        cands = {i: [remove_punc(instance['candidate'])] for i, instance in\n",
    "                 enumerate(examples.values())}\n",
    "        pred_scores = METEOR.compute_score(refs, cands)[1]\n",
    "        update_examples(examples.values(), \"meteor\", pred_scores)\n",
    "\n",
    "\n",
    "def add_rouge(mocha_dataset):\n",
    "    ROUGE = pccRouge()\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = {i: [remove_punc(instance['reference'])] for i, instance in\n",
    "                enumerate(examples.values())}\n",
    "        cands = {i: [remove_punc(instance['candidate'])] for i, instance in\n",
    "                 enumerate(examples.values())}\n",
    "        pred_scores = ROUGE.compute_score(refs, cands)[1]\n",
    "        update_examples(examples.values(), \"rougeL\", pred_scores)\n",
    "\n",
    "        \n",
    "def add_bertscore(mocha_dataset):\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = [remove_punc(instance['reference']) for instance in examples.values()]\n",
    "        cands = [remove_punc(instance['candidate']) for instance in examples.values()]\n",
    "        pred_scores = BERT_SCORE(cands, refs, lang='en')[-1].tolist()\n",
    "        update_examples(examples.values(), \"bertscore\", pred_scores)\n",
    "\n",
    "\n",
    "def add_bleurt(mocha_dataset):\n",
    "    BLEURT = load_metric(\"bleurt\", keep_in_memory=True)\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            scores = BLEURT.compute(predictions=[remove_punc(example[\"candidate\"])],\n",
    "                                    references=[remove_punc(example[\"reference\"])])\n",
    "            example[\"bleurt\"] = scores[\"scores\"][0]\n",
    "\n",
    "\n",
    "\n",
    "def add_char_edit_rate(mocha_dataset):\n",
    "    \"\"\"Compute word edit rate. \n",
    "    \n",
    "    The formula is like the character_edit_rate but using words\n",
    "    rather than characters.\n",
    "    \"\"\"\n",
    "    # https://github.com/huggingface/datasets/tree/fad939b5e17b672a4eda7de2cd8e24d98f3d5b26/metrics/wer\n",
    "    # !pip install jiwer\n",
    "    CER = load_metric(\"cer\", keep_in_memory=True)\n",
    "    \n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            candidate = remove_punc(example[\"candidate\"])\n",
    "            reference = remove_punc(example[\"reference\"])\n",
    "\n",
    "            scores = CER.compute(predictions=[candidate], references=[reference])\n",
    "            example[\"char_edit_score\"] = scores\n",
    "\n",
    "\n",
    "def add_word_edit_rate(mocha_dataset):\n",
    "    \"\"\"Compute word edit rate. \n",
    "    \n",
    "    The formula is like the character_edit_rate but using words\n",
    "    rather than characters.\n",
    "    \"\"\"\n",
    "    # https://github.com/huggingface/datasets/tree/fad939b5e17b672a4eda7de2cd8e24d98f3d5b26/metrics/wer\n",
    "    # !pip install jiwer\n",
    "    WER = load_metric(\"wer\", keep_in_memory=True)\n",
    "    \n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            candidate = remove_punc(example[\"candidate\"])\n",
    "            reference = remove_punc(example[\"reference\"])\n",
    "\n",
    "            scores = WER.compute(predictions=[candidate], references=[reference])\n",
    "            example[\"word_edit_score\"] = scores\n",
    "    \n",
    "def add_recall(mocha_dataset):\n",
    "    from collections import Counter\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            candidate = remove_punc(example[\"candidate\"]).split()\n",
    "            reference = remove_punc(example[\"reference\"]).split()\n",
    "\n",
    "            true_tks, pred_tks = Counter(reference), Counter(candidate)\n",
    "        \n",
    "            tp = sum((true_tks & pred_tks).values())\n",
    "            \n",
    "            if tp == 0:\n",
    "                example[\"recall\"] = 0\n",
    "            else:\n",
    "                example[\"recall\"] = tp / len(reference)\n",
    "\n",
    "            example[\"tp\"] = tp\n",
    "            example[\"fn\"] = len(reference) - tp\n",
    "\n",
    "\n",
    "def add_precision(mocha_dataset):\n",
    "    from collections import Counter\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            candidate = remove_punc(example[\"candidate\"]).split()\n",
    "            reference = remove_punc(example[\"reference\"]).split()\n",
    "\n",
    "            true_tks, pred_tks = Counter(reference), Counter(candidate)\n",
    "        \n",
    "            tp = sum((true_tks & pred_tks).values())\n",
    "            example[\"precision\"] = 0 if tp == 0 else tp / len(candidate)\n",
    "\n",
    "            example[\"tp\"] = tp\n",
    "            example[\"fp\"] = len(candidate) - tp\n",
    "\n",
    "\n",
    "def add_f_score(mocha_dataset, beta=1):\n",
    "    f_name = f\"f{beta}_score\"\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            recall = example[\"recall\"]\n",
    "            precis = example[\"precision\"]\n",
    "            \n",
    "            if precis == 0 or recall == 0:\n",
    "                example[f_name] = 0\n",
    "            else:\n",
    "                beta = beta*beta\n",
    "                num = precis * recall\n",
    "                den = (beta * precis + recall)\n",
    "                example[f_name] = (1+beta) * num / den\n",
    "\n",
    "\n",
    "def add_rouge_order_n(mocha_dataset, rouge_types, use_stemmer=False):\n",
    "    ROUGE = load_metric(\"rouge\", keep_in_memory=True) \n",
    "    #^Note: requires installing rouge-score (!pip install rouge-score)\n",
    "    # https://github.com/huggingface/datasets/issues/617\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        candidate = [[remove_punc(ex[\"candidate\"])] for ex in examples.values()]\n",
    "        reference = [[remove_punc(ex[\"reference\"])] for ex in examples.values()]\n",
    "\n",
    "        scores = ROUGE.compute(predictions=candidate,\n",
    "                               references=reference,\n",
    "                               use_stemmer=use_stemmer,\n",
    "                               use_aggregator=False)\n",
    "\n",
    "        for rouge_type in rouge_types:\n",
    "            rouge_scores = [s.fmeasure for s in scores[rouge_type]]\n",
    "            update_examples(examples.values(), \"hf_\" + rouge_type, rouge_scores)\n",
    "\n",
    "\n",
    "def add_sari(mocha_dataset, source_col=\"context\"):\n",
    "    \"\"\"Compute the SARI score.\n",
    "    \n",
    "    System output against references and against the input sentence.\n",
    "    Often used for evaluating automatic text simplification systems.    \n",
    "    https://github.com/huggingface/datasets/tree/master/metrics/sari.\n",
    "    \n",
    "    The range of values for the SARI score is between 0 and 100 -- the\n",
    "    higher the value, the better the performance of the model being\n",
    "    evaluated, with a SARI of 100 being a perfect score.\n",
    "    \n",
    "    We divide the score by 100 to be on the range (0, 1).\n",
    "    This score is computed as: \n",
    "        sari = ( F1_add + F1_keep + P_del) / 3,\n",
    "    where\n",
    "    - F1_add is the n-gram F1 score for add operations\n",
    "    - F1_keep is the n-gram F1 score for keep operations\n",
    "    - P_del is the n-gram precision score for delete operations\n",
    "\n",
    "    The number of n grams, n, is equal to 4, as in the original paper.\n",
    "    \"\"\"\n",
    "    SARI = load_metric(\"sari\", keep_in_memory=True)\n",
    "    \n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            sources = remove_punc(example[source_col])\n",
    "            candidate = remove_punc(example[\"candidate\"])\n",
    "            reference = remove_punc(example[\"reference\"])\n",
    "\n",
    "            scores = SARI.compute(sources=[sources], predictions=[candidate], references=[[reference]])\n",
    "            example[f\"sari_{source_col}\"] = scores[\"sari\"] / 100\n",
    "            \n",
    "def add_first_error_position(mocha_dataset):    \n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        raise NotImplementedError\n",
    "            \n",
    "def add_word_movers_distance(mocha_dataset):\n",
    "    # https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7add11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_char_edit_rate(data)\n",
    "# add_word_edit_rate(data)\n",
    "\n",
    "add_recall(data)\n",
    "add_precision(data)\n",
    "add_f_score(data)\n",
    "add_sari(data, \"context\")\n",
    "add_sari(data, \"question\")\n",
    "add_bleu(data)\n",
    "add_rouge(data)\n",
    "add_rouge_order_n(data, [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"])\n",
    "# add_meteor(data)\n",
    "\n",
    "# add_bertscore(data)\n",
    "#add_bleurt(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1fb66b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidate': 'his distant relative pascal rougon',\n",
       " 'context': \"The plot centres on the neurotic young priest Serge Mouret, first seen in La ConquĂŞte de Plassans, as he takes his orders and becomes the parish priest for the uninterested village of Artauds. The inbred villagers have no interest in religion and Serge is portrayed giving several wildly enthusiastic Masses to his completely empty, near-derelict church. Serge not only seems unperturbed by this state of affairs but actually appears to have positively sought it out especially, for it gives him time to contemplate religious affairs and to fully experience the fervour of his faith. Eventually he has a complete nervous breakdown and collapses into a near-comatose state, whereupon his distant relative, the unconventional doctor Pascal Rougon (the central character of the last novel in the series, 1893's Le Docteur Pascal), places him in the care of the inhabitants of a nearby derelict stately home, Le Paradou. The novel then takes a complete new direction in terms of both tone and style, as Serge - suffering from amnesia and total long-term memory loss, with no idea who or where he is beyond his first name - is doted upon by Albine, the whimsical, innocent and entirely uneducated girl who has been left to grow up practically alone and wild in the vast, sprawling, overgrown grounds of Le Paradou. The two of them live a life of idyllic bliss with many Biblical parallels, and over the course of a number of months, they fall deeply in love with one another; however, at the moment they consummate their relationship, they are discovered by Serge's monstrous former monseignor and his memory is instantly returned to him. Wracked with guilt at his unwitting sins, Serge is plunged into a deeper religious fervour than ever before, and poor Albine is left bewildered at the loss of her soulmate. As with many of Zola's earlier works, the novel then builds to a horrible climax.\",\n",
       " 'metadata': {'scores': [5, 5, 4], 'source': 'mhpg'},\n",
       " 'question': 'Who put Serge Mouret in a home care?',\n",
       " 'reference': 'Le Docteur Pascal',\n",
       " 'score': 4.666666666666667,\n",
       " 'recall': 0,\n",
       " 'tp': 0,\n",
       " 'fn': 3,\n",
       " 'precision': 0,\n",
       " 'fp': 5,\n",
       " 'f1_score': 0,\n",
       " 'rougeL': 0.0,\n",
       " '_rouge1': 0.25,\n",
       " '_rouge2': 0.0,\n",
       " '_rougeL': 0.25,\n",
       " '_rougeLsum': 0.25,\n",
       " 'hf_rouge1': 0.25,\n",
       " 'hf_rouge2': 0.0,\n",
       " 'hf_rougeL': 0.25,\n",
       " 'hf_rougeLsum': 0.25,\n",
       " 'sari': 20.833333333333336,\n",
       " 'sari_context': 0.5190831774304202,\n",
       " 'sari_question': 0.6875}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"narrativeqa\"][\"0005c7718ff653683df879622efb02d1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77affd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
