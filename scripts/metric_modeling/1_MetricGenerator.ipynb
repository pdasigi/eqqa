{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5287ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import datasets\n",
    "\n",
    "\n",
    "ROOT_DIR = \"../..\"\n",
    "ORIGINAL_MOCHA_DIR = f\"{ROOT_DIR}/data/metric-modeling/mocha\"\n",
    "SPLITS = (\"train\", \"dev\", \"test\")\n",
    "DATASETS = ('cosmosqa', 'drop', 'mcscript', 'narrativeqa', 'quoref', 'socialiqa')\n",
    "\n",
    "PREPROC_DIR = f\"{ROOT_DIR}/data/raw_splits\"\n",
    "os.makedirs(PREPROC_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57ef2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"{ORIGINAL_MOCHA_DIR}/{SPLITS[0]}.json\"\n",
    "\n",
    "data = json.load(open(filepath))\n",
    "datasets = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17da5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_dataset(parent_dir, filename, dataset=None) -> dict:\n",
    "    \"\"\"Loads the dataset from the specified path. \n",
    "    \n",
    "    It assumes the dataset is in JSON format and that is\n",
    "    represented as {tag1: {examples}, tag2: {...}, ...}\n",
    "    where tag1 and tag2 are dataset tags that the user\n",
    "    can specify. If none are specified all the datasets\n",
    "    will be returned.\n",
    "    \"\"\"\n",
    "    data = json.load(open(f\"{parent_dir}/{filename}.json\"))\n",
    "    \n",
    "    if dataset is None:\n",
    "        datasets = list(data.keys())\n",
    "    else:\n",
    "        datasets = dataset if isinstance(dataset, list) else [dataset]\n",
    "    \n",
    "    data = {d: datum for d, datum in data.items() if d in datasets}\n",
    "    return data\n",
    "\n",
    "\n",
    "# Sanity check (:\n",
    "data = read_json_dataset(ORIGINAL_MOCHA_DIR, \"dev\", \"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82057d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as BERT_SCORE\n",
    "from datasets import load_metric\n",
    "from pycocoevalcap.meteor.meteor import Meteor as pccMeteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge as pccRouge\n",
    "from pycocoevalcap.bleu.bleu import Bleu as pccBleu\n",
    "\n",
    "\n",
    "def remove_punc(s):\n",
    "    return s.replace('?', '').replace('.', '').replace('!', '')\n",
    "\n",
    "def update_examples(examples: dict, key, values):\n",
    "    assert len(examples) == len(values)\n",
    "\n",
    "    for example, value in zip(examples, values):\n",
    "        example[key] = value\n",
    "\n",
    "\n",
    "def add_bleu_predictions(mocha_dataset, order: int=4):\n",
    "    BLEU = pccBleu(order)\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = {i: [remove_punc(instance['reference'])] for i, instance in\n",
    "                enumerate(examples.values())}\n",
    "        cands = {i: [remove_punc(instance['candidate'])] for i, instance in\n",
    "                 enumerate(examples.values())}\n",
    "        \n",
    "        # compute_scores return (aggregate-bleu, instance-wise bleu)\n",
    "        # -- by accessing the first index, we get the bleu per instance\n",
    "        bleu_scores = BLEU.compute_score(refs, cands, verbose=0)[1]\n",
    "        \n",
    "        for i in range(order):\n",
    "            update_examples(examples.values(), f\"bleu{i+1}\", bleu_scores[i])\n",
    "\n",
    "\n",
    "def add_meteor_predictions(mocha_dataset):\n",
    "    METEOR = pccMeteor()\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = {i: [remove_punc(instance['reference'])] for i, instance in\n",
    "                enumerate(examples.values())}\n",
    "        cands = {i: [remove_punc(instance['candidate'])] for i, instance in\n",
    "                 enumerate(examples.values())}\n",
    "        pred_scores = METEOR.compute_score(refs, cands)[1]\n",
    "        update_examples(examples.values(), \"meteor\", pred_scores)\n",
    "\n",
    "\n",
    "def add_rouge_predictions(mocha_dataset):\n",
    "    ROUGE = pccRouge()\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = {i: [remove_punc(instance['reference'])] for i, instance in\n",
    "                enumerate(examples.values())}\n",
    "        cands = {i: [remove_punc(instance['candidate'])] for i, instance in\n",
    "                 enumerate(examples.values())}\n",
    "        pred_scores = ROUGE.compute_score(refs, cands)[1]\n",
    "        update_examples(examples.values(), \"rougeL\", pred_scores)\n",
    "\n",
    "        \n",
    "def add_bertscore_predictions(mocha_dataset):\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = [remove_punc(instance['reference']) for instance in examples.values()]\n",
    "        cands = [remove_punc(instance['candidate']) for instance in examples.values()]\n",
    "        pred_scores = BERT_SCORE(cands, refs, lang='en')[-1].tolist()\n",
    "        update_examples(examples.values(), \"bertscore\", pred_scores)\n",
    "\n",
    "\n",
    "def add_bleurt_predictions(mocha_dataset):\n",
    "    BLEURT = load_metric(\"bleurt\", keep_in_memory=True)\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            scores = BLEURT.compute(predictions=[remove_punc(example[\"candidate\"])],\n",
    "                                    references=[remove_punc(example[\"reference\"])])\n",
    "            example[\"bleurt\"] = scores[\"scores\"][0]\n",
    "\n",
    "\n",
    "def add_edit_score(mocha_dataset, **kwargs):\n",
    "    \"\"\"Compute the translation error rate to quantify the edit operations.\n",
    "    \n",
    "    We use the implementation available at \n",
    "    https://github.com/huggingface/datasets/tree/fad939b5e17b672a4eda7de2cd8e24d98f3d5b26/metrics/ter.\n",
    "    \n",
    "    TER score represents the fraction of edits divided over the reference length.\n",
    "\n",
    "    Keywords Arguments\n",
    "    ------------------\n",
    "    normalized: bool, defaults to False\n",
    "        If true, applies basic tokenization and normalization to sentences.\n",
    "    ignore_punct: bool, defaults to False\n",
    "        If true, applies basic tokenization and normalization to sentences.\n",
    "    case_sensitive: bool, defaults to False\n",
    "        If false, makes all predictions and references lowercase to ignore\n",
    "        differences in casing.\n",
    "    \"\"\"\n",
    "    EDIT_RATIO = load_metric(\"ter\", keep_in_memory=True)\n",
    "    \n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            candidate = remove_punc(example[\"candidate\"])\n",
    "            reference = remove_punc(example[\"reference\"])\n",
    "\n",
    "            scores = EDIT_RATIO.compute(predictions=[candidate], references=[[reference]])\n",
    "            example[\"edit_ratio\"] = scores[\"score\"] / 100\n",
    "\n",
    "\n",
    "def add_word_edit_rate(mocha_dataset):\n",
    "    \"\"\"Compute word edit rate. \n",
    "    \n",
    "    The formula is like the character_edit_rate but using words\n",
    "    rather than characters.\n",
    "    \"\"\"\n",
    "    # https://github.com/huggingface/datasets/tree/fad939b5e17b672a4eda7de2cd8e24d98f3d5b26/metrics/wer\n",
    "    # !pip install jiwer\n",
    "    WER = load_metric(\"wer\", keep_in_memory=True)\n",
    "    \n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            candidate = remove_punc(example[\"candidate\"])\n",
    "            reference = remove_punc(example[\"reference\"])\n",
    "\n",
    "            scores = WER.compute(predictions=[candidate], references=[reference])\n",
    "            example[\"word_edit_score\"] = scores\n",
    "    \n",
    "def add_recall(mocha_dataset):\n",
    "    from collections import Counter\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            candidate = remove_punc(example[\"candidate\"]).split()\n",
    "            reference = remove_punc(example[\"reference\"]).split()\n",
    "\n",
    "            true_tks, pred_tks = Counter(reference), Counter(candidate)\n",
    "        \n",
    "            tp = sum((true_tks & pred_tks).values())\n",
    "            \n",
    "            if tp == 0:\n",
    "                example[\"recall\"] = 0\n",
    "            else:\n",
    "                example[\"recall\"] = tp / len(reference)\n",
    "\n",
    "            example[\"tp\"] = tp\n",
    "            example[\"fn\"] = len(reference) - tp\n",
    "\n",
    "\n",
    "def add_precision(mocha_dataset):\n",
    "    from collections import Counter\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            candidate = remove_punc(example[\"candidate\"]).split()\n",
    "            reference = remove_punc(example[\"reference\"]).split()\n",
    "\n",
    "            true_tks, pred_tks = Counter(reference), Counter(candidate)\n",
    "        \n",
    "            tp = sum((true_tks & pred_tks).values())\n",
    "            example[\"precision\"] = 0 if tp == 0 else tp / len(candidate)\n",
    "\n",
    "            example[\"tp\"] = tp\n",
    "            example[\"fp\"] = len(candidate) - tp\n",
    "\n",
    "def add_rouge_order_n(mocha_dataset, order):\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        raise NotImplementedError    \n",
    "\n",
    "def add_first_error_position(mocha_dataset):    \n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        raise NotImplementedError\n",
    "\n",
    "            \n",
    "def add_word_movers_distance(mocha_dataset):\n",
    "    # https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\n",
    "    raise NotImplementedError\n",
    "\n",
    "def add_sari(mocha_dataset):\n",
    "    \"\"\"https://github.com/huggingface/datasets/tree/master/metrics/sari\"\"\"\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9487cbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "len(\"hello it's me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b5e34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_bleu_predictions(data)\n",
    "# add_meteor_predictions(data)\n",
    "# add_rouge_predictions(data)\n",
    "# add_bertscore_predictions(data)\n",
    "# add_bleurt_predictions(data)\n",
    "add_word_edit_rate(data)\n",
    "add_edit_score(data)\n",
    "add_recall(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3af3bf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidate': 'between the ages of 10 to 29',\n",
       " 'context': 'The age distribution, , in Lausanne is; 11,818 children or 9.4% of the population are between 0 and 9 years old and 12,128 teenagers or 9.7% are between 10 and 19. Of the adult population, 21,101 people or 16.8% of the population are between 20 and 29 years old. 22,158 people or 17.6% are between 30 and 39, 18,016 people or 14.4% are between 40 and 49, and 13,940 people or 11.1% are between 50 and 59. The senior population distribution is 11,041 people or 8.8% of the population are between 60 and 69 years old, 8,277 people or 6.6% are between 70 and 79, there are 5,896 people or 4.7% who are between 80 and 89, and there are 1,171 people or 0.9% who are 90 and older.',\n",
       " 'metadata': {'scores': [5, 5], 'source': 'naqanet'},\n",
       " 'question': 'Are more people between the ages of 10 to 29 or 80 and older?',\n",
       " 'reference': '10 to 29',\n",
       " 'score': 5,\n",
       " 'bleurt': -0.44349393248558044,\n",
       " 'char_edit_score': 2.5,\n",
       " 'edit_ratio': 1.333333333333333,\n",
       " 'word_edit_score': 1.3333333333333333,\n",
       " 'recall': 1.0,\n",
       " 'tp': 3,\n",
       " 'fn': 0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"drop\"][\"01d2dcd528219ac0739e8e07030ae88b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5d0d0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\n",
      "  Using cached jiwer-2.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting python-Levenshtein==0.12.2\n",
      "  Using cached python_Levenshtein-0.12.2-cp39-cp39-linux_x86_64.whl\n",
      "Requirement already satisfied: setuptools in /home/kat/miniconda3/envs/eqqa-env/lib/python3.9/site-packages (from python-Levenshtein==0.12.2->jiwer) (61.3.0)\n",
      "Installing collected packages: python-Levenshtein, jiwer\n",
      "Successfully installed jiwer-2.3.0 python-Levenshtein-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c9c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
