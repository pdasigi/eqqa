{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31148be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_datasets import read_json_dataset, write_json_dataset\n",
    "from dict_utils import update_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b255dd6",
   "metadata": {},
   "source": [
    "## Preprocess datasets for metric modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beda30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT_DIR = \"../..\"\n",
    "DATASET_RAW_DIR = f\"{ROOT_DIR}/data/raw_splits\"\n",
    "\n",
    "DATASET_PREPROC_DIR = f\"{ROOT_DIR}/data/preprocessing_minmaxscale\"\n",
    "os.makedirs(DATASET_PREPROC_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "FEATURES = [\n",
    "    'char_edit_score',\n",
    "    'word_edit_score',\n",
    "    'recall', \n",
    "    'tp', \n",
    "    'fn',\n",
    "    'precision',\n",
    "    'fp',\n",
    "    'f1_score',\n",
    "    'sari_context',\n",
    "    'sari_question',\n",
    "    'bleu1',\n",
    "    'bleu2',\n",
    "    'bleu3',\n",
    "    'bleu4',\n",
    "    'hf_bleu1', \n",
    "    'hf_bleu2',\n",
    "    'hf_bleu3',\n",
    "    'hf_bleu4',\n",
    "    'rougeL',\n",
    "    'hf_rouge1',\n",
    "    'hf_rouge2',\n",
    "    'hf_rougeL',\n",
    "    'hf_rougeLsum',\n",
    "    'precision_at_err1',\n",
    "    'recall_at_err1',\n",
    "    'meteor',\n",
    "    'bertscore',\n",
    "    'bleurt',\n",
    "    'wmd',\n",
    "]\n",
    "\n",
    "TARGET = 'score'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90803845",
   "metadata": {},
   "source": [
    "### Preprocess AD datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d13d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16c5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(data: dict) -> pd.DataFrame:\n",
    "    return pd.DataFrame.from_dict(data).T\n",
    "\n",
    "\n",
    "def cast_types(df: pd.DataFrame, features: list, label: str) -> pd.DataFrame:\n",
    "    print(\"Loaded\", len(df), \"examples\")\n",
    "\n",
    "    for feat in features + [label]:\n",
    "        df[feat] = df[feat].astype(float)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def imputation(df: pd.DataFrame, features: list) -> pd.DataFrame:\n",
    "    \"\"\"Currently only accounts for inf.\"\"\"    \n",
    "    \n",
    "    for feature in features:\n",
    "        # Currently, we do not support missing data imputation\n",
    "        assert not df[feature].isna().any(), f\"NaN found for feature: {feature}\"\n",
    "        \n",
    "        # For now, we will drop the infinite instances\n",
    "        valid_mask = df[feature].abs() != np.inf\n",
    "        df = df[valid_mask]\n",
    "\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def standardize(data, mean, std):\n",
    "    return (data - mean) / std\n",
    "\n",
    "\n",
    "def min_max_scaling(data, min, max):\n",
    "    return (data - min) / (max-min)\n",
    "\n",
    "\n",
    "def whitening_min_max(df: pd.DataFrame, features, params=None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    transform_params = params if params is not None else {}\n",
    "    \n",
    "    for feature in features:\n",
    "        if params is None:\n",
    "            transform_params[feature] = {\n",
    "                \"max\": df[feature].max(),\n",
    "                \"min\": df[feature].min(),\n",
    "            }\n",
    "        \n",
    "        df[feature] = min_max_scaling(df[feature], **transform_params[feature])\n",
    "        \n",
    "    return df, transform_params\n",
    "\n",
    "\n",
    "def whitening(df: pd.DataFrame, features, params=None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    transform_params = params if params is not None else {}\n",
    "    \n",
    "    for feature in features:\n",
    "        if params is None:\n",
    "            transform_params[feature] = {\n",
    "                \"mean\": df[feature].mean(),\n",
    "                \"std\": df[feature].std(),\n",
    "            }\n",
    "        \n",
    "        df[feature] = standardize(df[feature], **transform_params[feature])\n",
    "        \n",
    "    return df, transform_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1a272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31068 examples\n",
      "dev 4007\n",
      "Loaded 4007 examples\n",
      "dev 3897\n",
      "test 6321\n",
      "Loaded 6321 examples\n",
      "test 6161\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "      <th>hf_bleu1</th>\n",
       "      <th>hf_bleu2</th>\n",
       "      <th>hf_bleu3</th>\n",
       "      <th>hf_bleu4</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>...</th>\n",
       "      <th>precision_at_err1</th>\n",
       "      <th>recall_at_err1</th>\n",
       "      <th>char_edit_score</th>\n",
       "      <th>word_edit_score</th>\n",
       "      <th>sari_context</th>\n",
       "      <th>sari_question</th>\n",
       "      <th>bertscore</th>\n",
       "      <th>bleurt</th>\n",
       "      <th>wmd</th>\n",
       "      <th>score_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30274.000000</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>3.027400e+04</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "      <td>30274.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.439354</td>\n",
       "      <td>2.017022e-01</td>\n",
       "      <td>9.511891e-02</td>\n",
       "      <td>4.562428e-02</td>\n",
       "      <td>2.229490e-02</td>\n",
       "      <td>0.232572</td>\n",
       "      <td>0.113584</td>\n",
       "      <td>0.056009</td>\n",
       "      <td>0.027962</td>\n",
       "      <td>0.242285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174196</td>\n",
       "      <td>0.176845</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.542217</td>\n",
       "      <td>0.668281</td>\n",
       "      <td>0.596959</td>\n",
       "      <td>0.888389</td>\n",
       "      <td>0.363647</td>\n",
       "      <td>0.359838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.593652</td>\n",
       "      <td>2.340288e-01</td>\n",
       "      <td>1.980508e-01</td>\n",
       "      <td>1.500492e-01</td>\n",
       "      <td>1.098972e-01</td>\n",
       "      <td>0.249471</td>\n",
       "      <td>0.218263</td>\n",
       "      <td>0.168747</td>\n",
       "      <td>0.125237</td>\n",
       "      <td>0.257604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325738</td>\n",
       "      <td>0.327271</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.025673</td>\n",
       "      <td>0.192562</td>\n",
       "      <td>0.133987</td>\n",
       "      <td>0.176048</td>\n",
       "      <td>0.033452</td>\n",
       "      <td>0.187791</td>\n",
       "      <td>0.398413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.333332e-16</td>\n",
       "      <td>7.070087e-16</td>\n",
       "      <td>7.937004e-13</td>\n",
       "      <td>5.842795e-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.396020</td>\n",
       "      <td>0.565121</td>\n",
       "      <td>0.478105</td>\n",
       "      <td>0.875344</td>\n",
       "      <td>0.235128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.238397e-01</td>\n",
       "      <td>4.229931e-09</td>\n",
       "      <td>2.554365e-11</td>\n",
       "      <td>2.659147e-11</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.539975</td>\n",
       "      <td>0.653872</td>\n",
       "      <td>0.567548</td>\n",
       "      <td>0.889648</td>\n",
       "      <td>0.361540</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.333333e-01</td>\n",
       "      <td>2.236068e-08</td>\n",
       "      <td>7.937005e-08</td>\n",
       "      <td>8.633400e-09</td>\n",
       "      <td>0.397981</td>\n",
       "      <td>0.110314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.007150</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.676439</td>\n",
       "      <td>0.763062</td>\n",
       "      <td>0.701062</td>\n",
       "      <td>0.904256</td>\n",
       "      <td>0.489778</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              score         bleu1         bleu2         bleu3         bleu4  \\\n",
       "count  30274.000000  3.027400e+04  3.027400e+04  3.027400e+04  3.027400e+04   \n",
       "mean       2.439354  2.017022e-01  9.511891e-02  4.562428e-02  2.229490e-02   \n",
       "std        1.593652  2.340288e-01  1.980508e-01  1.500492e-01  1.098972e-01   \n",
       "min        1.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%        1.000000  3.333332e-16  7.070087e-16  7.937004e-13  5.842795e-13   \n",
       "50%        2.000000  1.238397e-01  4.229931e-09  2.554365e-11  2.659147e-11   \n",
       "75%        4.000000  3.333333e-01  2.236068e-08  7.937005e-08  8.633400e-09   \n",
       "max        5.000000  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "           hf_bleu1      hf_bleu2      hf_bleu3      hf_bleu4        rougeL  \\\n",
       "count  30274.000000  30274.000000  30274.000000  30274.000000  30274.000000   \n",
       "mean       0.232572      0.113584      0.056009      0.027962      0.242285   \n",
       "std        0.249471      0.218263      0.168747      0.125237      0.257604   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.163265      0.000000      0.000000      0.000000      0.177326   \n",
       "75%        0.397981      0.110314      0.000000      0.000000      0.420690   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...  precision_at_err1  recall_at_err1  char_edit_score  \\\n",
       "count  ...       30274.000000    30274.000000     30274.000000   \n",
       "mean   ...           0.174196        0.176845         0.009375   \n",
       "std    ...           0.325738        0.327271         0.020880   \n",
       "min    ...           0.000000        0.000000         0.000000   \n",
       "25%    ...           0.000000        0.000000         0.003531   \n",
       "50%    ...           0.000000        0.000000         0.005032   \n",
       "75%    ...           0.200000        0.200000         0.007150   \n",
       "max    ...           1.000000        1.000000         1.000000   \n",
       "\n",
       "       word_edit_score  sari_context  sari_question     bertscore  \\\n",
       "count     30274.000000  30274.000000   30274.000000  30274.000000   \n",
       "mean          0.014609      0.542217       0.668281      0.596959   \n",
       "std           0.025673      0.192562       0.133987      0.176048   \n",
       "min           0.000000      0.000000       0.000000      0.000000   \n",
       "25%           0.006061      0.396020       0.565121      0.478105   \n",
       "50%           0.009091      0.539975       0.653872      0.567548   \n",
       "75%           0.012121      0.676439       0.763062      0.701062   \n",
       "max           1.000000      1.000000       1.000000      1.000000   \n",
       "\n",
       "             bleurt           wmd  score_scaled  \n",
       "count  30274.000000  30274.000000  30274.000000  \n",
       "mean       0.888389      0.363647      0.359838  \n",
       "std        0.033452      0.187791      0.398413  \n",
       "min        0.000000      0.000000      0.000000  \n",
       "25%        0.875344      0.235128      0.000000  \n",
       "50%        0.889648      0.361540      0.250000  \n",
       "75%        0.904256      0.489778      0.750000  \n",
       "max        1.000000      1.000000      1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_all_datasets_experiment(raw_dir, output_dir, features, target, whitening_fn: callable):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train = read_json_dataset(raw_dir, \"train\")\n",
    "    update_examples(train.values(), \"original_filepath\", [f\"{raw_dir}/train.json\"] * len(train.values()))\n",
    "\n",
    "    df_train = to_dataframe(train)\n",
    "    df_train = cast_types(df_train, features, target)\n",
    "    df_train = imputation(df_train, features)\n",
    "    df_train, df_params = whitening_fn(df_train, features)\n",
    "    df_train[target + \"_scaled\"] = min_max_scaling(df_train[target], 1, 5)\n",
    "    train_json = df_train.T.to_dict()\n",
    "\n",
    "    write_json_dataset(train_json, output_dir, \"train\")\n",
    "    write_json_dataset(df_params, output_dir, \"preproc_params\")\n",
    "\n",
    "    for split in (\"dev\", \"test\"):\n",
    "        split_data = read_json_dataset(raw_dir, split)\n",
    "        update_examples(split_data.values(), \"original_filepath\", [f\"{raw_dir}/{split}.json\"] * len(split_data.values()))\n",
    "\n",
    "        df = to_dataframe(split_data)\n",
    "        print(split, len(df))\n",
    "        df = cast_types(df, features, target)\n",
    "        df = imputation(df, features)\n",
    "        df, _ = whitening_fn(df, features, params=df_params)\n",
    "        df[target + \"_scaled\"] = min_max_scaling(df[target], 1, 5)\n",
    "\n",
    "        print(split, len(df))\n",
    "        split_json = df.T.to_dict()\n",
    "        write_json_dataset(split_json, output_dir, split)\n",
    "\n",
    "    return df_train\n",
    "\n",
    "preprocess_all_datasets_experiment(\n",
    "    DATASET_RAW_DIR + \"/all_datasets\",\n",
    "    DATASET_PREPROC_DIR + \"/all_datasets\",\n",
    "    features=FEATURES,\n",
    "    target=TARGET,\n",
    "    whitening_fn=whitening_min_max,\n",
    "    \n",
    ").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841d1ef",
   "metadata": {},
   "source": [
    "### LOOV experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "428e3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_loov(raw_dir, output_dir, features, target, train_filename=\"train\", splits=(\"dev\", \"test\"), whitening_fn: callable=whitening):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    train = read_json_dataset(raw_dir, train_filename)\n",
    "    update_examples(train.values(), \"original_filepath\", [f\"{raw_dir}/{train_filename}.json\"] * len(train.values()))\n",
    "\n",
    "    df_train = to_dataframe(train)\n",
    "    df_train = cast_types(df_train, features, target)\n",
    "    df_train = imputation(df_train, features)\n",
    "    df_train, df_params = whitening_fn(df_train, features)\n",
    "    df_train[target + \"_scaled\"] = min_max_scaling(df_train[target], 1, 5)\n",
    "    train_json = df_train.T.to_dict()\n",
    "\n",
    "    write_json_dataset(train_json, output_dir, train_filename)\n",
    "    write_json_dataset(df_params, output_dir, f\"{train_filename}_preproc_params\")\n",
    "\n",
    "    for split in splits:\n",
    "        split_data = read_json_dataset(raw_dir, split)\n",
    "        update_examples(split_data.values(), \"original_filepath\", [f\"{raw_dir}/{split}.json\"] * len(split_data.values()))\n",
    "\n",
    "        df = to_dataframe(split_data)\n",
    "        print(\"Before imputation\", split, len(df))\n",
    "        df = cast_types(df, features, target)\n",
    "        df = imputation(df, features)\n",
    "        df, _ = whitening_fn(df, features, params=df_params)\n",
    "        print(\"After imputation\", split, len(df))\n",
    "        df[target + \"_scaled\"] = min_max_scaling(df[target], 1, 5)\n",
    "\n",
    "        split_json = df.T.to_dict()\n",
    "        write_json_dataset(split_json, output_dir, split)\n",
    "\n",
    "    return df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef1aab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26035 examples\n",
      "Before imputation except_cosmosqa_dev 3324\n",
      "Loaded 3324 examples\n",
      "After imputation except_cosmosqa_dev 3215\n",
      "Before imputation except_cosmosqa_test 5304\n",
      "Loaded 5304 examples\n",
      "After imputation except_cosmosqa_test 5145\n",
      "Before imputation cosmosqa_test 1017\n",
      "Loaded 1017 examples\n",
      "After imputation cosmosqa_test 1016\n",
      "Loaded 30381 examples\n",
      "Before imputation except_drop_dev 3910\n",
      "Loaded 3910 examples\n",
      "After imputation except_drop_dev 3801\n",
      "Before imputation except_drop_test 6169\n",
      "Loaded 6169 examples\n",
      "After imputation except_drop_test 6009\n",
      "Before imputation drop_test 152\n",
      "Loaded 152 examples\n",
      "After imputation drop_test 152\n",
      "Loaded 23858 examples\n",
      "Before imputation except_mcscript_dev 3029\n",
      "Loaded 3029 examples\n",
      "After imputation except_mcscript_dev 3024\n",
      "Before imputation except_mcscript_test 4912\n",
      "Loaded 4912 examples\n",
      "After imputation except_mcscript_test 4896\n",
      "Before imputation mcscript_test 1409\n",
      "Loaded 1409 examples\n",
      "After imputation mcscript_test 1265\n",
      "Loaded 23598 examples\n",
      "Before imputation except_narrativeqa_dev 3118\n",
      "Loaded 3118 examples\n",
      "After imputation except_narrativeqa_dev 3011\n",
      "Before imputation except_narrativeqa_test 4614\n",
      "Loaded 4614 examples\n",
      "After imputation except_narrativeqa_test 4466\n",
      "Before imputation narrativeqa_test 1707\n",
      "Loaded 1707 examples\n",
      "After imputation narrativeqa_test 1695\n",
      "Loaded 27809 examples\n",
      "Before imputation except_quoref_dev 3664\n",
      "Loaded 3664 examples\n",
      "After imputation except_quoref_dev 3554\n",
      "Before imputation except_quoref_test 5812\n",
      "Loaded 5812 examples\n",
      "After imputation except_quoref_test 5654\n",
      "Before imputation quoref_test 509\n",
      "Loaded 509 examples\n",
      "After imputation quoref_test 507\n",
      "Loaded 23659 examples\n",
      "Before imputation except_socialiqa_dev 2990\n",
      "Loaded 2990 examples\n",
      "After imputation except_socialiqa_dev 2880\n",
      "Before imputation except_socialiqa_test 4794\n",
      "Loaded 4794 examples\n",
      "After imputation except_socialiqa_test 4635\n",
      "Before imputation socialiqa_test 1527\n",
      "Loaded 1527 examples\n",
      "After imputation socialiqa_test 1526\n"
     ]
    }
   ],
   "source": [
    "for dataset in (\"cosmosqa\", \"drop\", \"mcscript\", \"narrativeqa\", \"quoref\", \"socialiqa\"):\n",
    "    # The train for the loov experiment will be \"except_{dataset}_(train|dev)\"\n",
    "    # The evaluation for the loov experiment will be \"{dataset}_test\"\n",
    "    preprocess_loov(\n",
    "        raw_dir= DATASET_RAW_DIR + \"/loov_datasets\",\n",
    "        output_dir = DATASET_PREPROC_DIR + \"/loov_datasets\",    \n",
    "        features=FEATURES,\n",
    "        target=TARGET,\n",
    "        train_filename=f\"except_{dataset}_train\",\n",
    "        splits=(f\"except_{dataset}_dev\", f\"except_{dataset}_test\", f\"{dataset}_test\"),\n",
    "        whitening_fn=whitening_min_max\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d3b8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
