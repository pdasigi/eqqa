{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd83223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import datasets\n",
    "\n",
    "\n",
    "ROOT_DIR = \"../..\"\n",
    "ORIGINAL_MOCHA_DIR = f\"{ROOT_DIR}/data/metric-modeling/mocha\"\n",
    "SPLITS = (\"train\", \"dev\", \"test\")\n",
    "DATASETS = ('cosmosqa', 'drop', 'mcscript', 'narrativeqa', 'quoref', 'socialiqa')\n",
    "\n",
    "PREPROC_DIR = f\"{ROOT_DIR}/data/raw_splits\"\n",
    "os.makedirs(PREPROC_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be1d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"{ORIGINAL_MOCHA_DIR}/{SPLITS[0]}.json\"\n",
    "\n",
    "data = json.load(open(filepath))\n",
    "datasets = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a549706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_dataset(parent_dir, filename, dataset=None) -> dict:\n",
    "    \"\"\"Loads the dataset from the specified path. \n",
    "    \n",
    "    It assumes the dataset is in JSON format and that is\n",
    "    represented as {tag1: {examples}, tag2: {...}, ...}\n",
    "    where tag1 and tag2 are dataset tags that the user\n",
    "    can specify. If none are specified all the datasets\n",
    "    will be returned.\n",
    "    \"\"\"\n",
    "    data = json.load(open(f\"{parent_dir}/{filename}.json\"))\n",
    "    \n",
    "    if dataset is None:\n",
    "        datasets = list(data.keys())\n",
    "    else:\n",
    "        datasets = dataset if isinstance(dataset, list) else [dataset]\n",
    "    \n",
    "    data = {d: datum for d, datum in data.items() if d in datasets}\n",
    "    return data\n",
    "\n",
    "\n",
    "# Sanity check (:\n",
    "data = read_json_dataset(ORIGINAL_MOCHA_DIR, \"dev\", \"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f43b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as BERT_SCORE\n",
    "from datasets import load_metric\n",
    "from pycocoevalcap.meteor.meteor import Meteor as pccMeteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge as pccRouge\n",
    "from pycocoevalcap.bleu.bleu import Bleu as pccBleu\n",
    "\n",
    "\n",
    "def remove_punc(s):\n",
    "    return s.replace('?', '').replace('.', '').replace('!', '')\n",
    "\n",
    "def update_examples(examples: dict, key, values):\n",
    "    assert len(examples) == len(values)\n",
    "\n",
    "    for example, value in zip(examples, values):\n",
    "        example[key] = value\n",
    "\n",
    "\n",
    "def add_bleu_predictions(mocha_dataset, order: int=4):\n",
    "    BLEU = pccBleu(order)\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = {i: [remove_punc(instance['reference'])] for i, instance in\n",
    "                enumerate(examples.values())}\n",
    "        cands = {i: [remove_punc(instance['candidate'])] for i, instance in\n",
    "                 enumerate(examples.values())}\n",
    "        \n",
    "        # compute_scores return (aggregate-bleu, instance-wise bleu)\n",
    "        # -- by accessing the first index, we get the bleu per instance\n",
    "        bleu_scores = BLEU.compute_score(refs, cands, verbose=0)[1]\n",
    "        \n",
    "        for i in range(order):\n",
    "            update_examples(examples.values(), f\"bleu{i+1}\", bleu_scores[i])\n",
    "\n",
    "\n",
    "def add_meteor_predictions(mocha_dataset):\n",
    "    METEOR = pccMeteor()\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = {i: [remove_punc(instance['reference'])] for i, instance in\n",
    "                enumerate(examples.values())}\n",
    "        cands = {i: [remove_punc(instance['candidate'])] for i, instance in\n",
    "                 enumerate(examples.values())}\n",
    "        pred_scores = METEOR.compute_score(refs, cands)[1]\n",
    "        update_examples(examples.values(), \"meteor\", pred_scores)\n",
    "\n",
    "\n",
    "def add_rouge_predictions(mocha_dataset):\n",
    "    ROUGE = pccRouge()\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = {i: [remove_punc(instance['reference'])] for i, instance in\n",
    "                enumerate(examples.values())}\n",
    "        cands = {i: [remove_punc(instance['candidate'])] for i, instance in\n",
    "                 enumerate(examples.values())}\n",
    "        pred_scores = ROUGE.compute_score(refs, cands)[1]\n",
    "        update_examples(examples.values(), \"rougeL\", pred_scores)\n",
    "\n",
    "        \n",
    "def add_bertscore_predictions(mocha_dataset):\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        refs = [remove_punc(instance['reference']) for instance in examples.values()]\n",
    "        cands = [remove_punc(instance['candidate']) for instance in examples.values()]\n",
    "        pred_scores = BERT_SCORE(cands, refs, lang='en')[-1].tolist()\n",
    "        update_examples(examples.values(), \"bertscore\", pred_scores)\n",
    "\n",
    "\n",
    "def add_first_error_position(mocha_dataset):    \n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "def add_bleurt_predictions(mocha_dataset):\n",
    "    BLEURT = load_metric(\"bleurt\", keep_in_memory=True)\n",
    "\n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            scores = BLEURT.compute(predictions=[remove_punc(example[\"candidate\"])],\n",
    "                                    references=[remove_punc(example[\"reference\"])])\n",
    "            example[\"bleurt\"] = scores[\"scores\"][0]\n",
    "\n",
    "\n",
    "def add_recall_predictions(mocha_dataset):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "def add_precision_predictions(mocha_dataset):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "def add_rouge_n_predictions(mocha_dataset, order):\n",
    "    raise NotImplementedError\n",
    "\n",
    "def add_edit_score(mocha_dataset, order):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def add_char_edit_rate(mocha_dataset):\n",
    "    \"\"\"Computes the character edit distance between candidate-reference pairs.\n",
    "    \n",
    "    The character error rate is computed as \n",
    "    $CER = (S + D + I) / N$ (or equivalently $CER = (S + D + I) / (S + D + C)$),\n",
    "\n",
    "    where S, D, and I are the number of substitutions, deletions, and insertions,\n",
    "    and where N is the number of characters in the reference.\n",
    "    \n",
    "    The lower the metric value the better (best value is 0 and is \"unbounded\").\n",
    "    \n",
    "    \"\"\"\n",
    "    # https://github.com/huggingface/datasets/tree/master/metrics/cer\n",
    "    # !pip install jiwer\n",
    "    CER = load_metric(\"cer\", keep_in_memory=True)\n",
    "    \n",
    "    for dataset, examples in mocha_dataset.items():\n",
    "        for example in examples.values():\n",
    "            candidate = remove_punc(example[\"candidate\"])\n",
    "            reference = remove_punc(example[\"reference\"])\n",
    "\n",
    "            scores = CER.compute(predictions=[candidate], references=[reference])\n",
    "            example[\"char_edit_score\"] = scores\n",
    "            \n",
    "def add_word_movers_distance(mocha_dataset):\n",
    "    # https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2e3d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_bleu_predictions(data)\n",
    "# add_meteor_predictions(data)\n",
    "# add_rouge_predictions(data)\n",
    "# add_bertscore_predictions(data)\n",
    "# add_bleurt_predictions(data)\n",
    "# add_char_edit_rate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d17320dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidate': 'between the ages of 10 to 29',\n",
       " 'context': 'The age distribution, , in Lausanne is; 11,818 children or 9.4% of the population are between 0 and 9 years old and 12,128 teenagers or 9.7% are between 10 and 19. Of the adult population, 21,101 people or 16.8% of the population are between 20 and 29 years old. 22,158 people or 17.6% are between 30 and 39, 18,016 people or 14.4% are between 40 and 49, and 13,940 people or 11.1% are between 50 and 59. The senior population distribution is 11,041 people or 8.8% of the population are between 60 and 69 years old, 8,277 people or 6.6% are between 70 and 79, there are 5,896 people or 4.7% who are between 80 and 89, and there are 1,171 people or 0.9% who are 90 and older.',\n",
       " 'metadata': {'scores': [5, 5], 'source': 'naqanet'},\n",
       " 'question': 'Are more people between the ages of 10 to 29 or 80 and older?',\n",
       " 'reference': '10 to 29',\n",
       " 'score': 5,\n",
       " 'bleurt': -0.44349393248558044,\n",
       " 'char_edit_score': 2.5}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"drop\"][\"01d2dcd528219ac0739e8e07030ae88b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90de80d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\n",
      "  Using cached jiwer-2.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting python-Levenshtein==0.12.2\n",
      "  Using cached python_Levenshtein-0.12.2-cp39-cp39-linux_x86_64.whl\n",
      "Requirement already satisfied: setuptools in /home/kat/miniconda3/envs/eqqa-env/lib/python3.9/site-packages (from python-Levenshtein==0.12.2->jiwer) (61.3.0)\n",
      "Installing collected packages: python-Levenshtein, jiwer\n",
      "Successfully installed jiwer-2.3.0 python-Levenshtein-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dd4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
