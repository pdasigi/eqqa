{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdeca9b1",
   "metadata": {},
   "source": [
    "To run this script, download the [MOCHA.tar.gz](https://github.com/anthonywchen/MOCHA/blob/main/data/mocha.tar.gz) and extract it to `data/mocha`. This script will use the following frameworks:\n",
    "- `SPACY` for tokenization;\n",
    "- HuggingFace `datasets` for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa7630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "import json\n",
    "\n",
    "# Tokenization\n",
    "import spacy\n",
    "\n",
    "# Evaluation\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory: where to look for a model\n",
    "DATA_DIR = \"../data/mocha/\"\n",
    "\n",
    "SPLIT = \"dev\"\n",
    "# Full filepath to load\n",
    "FILEPATH = f\"{DATA_DIR}/dev.json\"\n",
    "\n",
    "# ouput directory\n",
    "OUTPUT_DIR = \"../outputs/proxy_metrics\"\n",
    "\n",
    "# Evaluation tokenizer\n",
    "EVALUATION_TOKENIZER = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caefbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(FILEPATH))\n",
    "print(\"Number of examples:\", len(data))\n",
    "next(iter(data.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37f5b7",
   "metadata": {},
   "source": [
    "### Token overlap metrics\n",
    "\n",
    "Current QA evaluation relies on string matching or token overlap metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df93f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU = datasets.load_metric(\"bleu\", keep_in_memory=True)\n",
    "BLEURT = datasets.load_metric(\"bleurt\", keep_in_memory=True)\n",
    "#^Note: requires installing bleurt !pip install git+https://github.com/google-research/bleurt.git\n",
    "ROUGE = datasets.load_metric(\"rouge\", keep_in_memory=True) \n",
    "#^Note: requires installing rouge-score (!pip install rouge-score)\n",
    "\n",
    "METEOR = datasets.load_metric(\"meteor\", keep_in_memory=True)\n",
    "EXACT_MATCH = datasets.load_metric(\"exact_match\", keep_in_memory=True)\n",
    "\n",
    "BERT_SCORE = datasets.load_metric(\"bertscore\", keep_in_memory=True)\n",
    "#^Note: requires installing bert-score: https://pypi.org/project/bert-score/\n",
    "\n",
    "EDIT_RATIO = datasets.load_metric(\"ter\", keep_in_memory=True)\n",
    "#^Note: requires installing sacrebleu\n",
    "# pip install sacrebleu sacrebleu\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Union\n",
    "import logging\n",
    "\n",
    "\n",
    "Tokens = List[str]\n",
    "Text = Union[str, Tokens]\n",
    "\n",
    "\n",
    "def exact_match(y_true: Text, y_pred: Text) -> int:\n",
    "    \"\"\"Determine whether two texts (or sequences of tokens) are equal.\"\"\"\n",
    "    if isinstance(y_true, str) and isinstance(y_pred, str):\n",
    "        return int(y_true == y_pred)\n",
    "\n",
    "    elif isinstance(y_true, (list, tuple)) and isinstance(y_pred, (list, tuple)):\n",
    "        if len(y_true) != len(y_pred):\n",
    "            logging.debug(\n",
    "                f\"Dimension mismatch (default value is 0): {y_true} vs {y_pred}\"\n",
    "            )\n",
    "            return 0\n",
    "        return int(all(map(lambda t1, t2: t1 == t2, y_true, y_pred)))\n",
    "    else:\n",
    "        error_msg = f\"y_true ({type(y_true)}) and y_pred ({type(y_pred)})\"\n",
    "        raise ValueError(\n",
    "            f\"Cannot compare `exact_match` for argument types: {error_msg}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def first_error_position(y_true: Tokens, y_pred: Tokens, no_err_val: int = None) -> int:\n",
    "    \"\"\"Determine the position in the predicted sequence of the first error.\n",
    "    Notes\n",
    "    -----\n",
    "    If both text sequences are equivalent we return ``no_err_val`` as the position.\n",
    "    Otherwise, we iterate for each token in ``y_pred`` and look for the first\n",
    "    mismatch between ``y_pred`` and ``y_true`` tokens returning that position.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> y_true = [\"The\", \"sky\", \"is\", \"blue\"]\n",
    "    >>> y_pred = [\"A\", \"sky\", \"is\", \"blue\"]\n",
    "    >>> first_error_position(y_true, y_pred)\n",
    "    1\n",
    "    >>> y_pred = [\"The\", \"sky\", \"IS\", \"blue\"]\n",
    "    >>> first_error_position(y_true, y_pred)\n",
    "    3\n",
    "    >>> first_error_position(y_true, y_true, no_err_val=-1)\n",
    "    -1\n",
    "    \"\"\"\n",
    "    assert isinstance(y_true, (list, tuple)) and len(y_true) != 0\n",
    "    assert isinstance(y_pred, (list, tuple)) and len(y_pred) != 0\n",
    "\n",
    "    # When no error occurs return the `no_err_val`\n",
    "    if exact_match(y_true, y_pred):\n",
    "        return no_err_val\n",
    "\n",
    "    # If there are differences then they are one of two types:\n",
    "    # 1. Token mismatch: which will occur in the common length of\n",
    "    # the two sequences. Values can vary between 0 and min(lengths)\n",
    "    # 2. Misnumber of tokens: one of the sequences is longer than the\n",
    "    # other, causing them to be wrong.\n",
    "    max_mismatch_ix = min(len(y_true), len(y_pred))\n",
    "\n",
    "    for i in range(max_mismatch_ix):\n",
    "        if y_true[i] != y_pred[i]:\n",
    "            return i\n",
    "    return max_mismatch_ix\n",
    "\n",
    "\n",
    "def _precision(tp, fp, tn, fn) -> float:\n",
    "    return 0 if tp == 0 else tp / (tp + fp)\n",
    "\n",
    "\n",
    "def _recall(tp, fp, tn, fn) -> float:\n",
    "    return 0 if tp == 0 else tp / (tp + fn)\n",
    "\n",
    "\n",
    "def _critical_success_index(tp, fp, tn, fn):\n",
    "    \"Ratio of positives w.r.t. number of errors (also dubbed threat score).\"\n",
    "    return 0 if tp == 0 else tp / (tp + fn + fp)\n",
    "\n",
    "\n",
    "def _f1_score(precision=None, recall=None, **kwargs) -> float:\n",
    "    if precision is not None and recall is not None:\n",
    "        p = precision\n",
    "        r = recall\n",
    "        # return if precision or recall are 0\n",
    "        if p == 0 or r == 0:\n",
    "            return 0\n",
    "    else:\n",
    "        p = _precision(**kwargs)\n",
    "        r = _recall(**kwargs)\n",
    "\n",
    "    return (2 * p * r) / (p + r)\n",
    "\n",
    "\n",
    "def f_metrics(references: Tokens, predictions: Tokens) -> Dict[str, float]:\n",
    "    true_tokens, pred_tokens = Counter(references), Counter(predictions)\n",
    "    tp = sum((true_tokens & pred_tokens).values())\n",
    "    fp = len(predictions) - tp\n",
    "    fn = len(references) - tp\n",
    "    tn = 0\n",
    "    assert tp + fp + fn == sum((true_tokens | pred_tokens).values())\n",
    "\n",
    "    prec = _precision(tp=tp, fp=fp, tn=tn, fn=fn)\n",
    "    rec = _recall(tp=tp, fp=fp, tn=tn, fn=fn)\n",
    "    return {\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1_score\": _f1_score(precision=prec, recall=rec),\n",
    "        \"csi\": _critical_success_index(tp=tp, fp=fp, tn=tn, fn=fn),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts, tokens=True, sep: str = \" \", tokenizer=EVALUATION_TOKENIZER) -> list:\n",
    "    arg_is_str = isinstance(texts, str)\n",
    "    if arg_is_str:\n",
    "        texts = [texts]\n",
    "\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        text_tokens = tokenizer(text)\n",
    "        text_tokens = [str(t) for t in text_tokens]\n",
    "        results.append(text_tokens if tokens else [sep.join(text_tokens)])\n",
    "\n",
    "    return results[0] if arg_is_str else results\n",
    "\n",
    "\n",
    "def remove_punc(s):\n",
    "    return s.replace('?', '').replace('.', '').replace('!', '')\n",
    "\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    import re, string\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "assert tokenize(\"Hello, world!\") == ['Hello', ',', 'world', '!']\n",
    "assert tokenize([\"Hello, world!\"]) == [['Hello', ',', 'world', '!']]\n",
    "assert tokenize([\"Hello, world!\", \"Ola, mundo!\"]) == [['Hello', ',', 'world', '!'], [\"Ola\", \",\", \"mundo\", \"!\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c25a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(data):\n",
    "    references = {example_id: (example[\"reference\"]) for example_id, example in data.items()}\n",
    "    candidates = {example_id: (example[\"candidate\"]) for example_id, example in data.items()}\n",
    "    human_judgement = {example_id: (example[\"score\"]) for example_id, example in data.items()}\n",
    "    \n",
    "    return references, candidates, human_judgement\n",
    "\n",
    "\n",
    "def apply(d: dict, fn):\n",
    "    return {k: fn(elem) for k, elem in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbdc2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(**kwargs):\n",
    "    results = ROUGE.compute(**kwargs)\n",
    "    return {rouge_type: rouge.mid.fmeasure for rouge_type, rouge in results.items()}\n",
    "\n",
    "def compute_bleu(max_order=4, **kwargs):\n",
    "    bleu = {}\n",
    "    for i in range(1, max_order+1):\n",
    "        try: \n",
    "            results = BLEU.compute(**kwargs, max_order=i)[\"bleu\"]\n",
    "            bleu[f\"bleu{i}\"] = results\n",
    "        except: \n",
    "            bleu[f\"bleu{i}\"] = 0\n",
    "        \n",
    "    return bleu\n",
    "\n",
    "def compute_edit_score(**kwargs):\n",
    "    results = EDIT_RATIO.compute(**kwargs)\n",
    "    # Note: {'score': 133.33333333333331, 'num_edits': 4, 'ref_length': 3.0}\n",
    "    results[\"edit_score\"] = results.pop(\"score\") / 100\n",
    "    results.pop(\"ref_length\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704aefc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(data):\n",
    "    # Separate data in references and candidates\n",
    "    references, candidates, human_judgements = get_examples(data)\n",
    "    assert len(references) == len(candidates)\n",
    "\n",
    "    ## Compute metrics that rely on strings ()\n",
    "    # references_norm = apply(references, fn=normalize_answer)\n",
    "    # candidates_norm = apply(candidates, fn=normalize_answer)\n",
    "    ## ^NOTE: This was causing some inconsistency in the results since\n",
    "    ## there were a few examples where the candidate answer is simply\n",
    "    ## \"the\" or \"an\" --> therefore becoming empty str\n",
    "    references_norm = apply(references, fn=remove_punc)\n",
    "    candidates_norm = apply(candidates, fn=remove_punc)\n",
    "    \n",
    "    # Tokenize data\n",
    "    references_tokens = apply(references_norm, fn=tokenize)\n",
    "    candidates_tokens = apply(candidates_norm, fn=tokenize)\n",
    "\n",
    "    metric_results = []\n",
    "    for example_id, correctness in human_judgements.items():\n",
    "        reference = references_norm[example_id]\n",
    "        candidate = candidates_norm[example_id]\n",
    "\n",
    "        reference_tks = references_tokens[example_id]\n",
    "        candidate_tks = candidates_tokens[example_id]    \n",
    "\n",
    "        metrics = {\n",
    "            \"example_id\": example_id,\n",
    "\n",
    "            \"reference\": reference,\n",
    "            \"candidate\": candidate,\n",
    "\n",
    "            \"reference_tokens\": reference_tks,\n",
    "            \"candidate_tokens\": candidate_tks,\n",
    "\n",
    "            \"human_correctness_original\": correctness,\n",
    "            \"human_correctness\": (correctness - 1) / (5-1),\n",
    "        }\n",
    "\n",
    "        text_args = {\"predictions\": [candidate], \"references\": [reference]}\n",
    "        metrics.update(text_args)\n",
    "        metrics.update(EXACT_MATCH.compute(**text_args))\n",
    "        metrics.update(METEOR.compute(**text_args))\n",
    "        metrics.update(compute_rouge(**text_args))\n",
    "        metrics.update({\"bleurt\": BLEURT.compute(**text_args)[\"scores\"][0]})\n",
    "\n",
    "        # keyword arguments for BLEU-like metrics\n",
    "        if not candidate:\n",
    "            print(data[example_id], metrics)\n",
    "            \n",
    "        token_args_w_mult_refs_args = {\"predictions\": [candidate_tks], \"references\": [[reference_tks]]}\n",
    "        metrics.update(compute_bleu(**token_args_w_mult_refs_args))\n",
    "\n",
    "        token_args = {\"predictions\": candidate_tks, \"references\": reference_tks}\n",
    "        metrics.update(f_metrics(**token_args))\n",
    "\n",
    "\n",
    "        text_w_mult_refs_args = {\"predictions\": [candidate], \"references\": [[reference]]}\n",
    "        # TER score (num_edits / sum_ref_lengths * 100)\n",
    "        metrics.update(compute_edit_score(**text_w_mult_refs_args))\n",
    "\n",
    "        metric_results.append(metrics)\n",
    "\n",
    "    return metric_results\n",
    "\n",
    "# sanity check (:\n",
    "d = compute_metrics(data[\"drop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ac2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5906c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(metrics, split, dataset=None, output_dir=OUTPUT_DIR):\n",
    "    import os\n",
    "    # Write predictions into `<output_dir>/<dataset>_<split>/`\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    output_file = f\"{output_dir}/{split}_{dataset}_metrics.csv.gz\" \n",
    "    print(\"Writing metrics at\", output_file)\n",
    "    metrics.to_csv(output_file, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_metrics(split, dataset=None, input_dir=DATA_DIR, output_dir=OUTPUT_DIR):\n",
    "    _filepath = f\"{input_dir}/{split}.json\"\n",
    "\n",
    "    data = json.load(open(_filepath))\n",
    "    \n",
    "    if isinstance(dataset, str):\n",
    "        dataset = [dataset]\n",
    "\n",
    "    data = {k: v for k, v in data.items() if (dataset is None) or (k in dataset)}\n",
    "    print(\"Number of datasets:\", len(data))\n",
    "\n",
    "    all_metrics = []\n",
    "\n",
    "    for dataset_name, dataset in data.items():\n",
    "        print(\"Computing metrics\", len(dataset),\"examples of dataset\", dataset_name)\n",
    "        metrics = compute_metrics(dataset)\n",
    "        metrics = pd.DataFrame(metrics)\n",
    "\n",
    "        metrics[\"dataset\"] = dataset_name\n",
    "        metrics[\"split\"] = split\n",
    "        \n",
    "        write_predictions(metrics, split=split, dataset=dataset_name, output_dir=output_dir)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    all_metrics = pd.concat(all_metrics).reset_index(drop=True)\n",
    "    \n",
    "    if len(data) > 1:\n",
    "        write_predictions(metrics, split=split, dataset=\"all_datasets\", output_dir=output_dir)\n",
    "        \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1acdf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_metrics = persist_metrics(\"train\")\n",
    "train_metrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dev_metrics = persist_metrics(\"dev\")\n",
    "dev_metrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d703e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"{OUTPUT_DIR}/dev_all_datasets_metrics.csv.gz\", index_col=0).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bfd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"{OUTPUT_DIR}/dev_all_datasets_metrics.csv.gz\", index_col=0).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as BERT_SCORE\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.bleu.bleu import Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fedaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Bleu(1).compute_score({0: [reference]}, {0: [candidate]})[1][0])\n",
    "print(Bleu(2).compute_score({0: [reference]}, {0: [candidate]})[1][0])\n",
    "print(Bleu(3).compute_score({0: [reference]}, {0: [candidate]})[1][0])\n",
    "print(Bleu(4).compute_score({0: [reference]}, {0: [candidate]})[1][0])\n",
    "print(Meteor().compute_score({0: [reference]}, {0: [candidate]})[1])\n",
    "print(Rouge().compute_score({0: [reference]}, {0: [candidate]})[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
