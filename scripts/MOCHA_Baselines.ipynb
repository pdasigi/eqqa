{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c200437",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Train and evaluate the baselines. Baselines are: \n",
    "\n",
    "- Linear Regression\n",
    "- L1 regression (Lasso Regression)\n",
    "- Decision Tree \n",
    "- Random Forests \n",
    "\n",
    "For every baseline above (except L1), we'll also consider a PCA version where we first reduce the dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf925ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read dataset with 31069 examples from ../outputs/metrics/proxy_metrics_20220426/train_all_datasets_metrics.csv.gz\n",
      "Read (holdout) dataset with 4009 examples from ../outputs/metrics/proxy_metrics_20220426/dev_all_datasets_metrics.csv.gz\n",
      "Features:\t ['exact_match', 'meteor', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'bleurt', 'bert-score', 'bleu1', 'bleu2', 'bleu3', 'bleu4', 'bleu-precision0', 'bleu-precision1', 'bleu-precision2', 'bleu-precision3', 'precision', 'recall', 'f1_score', 'csi', 'num_edits', 'edit_score'] \n",
      "\n",
      "Target:\t human_correctness\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "METRICS_DIR = \"../outputs/metrics/proxy_metrics_20220426\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# DATA LOADING\n",
    "# ---------------------------------------------------------------\n",
    "TRAIN_FILEPATH = f\"{METRICS_DIR}/train_all_datasets_metrics.csv.gz\"\n",
    "DEV_FILEPATH = f\"{METRICS_DIR}/dev_all_datasets_metrics.csv.gz\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILEPATH, index_col=0)\n",
    "print(f\"Read dataset with {len(train_df)} examples from {TRAIN_FILEPATH}\")\n",
    "\n",
    "dev_df = pd.read_csv(DEV_FILEPATH, index_col=0)\n",
    "print(f\"Read (holdout) dataset with {len(dev_df)} examples from {DEV_FILEPATH}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# FEATURES and TARGET \n",
    "# ---------------------------------------------------------------\n",
    "METRIC_COLS = train_df.select_dtypes(\"number\").columns\n",
    "features = list(METRIC_COLS[2:])\n",
    "\n",
    "# Target column will be the normalized human correctness\n",
    "target = METRIC_COLS[1]\n",
    "print(\"Features:\\t\", features, \"\\n\\nTarget:\\t\", target)\n",
    "      \n",
    "DATASET_NAMES = sorted(train_df[\"dataset\"].unique())\n",
    "\n",
    "TRAIN_DATASETS = {\"all\": train_df}\n",
    "TRAIN_DATASETS.update({d: train_df[train_df[\"dataset\"] == d] for d in DATASET_NAMES})\n",
    "\n",
    "EVAL_DATASETS = {\"all\": dev_df}\n",
    "EVAL_DATASETS.update({d: dev_df[dev_df[\"dataset\"] == d] for d in DATASET_NAMES})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21c214",
   "metadata": {},
   "source": [
    "TODO \n",
    "- [ ] Compute Baselines (Avg metric)\n",
    "- [ ] Compute Linear Regression \n",
    "- [ ] L1 regression\n",
    "- [ ] Decision Tree\n",
    "- [ ] Random Forest\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f670c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPreprocessing:\n",
    "    def fit(self, *args, **kwargs): \n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, *args, **kwargs): \n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X, *args, **kwargs): \n",
    "        return X\n",
    "    \n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, model_hparams, dataset, features, target, seed=81263):\n",
    "        self.model_class = model_class\n",
    "        self.model_hparams = model_hparams\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.seed = seed\n",
    "        \n",
    "    def load_data(self, data):\n",
    "        \"\"\"\"\"\"\n",
    "        print(f\"Loading dataset '{self.dataset}'\")\n",
    "        if self.dataset == \"all\":\n",
    "            data = data.copy()\n",
    "        else:\n",
    "            data = data[data[\"dataset\"] == self.dataset].copy()\n",
    "\n",
    "        self.X_train = data[self.features]\n",
    "        self.y_train = data[self.target]\n",
    "        \n",
    "    def split(self, holdout_fraction=0.2):\n",
    "        \"\"\"\"\"\"\n",
    "        print(f\"Splitting dataset holdout_fraction={holdout_fraction}\")\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X_train, self.y_train,\n",
    "            test_size=holdout_fraction, \n",
    "            random_state=self.seed, \n",
    "            stratify=self.y_train,\n",
    "        )\n",
    "            \n",
    "        self.X_train, self.X_test = X_train, X_test\n",
    "        self.y_train, self.y_test = y_train, y_test        \n",
    "        \n",
    "    def preprocess(self, with_std=True, with_pca=False, **kwargs):\n",
    "        \"\"\"\"\"\"\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.decomposition import PCA\n",
    "        \n",
    "        operations = []\n",
    "        \n",
    "        if with_std:\n",
    "            print(\"Using StandardScaler\")\n",
    "            operations.append(StandardScaler())\n",
    "        if with_pca:\n",
    "            print(\"Using PCA\")\n",
    "            operations.append(PCA(random_state=self.seed, **kwargs))\n",
    "        \n",
    "        self.preproc_fn = make_pipeline(*operations) \\\n",
    "            if len(operations) > 0 else NoPreprocessing()\n",
    "        \n",
    "        self.preproc_fn.fit(self.X_train)\n",
    "        self.X_train = self.preproc_fn.transform(self.X_train)\n",
    "        \n",
    "        if getattr(self, \"X_test\", None) is not None:\n",
    "            self.X_test = self.preproc_fn.transform(self.X_test)\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\"\"\"\n",
    "        self.model = self.model_class(**self.model_hparams)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def evaluate(self, eval_dataset=None):\n",
    "        \"\"\"\"\"\"\n",
    "        import sklearn.metrics as m\n",
    "        import scipy.stats as st\n",
    "        \n",
    "        if eval_dataset is None:\n",
    "            print(\"Evaluating holdout dev set.\")\n",
    "            X_test, y_test = self.X_test, self.y_test\n",
    "        else:\n",
    "            X_test = eval_dataset[self.features]\n",
    "            y_test = eval_dataset[self.target]    \n",
    "            X_test = self.preproc_fn.transform(X_test)\n",
    "        \n",
    "        # Evaluation\n",
    "        scores = self.model.predict(X_test)\n",
    "        \n",
    "        return {\n",
    "            \"mse\": m.mean_squared_error(y_pred=scores, y_true=y_test),\n",
    "            \"r2\": m.r2_score(y_pred=scores, y_true=y_test),\n",
    "            \"pearson\": st.pearsonr(scores, y_test)[0],\n",
    "            \"spearman\": st.spearmanr(scores, y_test)[0],\n",
    "            \"trained_on\": self.dataset,\n",
    "        }\n",
    "        \n",
    "    def evaluate_multiple(self, eval_datasets: dict):\n",
    "        all_results = []\n",
    "        \n",
    "        for name, eval_dataset in eval_datasets.items():\n",
    "            eval_result = self.evaluate(eval_dataset)\n",
    "            eval_result[\"evaluated_on\"] = name\n",
    "            all_results.append(eval_result)\n",
    "            \n",
    "        return all_results\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Abstract features and targets\n",
    "general_pipeline = partial(Pipeline, features=features, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "55d7203b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_model_selection(model_class, model_hyperparams={}):\n",
    "    pipelines = {}\n",
    "    all_results = []\n",
    "    for train_name, train_data in TRAIN_DATASETS.items(): \n",
    "        p = general_pipeline(model_class, model_hyperparams, dataset=train_name)\n",
    "\n",
    "        p.load_data(train_data)\n",
    "        p.split(holdout_fraction=0.2)\n",
    "        p.preprocess(with_std=True)\n",
    "        p.fit()\n",
    "        \n",
    "        result = p.evaluate()\n",
    "        result[\"model_class\"] = model_class.__name__\n",
    "        result[\"model_hyperparams\"] = str(model_hyperparams)\n",
    "        \n",
    "        pipelines[\"dataset\"] = p\n",
    "        all_results.append(result)\n",
    "\n",
    "    return pd.DataFrame(all_results), pipelines\n",
    "\n",
    "\n",
    "def run_eval(model_class, model_hyperparams={}):\n",
    "                           \n",
    "    all_results = []\n",
    "    for train_name, train_data in TRAIN_DATASETS.items(): \n",
    "        p = general_pipeline(model_class, model_hyperparams, dataset=train_name)\n",
    "\n",
    "        p.load_data(train_data)\n",
    "        # p.split(holdout_fraction=0.2)\n",
    "        # ^Note: We want to train with the whole training data when evaluating\n",
    "        p.preprocess(with_std=True)\n",
    "        p.fit()\n",
    "        all_results.append(p.evaluate_multiple(EVAL_DATASETS))\n",
    "\n",
    "    return pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "532d83fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'all'\n",
      "Splitting dataset holdout_fraction=0.2\n",
      "Using StandardScaler\n",
      "Evaluating holdout dev set.\n",
      "Loading dataset 'cosmosqa'\n",
      "Splitting dataset holdout_fraction=0.2\n",
      "Using StandardScaler\n",
      "Evaluating holdout dev set.\n",
      "Loading dataset 'drop'\n",
      "Splitting dataset holdout_fraction=0.2\n",
      "Using StandardScaler\n",
      "Evaluating holdout dev set.\n",
      "Loading dataset 'mcscript'\n",
      "Splitting dataset holdout_fraction=0.2\n",
      "Using StandardScaler\n",
      "Evaluating holdout dev set.\n",
      "Loading dataset 'narrativeqa'\n",
      "Splitting dataset holdout_fraction=0.2\n",
      "Using StandardScaler\n",
      "Evaluating holdout dev set.\n",
      "Loading dataset 'quoref'\n",
      "Splitting dataset holdout_fraction=0.2\n",
      "Using StandardScaler\n",
      "Evaluating holdout dev set.\n",
      "Loading dataset 'socialiqa'\n",
      "Splitting dataset holdout_fraction=0.2\n",
      "Using StandardScaler\n",
      "Evaluating holdout dev set.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>r2</th>\n",
       "      <th>pearson</th>\n",
       "      <th>spearman</th>\n",
       "      <th>trained_on</th>\n",
       "      <th>model_class</th>\n",
       "      <th>model_hyperparams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093295</td>\n",
       "      <td>0.414422</td>\n",
       "      <td>0.643876</td>\n",
       "      <td>0.641206</td>\n",
       "      <td>all</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050313</td>\n",
       "      <td>0.678596</td>\n",
       "      <td>0.823920</td>\n",
       "      <td>0.761769</td>\n",
       "      <td>cosmosqa</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072685</td>\n",
       "      <td>0.432717</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.674960</td>\n",
       "      <td>drop</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110449</td>\n",
       "      <td>0.356385</td>\n",
       "      <td>0.597036</td>\n",
       "      <td>0.594506</td>\n",
       "      <td>mcscript</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.108332</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>0.611243</td>\n",
       "      <td>0.612935</td>\n",
       "      <td>narrativeqa</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.042555</td>\n",
       "      <td>0.534231</td>\n",
       "      <td>0.732197</td>\n",
       "      <td>0.721967</td>\n",
       "      <td>quoref</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.070787</td>\n",
       "      <td>0.536266</td>\n",
       "      <td>0.732457</td>\n",
       "      <td>0.721446</td>\n",
       "      <td>socialiqa</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mse        r2   pearson  spearman   trained_on       model_class  \\\n",
       "0  0.093295  0.414422  0.643876  0.641206          all  LinearRegression   \n",
       "1  0.050313  0.678596  0.823920  0.761769     cosmosqa  LinearRegression   \n",
       "2  0.072685  0.432717  0.664384  0.674960         drop  LinearRegression   \n",
       "3  0.110449  0.356385  0.597036  0.594506     mcscript  LinearRegression   \n",
       "4  0.108332  0.373252  0.611243  0.612935  narrativeqa  LinearRegression   \n",
       "5  0.042555  0.534231  0.732197  0.721967       quoref  LinearRegression   \n",
       "6  0.070787  0.536266  0.732457  0.721446    socialiqa  LinearRegression   \n",
       "\n",
       "  model_hyperparams  \n",
       "0                {}  \n",
       "1                {}  \n",
       "2                {}  \n",
       "3                {}  \n",
       "4                {}  \n",
       "5                {}  \n",
       "6                {}  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "run_model_selection(LinearRegression)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a34e976",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2979854791.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [99]\u001b[0;36m\u001b[0m\n\u001b[0;31m    alpha_configs = {\"alpha\": }\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# -------------------------------------------\n",
    "# Linear regression\n",
    "# -------------------------------------------\n",
    "rs = run_model_selection(LinearRegression)[0]\n",
    "results.append(rs)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# Lasso regression\n",
    "# -------------------------------------------\n",
    "alpha_configs = {\"alpha\": }\n",
    "rs = run_model_selection(Lasso)[0]\n",
    "results.append(rs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f063201c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_parse_args() missing 1 required positional argument: 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [106]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParameterSampler\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lognorm\n\u001b[0;32m----> 4\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mlognorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m}\n\u001b[1;32m      5\u001b[0m param_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ParameterSampler(param_grid, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/eqqa-env/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:866\u001b[0m, in \u001b[0;36mrv_generic.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m--> 866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eqqa-env/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:863\u001b[0m, in \u001b[0;36mrv_generic.freeze\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfreeze\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;124;03m\"\"\"Freeze the distribution for the given arguments.\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrv_frozen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eqqa-env/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:441\u001b[0m, in \u001b[0;36mrv_frozen.__init__\u001b[0;34m(self, dist, *args, **kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# create a new instance\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdist\u001b[38;5;241m.\u001b[39m_updated_ctor_param())\n\u001b[0;32m--> 441\u001b[0m shapes, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39m_get_support(\u001b[38;5;241m*\u001b[39mshapes)\n",
      "\u001b[0;31mTypeError\u001b[0m: _parse_args() missing 1 required positional argument: 's'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "param_grid = {'a':[1, 2], 'b': lognorm(loc=0, scale=1)}\n",
    "param_list = list(ParameterSampler(param_grid, n_iter=4, random_state=1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5acab6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'socialiqa'\n",
      "Splitting dataset holdout_fraction=0.2\n",
      "Using StandardScaler\n",
      "Evaluating holdout dev set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kat/miniconda3/envs/eqqa-env/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/kat/miniconda3/envs/eqqa-env/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4529: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(SpearmanRConstantInputWarning())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse': 0.15264512887306073,\n",
       " 'r2': -4.773249147049796e-07,\n",
       " 'pearson': nan,\n",
       " 'spearman': nan,\n",
       " 'trained_on': 'socialiqa'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "p = general_pipeline(Lasso, {\"alpha\": 1}, dataset=train_name)\n",
    "\n",
    "p.load_data(train_data)\n",
    "p.split(holdout_fraction=0.2)\n",
    "p.preprocess(with_std=True)\n",
    "p.fit()\n",
    "p.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4c8829ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4b1dc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bleurt'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf214237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lasso(alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2388c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80efd487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c92463c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameGroupBy' object has no attribute 'unstack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrained_on\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluated_on\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstack\u001b[49m()\n",
      "File \u001b[0;32m~/miniconda3/envs/eqqa-env/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:904\u001b[0m, in \u001b[0;36mGroupBy.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[attr]\n\u001b[0;32m--> 904\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    906\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameGroupBy' object has no attribute 'unstack'"
     ]
    }
   ],
   "source": [
    "pd.crosstab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68b1879b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"7\" halign=\"left\">mse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evaluated_on</th>\n",
       "      <th>all</th>\n",
       "      <th>cosmosqa</th>\n",
       "      <th>drop</th>\n",
       "      <th>mcscript</th>\n",
       "      <th>narrativeqa</th>\n",
       "      <th>quoref</th>\n",
       "      <th>socialiqa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trained_on</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.068703</td>\n",
       "      <td>0.052023</td>\n",
       "      <td>0.101517</td>\n",
       "      <td>0.094995</td>\n",
       "      <td>0.076208</td>\n",
       "      <td>0.043676</td>\n",
       "      <td>0.053389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosmosqa</th>\n",
       "      <td>0.094798</td>\n",
       "      <td>0.037583</td>\n",
       "      <td>0.191385</td>\n",
       "      <td>0.131905</td>\n",
       "      <td>0.115565</td>\n",
       "      <td>0.125797</td>\n",
       "      <td>0.059669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drop</th>\n",
       "      <td>0.096497</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.070072</td>\n",
       "      <td>0.131983</td>\n",
       "      <td>0.115645</td>\n",
       "      <td>0.034393</td>\n",
       "      <td>0.085979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcscript</th>\n",
       "      <td>0.083360</td>\n",
       "      <td>0.066408</td>\n",
       "      <td>0.169750</td>\n",
       "      <td>0.089846</td>\n",
       "      <td>0.081210</td>\n",
       "      <td>0.133941</td>\n",
       "      <td>0.065040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>narrativeqa</th>\n",
       "      <td>0.081593</td>\n",
       "      <td>0.077619</td>\n",
       "      <td>0.177430</td>\n",
       "      <td>0.093936</td>\n",
       "      <td>0.074912</td>\n",
       "      <td>0.099072</td>\n",
       "      <td>0.063187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quoref</th>\n",
       "      <td>0.090523</td>\n",
       "      <td>0.062699</td>\n",
       "      <td>0.076701</td>\n",
       "      <td>0.136007</td>\n",
       "      <td>0.106423</td>\n",
       "      <td>0.025276</td>\n",
       "      <td>0.074943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>socialiqa</th>\n",
       "      <td>0.081158</td>\n",
       "      <td>0.043246</td>\n",
       "      <td>0.195842</td>\n",
       "      <td>0.108009</td>\n",
       "      <td>0.089985</td>\n",
       "      <td>0.117177</td>\n",
       "      <td>0.049950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   mse                                                      \\\n",
       "evaluated_on       all  cosmosqa      drop  mcscript narrativeqa    quoref   \n",
       "trained_on                                                                   \n",
       "all           0.068703  0.052023  0.101517  0.094995    0.076208  0.043676   \n",
       "cosmosqa      0.094798  0.037583  0.191385  0.131905    0.115565  0.125797   \n",
       "drop          0.096497  0.071429  0.070072  0.131983    0.115645  0.034393   \n",
       "mcscript      0.083360  0.066408  0.169750  0.089846    0.081210  0.133941   \n",
       "narrativeqa   0.081593  0.077619  0.177430  0.093936    0.074912  0.099072   \n",
       "quoref        0.090523  0.062699  0.076701  0.136007    0.106423  0.025276   \n",
       "socialiqa     0.081158  0.043246  0.195842  0.108009    0.089985  0.117177   \n",
       "\n",
       "                        \n",
       "evaluated_on socialiqa  \n",
       "trained_on              \n",
       "all           0.053389  \n",
       "cosmosqa      0.059669  \n",
       "drop          0.085979  \n",
       "mcscript      0.065040  \n",
       "narrativeqa   0.063187  \n",
       "quoref        0.074943  \n",
       "socialiqa     0.049950  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results.pivot(index=\"trained_on\", columns=\"evaluated_on\", values=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e45b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 22)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4681cbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mse': 0.06866390421449335,\n",
       "  'r2': 0.5108459457282504,\n",
       "  'pearson': 0.7160323651395801,\n",
       "  'spearman': 0.6973840280825205,\n",
       "  'trained_on': 'all',\n",
       "  'evaluated_on': 'all'},\n",
       " {'mse': 0.051919156141483284,\n",
       "  'r2': 0.6544272576304422,\n",
       "  'pearson': 0.8496092613008309,\n",
       "  'spearman': 0.8070434965692682,\n",
       "  'trained_on': 'all',\n",
       "  'evaluated_on': 'cosmosqa'},\n",
       " {'mse': 0.10178139070316412,\n",
       "  'r2': 0.24327344336532886,\n",
       "  'pearson': 0.6433292806727148,\n",
       "  'spearman': 0.6417500276605237,\n",
       "  'trained_on': 'all',\n",
       "  'evaluated_on': 'drop'},\n",
       " {'mse': 0.09478800133442038,\n",
       "  'r2': 0.3524520806796013,\n",
       "  'pearson': 0.6098660630245658,\n",
       "  'spearman': 0.5850000860484836,\n",
       "  'trained_on': 'all',\n",
       "  'evaluated_on': 'mcscript'},\n",
       " {'mse': 0.07609741456403131,\n",
       "  'r2': 0.49226871096094127,\n",
       "  'pearson': 0.7101910798185168,\n",
       "  'spearman': 0.7233652532110437,\n",
       "  'trained_on': 'all',\n",
       "  'evaluated_on': 'narrativeqa'},\n",
       " {'mse': 0.0440114168748348,\n",
       "  'r2': 0.4116223570589844,\n",
       "  'pearson': 0.7301193495597244,\n",
       "  'spearman': 0.698540199012391,\n",
       "  'trained_on': 'all',\n",
       "  'evaluated_on': 'quoref'},\n",
       " {'mse': 0.05346186999121898,\n",
       "  'r2': 0.6002843188743253,\n",
       "  'pearson': 0.7810650843800475,\n",
       "  'spearman': 0.7567404179897205,\n",
       "  'trained_on': 'all',\n",
       "  'evaluated_on': 'socialiqa'},\n",
       " {'mse': 0.09677345312576584,\n",
       "  'r2': 0.3105966302983145,\n",
       "  'pearson': 0.6206319826814726,\n",
       "  'spearman': 0.6266148282966025,\n",
       "  'trained_on': 'cosmosqa',\n",
       "  'evaluated_on': 'all'},\n",
       " {'mse': 0.037694832284860764,\n",
       "  'r2': 0.7491040391653845,\n",
       "  'pearson': 0.866951085088667,\n",
       "  'spearman': 0.8250022146312719,\n",
       "  'trained_on': 'cosmosqa',\n",
       "  'evaluated_on': 'cosmosqa'},\n",
       " {'mse': 0.20194544335462505,\n",
       "  'r2': -0.5014284922033077,\n",
       "  'pearson': 0.2967858360804036,\n",
       "  'spearman': 0.3971087946743816,\n",
       "  'trained_on': 'cosmosqa',\n",
       "  'evaluated_on': 'drop'},\n",
       " {'mse': 0.13305505544677287,\n",
       "  'r2': 0.09102921153870847,\n",
       "  'pearson': 0.4986024549301564,\n",
       "  'spearman': 0.48260645351030645,\n",
       "  'trained_on': 'cosmosqa',\n",
       "  'evaluated_on': 'mcscript'},\n",
       " {'mse': 0.11689272366318085,\n",
       "  'r2': 0.22007740204033044,\n",
       "  'pearson': 0.6327642005416849,\n",
       "  'spearman': 0.6735893089801195,\n",
       "  'trained_on': 'cosmosqa',\n",
       "  'evaluated_on': 'narrativeqa'},\n",
       " {'mse': 0.13830801389083824,\n",
       "  'r2': -0.8490053034278757,\n",
       "  'pearson': 0.2888101674917586,\n",
       "  'spearman': 0.3563623429422263,\n",
       "  'trained_on': 'cosmosqa',\n",
       "  'evaluated_on': 'quoref'},\n",
       " {'mse': 0.05987234027493951,\n",
       "  'r2': 0.5523554773239996,\n",
       "  'pearson': 0.7584634082016788,\n",
       "  'spearman': 0.73484438436293,\n",
       "  'trained_on': 'cosmosqa',\n",
       "  'evaluated_on': 'socialiqa'},\n",
       " {'mse': 0.10360412302467475,\n",
       "  'r2': 0.26193569392036176,\n",
       "  'pearson': 0.604002375880016,\n",
       "  'spearman': 0.6067831788809068,\n",
       "  'trained_on': 'drop',\n",
       "  'evaluated_on': 'all'},\n",
       " {'mse': 0.09038907904694268,\n",
       "  'r2': 0.39837231095608605,\n",
       "  'pearson': 0.6843079035056604,\n",
       "  'spearman': 0.6688374046676894,\n",
       "  'trained_on': 'drop',\n",
       "  'evaluated_on': 'cosmosqa'},\n",
       " {'mse': 0.06879105782070896,\n",
       "  'r2': 0.4885507070370244,\n",
       "  'pearson': 0.7103155511615447,\n",
       "  'spearman': 0.6091448736640478,\n",
       "  'trained_on': 'drop',\n",
       "  'evaluated_on': 'drop'},\n",
       " {'mse': 0.13544348937688536,\n",
       "  'r2': 0.07471253221110097,\n",
       "  'pearson': 0.5308653963931579,\n",
       "  'spearman': 0.5099450120591018,\n",
       "  'trained_on': 'drop',\n",
       "  'evaluated_on': 'mcscript'},\n",
       " {'mse': 0.12317001585472906,\n",
       "  'r2': 0.17819453815659392,\n",
       "  'pearson': 0.5974112516883089,\n",
       "  'spearman': 0.6195879021027982,\n",
       "  'trained_on': 'drop',\n",
       "  'evaluated_on': 'narrativeqa'},\n",
       " {'mse': 0.03245673955804034,\n",
       "  'r2': 0.566093953007236,\n",
       "  'pearson': 0.7775132467735023,\n",
       "  'spearman': 0.7844557868358326,\n",
       "  'trained_on': 'drop',\n",
       "  'evaluated_on': 'quoref'},\n",
       " {'mse': 0.09212417942869405,\n",
       "  'r2': 0.311219769631468,\n",
       "  'pearson': 0.6376192673552992,\n",
       "  'spearman': 0.639587220482604,\n",
       "  'trained_on': 'drop',\n",
       "  'evaluated_on': 'socialiqa'},\n",
       " {'mse': 0.0841059747148123,\n",
       "  'r2': 0.40083834452944145,\n",
       "  'pearson': 0.6866351303978303,\n",
       "  'spearman': 0.680922626743781,\n",
       "  'trained_on': 'mcscript',\n",
       "  'evaluated_on': 'all'},\n",
       " {'mse': 0.06760002655373269,\n",
       "  'r2': 0.5500557347895112,\n",
       "  'pearson': 0.8370186550045744,\n",
       "  'spearman': 0.7978734829649731,\n",
       "  'trained_on': 'mcscript',\n",
       "  'evaluated_on': 'cosmosqa'},\n",
       " {'mse': 0.17248981280755937,\n",
       "  'r2': -0.28243111239357077,\n",
       "  'pearson': 0.58787803243912,\n",
       "  'spearman': 0.6314848994023072,\n",
       "  'trained_on': 'mcscript',\n",
       "  'evaluated_on': 'drop'},\n",
       " {'mse': 0.08999396976801093,\n",
       "  'r2': 0.38520269386145634,\n",
       "  'pearson': 0.6231388910457308,\n",
       "  'spearman': 0.5993138733018735,\n",
       "  'trained_on': 'mcscript',\n",
       "  'evaluated_on': 'mcscript'},\n",
       " {'mse': 0.0812497425620932,\n",
       "  'r2': 0.4578917462375629,\n",
       "  'pearson': 0.6876464183940573,\n",
       "  'spearman': 0.6968812594722265,\n",
       "  'trained_on': 'mcscript',\n",
       "  'evaluated_on': 'narrativeqa'},\n",
       " {'mse': 0.13745758099409397,\n",
       "  'r2': -0.8376360783768184,\n",
       "  'pearson': 0.6284682850056454,\n",
       "  'spearman': 0.6745053740347308,\n",
       "  'trained_on': 'mcscript',\n",
       "  'evaluated_on': 'quoref'},\n",
       " {'mse': 0.06555235150226538,\n",
       "  'r2': 0.5098880223527285,\n",
       "  'pearson': 0.7683281241505397,\n",
       "  'spearman': 0.7465466422256284,\n",
       "  'trained_on': 'mcscript',\n",
       "  'evaluated_on': 'socialiqa'},\n",
       " {'mse': 0.08117776203764424,\n",
       "  'r2': 0.4216986075627275,\n",
       "  'pearson': 0.6934864387033185,\n",
       "  'spearman': 0.6836717627069856,\n",
       "  'trained_on': 'narrativeqa',\n",
       "  'evaluated_on': 'all'},\n",
       " {'mse': 0.07780764546112222,\n",
       "  'r2': 0.4821140515834641,\n",
       "  'pearson': 0.8412366472148001,\n",
       "  'spearman': 0.8010282394755478,\n",
       "  'trained_on': 'narrativeqa',\n",
       "  'evaluated_on': 'cosmosqa'},\n",
       " {'mse': 0.17183774432955215,\n",
       "  'r2': -0.27758309911095136,\n",
       "  'pearson': 0.5134602056878195,\n",
       "  'spearman': 0.6067635983721704,\n",
       "  'trained_on': 'narrativeqa',\n",
       "  'evaluated_on': 'drop'},\n",
       " {'mse': 0.0942059863328115,\n",
       "  'r2': 0.3564281388093157,\n",
       "  'pearson': 0.5977090406248586,\n",
       "  'spearman': 0.573554075710439,\n",
       "  'trained_on': 'narrativeqa',\n",
       "  'evaluated_on': 'mcscript'},\n",
       " {'mse': 0.07483851068808157,\n",
       "  'r2': 0.5006682721204652,\n",
       "  'pearson': 0.7145040920960768,\n",
       "  'spearman': 0.7234410082920902,\n",
       "  'trained_on': 'narrativeqa',\n",
       "  'evaluated_on': 'narrativeqa'},\n",
       " {'mse': 0.09361437210859908,\n",
       "  'r2': -0.2515071660452519,\n",
       "  'pearson': 0.6695227351749401,\n",
       "  'spearman': 0.721680385168802,\n",
       "  'trained_on': 'narrativeqa',\n",
       "  'evaluated_on': 'quoref'},\n",
       " {'mse': 0.06360638329180172,\n",
       "  'r2': 0.5244373452406512,\n",
       "  'pearson': 0.7751206510817122,\n",
       "  'spearman': 0.7527647007302144,\n",
       "  'trained_on': 'narrativeqa',\n",
       "  'evaluated_on': 'socialiqa'},\n",
       " {'mse': 0.09130092459225073,\n",
       "  'r2': 0.3495823179009969,\n",
       "  'pearson': 0.6309672258370465,\n",
       "  'spearman': 0.6134611493205945,\n",
       "  'trained_on': 'quoref',\n",
       "  'evaluated_on': 'all'},\n",
       " {'mse': 0.0636192433583799,\n",
       "  'r2': 0.5765517387277872,\n",
       "  'pearson': 0.770675658300283,\n",
       "  'spearman': 0.7245311859413517,\n",
       "  'trained_on': 'quoref',\n",
       "  'evaluated_on': 'cosmosqa'},\n",
       " {'mse': 0.07678265934612932,\n",
       "  'r2': 0.4291345695432406,\n",
       "  'pearson': 0.6568048507960779,\n",
       "  'spearman': 0.6427904617880517,\n",
       "  'trained_on': 'quoref',\n",
       "  'evaluated_on': 'drop'},\n",
       " {'mse': 0.13695042528638118,\n",
       "  'r2': 0.06441784091045344,\n",
       "  'pearson': 0.4958313604972671,\n",
       "  'spearman': 0.4828789915576668,\n",
       "  'trained_on': 'quoref',\n",
       "  'evaluated_on': 'mcscript'},\n",
       " {'mse': 0.10654628879731895,\n",
       "  'r2': 0.2891100852332984,\n",
       "  'pearson': 0.6360646762931376,\n",
       "  'spearman': 0.6519905128324596,\n",
       "  'trained_on': 'quoref',\n",
       "  'evaluated_on': 'narrativeqa'},\n",
       " {'mse': 0.025140104990259253,\n",
       "  'r2': 0.6639082136454412,\n",
       "  'pearson': 0.8150358509063312,\n",
       "  'spearman': 0.8207272224523684,\n",
       "  'trained_on': 'quoref',\n",
       "  'evaluated_on': 'quoref'},\n",
       " {'mse': 0.07641458844015865,\n",
       "  'r2': 0.42867488040891677,\n",
       "  'pearson': 0.6803716538916693,\n",
       "  'spearman': 0.6549260756821321,\n",
       "  'trained_on': 'quoref',\n",
       "  'evaluated_on': 'socialiqa'},\n",
       " {'mse': 0.08061578871652675,\n",
       "  'r2': 0.4257020433061782,\n",
       "  'pearson': 0.6578696873170918,\n",
       "  'spearman': 0.6590751479594589,\n",
       "  'trained_on': 'socialiqa',\n",
       "  'evaluated_on': 'all'},\n",
       " {'mse': 0.04323700261180769,\n",
       "  'r2': 0.7122154773917084,\n",
       "  'pearson': 0.8563153809046965,\n",
       "  'spearman': 0.817232935463252,\n",
       "  'trained_on': 'socialiqa',\n",
       "  'evaluated_on': 'cosmosqa'},\n",
       " {'mse': 0.1815694060057716,\n",
       "  'r2': -0.3499362746738217,\n",
       "  'pearson': 0.372497998790522,\n",
       "  'spearman': 0.5063177430603304,\n",
       "  'trained_on': 'socialiqa',\n",
       "  'evaluated_on': 'drop'},\n",
       " {'mse': 0.10786494047191268,\n",
       "  'r2': 0.263116462137683,\n",
       "  'pearson': 0.5820179899357067,\n",
       "  'spearman': 0.5518437553395636,\n",
       "  'trained_on': 'socialiqa',\n",
       "  'evaluated_on': 'mcscript'},\n",
       " {'mse': 0.09058186797665777,\n",
       "  'r2': 0.39562666018494674,\n",
       "  'pearson': 0.6775490240730504,\n",
       "  'spearman': 0.6878421597207655,\n",
       "  'trained_on': 'socialiqa',\n",
       "  'evaluated_on': 'narrativeqa'},\n",
       " {'mse': 0.11440994908081745,\n",
       "  'r2': -0.5295180421165584,\n",
       "  'pearson': 0.4122959571355141,\n",
       "  'spearman': 0.5029980685040566,\n",
       "  'trained_on': 'socialiqa',\n",
       "  'evaluated_on': 'quoref'},\n",
       " {'mse': 0.049733328449925306,\n",
       "  'r2': 0.6281613183513051,\n",
       "  'pearson': 0.7930207456306113,\n",
       "  'spearman': 0.7661838194426461,\n",
       "  'trained_on': 'socialiqa',\n",
       "  'evaluated_on': 'socialiqa'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e41cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = train_df.dataset.unique()\n",
    "DATASET_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_pipeline = Pipeline(LinearRegression, {}, \"drop\", features, target, seed=1295532)\n",
    "lr_pipeline.load_data(train_df)\n",
    "lr_pipeline.split(holdout_fraction=0.2)\n",
    "lr_pipeline.preprocess(with_std=True)\n",
    "lr_pipeline.fit()\n",
    "lr_pipeline.evaluate()\n",
    "pd.DataFrame(lr_pipeline.evaluate_multiple({k: dev_df[dev_df.dataset == k] for k in DATASET_NAMES}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce3cf17",
   "metadata": {},
   "source": [
    "###  Average Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af31316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgBaseline:\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.mean(X, axis=1)\n",
    "        assert len(y_pred) == X.shape[0]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd86965",
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgBaseline().predict(dev_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6252217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import scipy.stats as st\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "\n",
    "EXPERIMENT_DIR = \"../outputs/experiment\"\n",
    "print(\"Persisting experiment results at\", EXPERIMENT_DIR)\n",
    "os.makedirs(EXPERIMENT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93809dc9",
   "metadata": {},
   "source": [
    "### Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.dataset == \"drop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"drop\"\n",
    "dataset = TRAIN_DATASETS[dataset_name]\n",
    "\n",
    "# preprocess dataset\n",
    "data = train_test_split(dataset[features], dataset[target], preprocessor, test_fraction=0.2, seed=1295532)\n",
    "\n",
    "# fit model\n",
    "model, model_results = fit_model(AvgBaseline, data)\n",
    "\n",
    "# evaluate in validation\n",
    "model_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbf7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset in TRAIN_DATASETS.items():\n",
    "    # Dataset preprocessor\n",
    "    preprocessor = scaler()\n",
    "    \n",
    "    data = train_test_split(dataset[features], dataset[target], preprocessor, test_fraction=0.2, seed=SEED)\n",
    "    model, model_results = fit_model(AvgBaseline, data, features=features[1:])\n",
    "    model_results[\"dataset\"] = dataset_name\n",
    "    model_results[\"seed\"] = SEED\n",
    "    model_results[\"test_fraction\"] = 0.2\n",
    "    model_results[\"preprocessing\"] = \"StandardScaler\"\n",
    "    results.append(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14a886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model_class, data: tuple, **kwargs) -> tuple:\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "    \n",
    "    # Estimator\n",
    "    model = model_class(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate in holdout set\n",
    "    scores = model.predict(X_test)\n",
    "    results = {\n",
    "        \"mse\": metrics.mean_squared_error(y_true=y_test, y_pred=scores),\n",
    "        \"r2\": metrics.r2_score(y_true=y_test, y_pred=scores),\n",
    "        \"pearson\": st.pearsonr(scores, y_test)[0],\n",
    "        \"spearman\": st.spearmanr(scores, y_test)[0],\n",
    "    }\n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "train_datasets = TRAIN_DATASETS\n",
    "preproc_fn = scaler\n",
    "test_fraction = 0.2\n",
    "seed = seed\n",
    "\n",
    "model_selection_results = []\n",
    "for dataset_name, dataset in train_datasets.items():\n",
    "    # Dataset preprocessor\n",
    "    preprocessor = preproc_fn()\n",
    "    \n",
    "    data = train_test_split(dataset[features], dataset[target], preprocessor, test_fraction=test_fraction, seed=SEED)\n",
    "    model, model_results = fit_model(LinearRegression, data)\n",
    "    model_results[\"dataset\"] = dataset_name\n",
    "    model_results[\"seed\"] = SEED\n",
    "    model_results[\"test_fraction\"] = test_fraction\n",
    "    model_results[\"preprocessing\"] = \"StandardScaler\"\n",
    "    results.append(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee3fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66742617",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_model(data, estimator, dataset=None):\n",
    "\n",
    "    if dataset is not None:\n",
    "        data = data[data[\"dataset\"] == dataset]\n",
    "    \n",
    "    print(\"Considering dataset with\", len(data), \"examples, spanning datasets:\", data.dataset.unique())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.20, random_state=78452, stratify=data[target])\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    \n",
    "    # Preprocessing data (since LR may be sensitive to it)\n",
    "    X_train_prec, scalers = preprocess(X_train)\n",
    "    X_test_prec, _ = preprocess(X_test, scalers=scalers)\n",
    "\n",
    "    # Create estimator\n",
    "    clf = estimator()\n",
    "    clf.fit(X_train_prec, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    scores = clf.predict(X_test_prec)\n",
    "    results = {\n",
    "        \"mse\": metrics.mean_squared_error(y_test, scores),\n",
    "        \"r2\": metrics.r2_score(y_test, scores),\n",
    "        \"pearson\": pearsonr(scores, y_test)[0],\n",
    "        \"spearman\": spearmanr(scores, y_test),\n",
    "    }\n",
    "    return clf, scalers, results\n",
    "\n",
    "\n",
    "def eval_datasets(model, eval_datasets: dict, scalers: dict):\n",
    "    eval_results = {}\n",
    "    eval_scores = {}\n",
    "    for dataset_name, dataset in eval_datasets.items():\n",
    "        X, y = dataset[features], dataset[target]\n",
    "\n",
    "        X_prec, _ = preprocess(X.copy(), scalers=scalers)\n",
    "\n",
    "        scores = model.predict(X_prec)\n",
    "        eval_results[dataset_name] = {\n",
    "            \"mse\": metrics.mean_squared_error(y, scores),\n",
    "            \"r2\": metrics.r2_score(y, scores),\n",
    "            \"pearson\": pearsonr(scores, y)[0],\n",
    "            \"spearman\": spearmanr(scores, y)[0],\n",
    "        }\n",
    "        eval_scores[dataset_name] = scores\n",
    "        \n",
    "    return eval_results, eval_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sanity check\n",
    "lr, lr_scalers, valid_results = fit_model(train_df, LinearRegression, dataset=\"narrativeqa\")\n",
    "valid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b913952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "    \n",
    "    \n",
    "model = AvgBaseline(features[1:])\n",
    "print(\"Using metrics:\", model._features)\n",
    "avg_metrics = model.predict(dev_df)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print(\"MSE:\", mean_squared_error(y_pred=avg_metrics, y_true=dev_df[target]))\n",
    "print(\"MAE:\", mean_absolute_error(y_pred=avg_metrics, y_true=dev_df[target]))\n",
    "print(\"Pearson:\", pearsonr(avg_metrics, dev_df[target])[0])\n",
    "print(\"Spearman:\", spearmanr(avg_metrics, dev_df[target])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320fef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print(\"MSE:\", mean_squared_error(y_pred=avg_metrics, y_true=dev_df[target]))\n",
    "print(\"MAE:\", mean_absolute_error(y_pred=avg_metrics, y_true=dev_df[target]))\n",
    "print(\"Pearson:\", pearsonr(avg_metrics, dev_df[target])[0])\n",
    "print(\"Spearman:\", spearmanr(avg_metrics, dev_df[target])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23f3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Unique datasets\n",
    "unique_datasets = list(train_df.dataset.unique())\n",
    "\n",
    "# Evaluation datasets\n",
    "# includes all_datasets (macro eval), as well as individual datasets\n",
    "dev_orig_datasets = {None: dev_df}\n",
    "dev_orig_datasets.update({dataset: dev_df[dev_df.dataset == dataset] for dataset in unique_datasets})\n",
    "\n",
    "models = {}\n",
    "results_by_dataset = {}\n",
    "for dataset_name in dev_orig_datasets.keys():\n",
    "    print(\"Fitting model using\", \"all\" if dataset_name is None else dataset_name, \"datasets\")\n",
    "    model, model_scalers, valid_results = fit_model(train_df, LinearRegression, dataset=dataset_name)\n",
    "    \n",
    "    models[dataset_name] = model\n",
    "    results, scores = eval_datasets(model, dev_orig_datasets, model_scalers)\n",
    "    \n",
    "    results_by_dataset[dataset_name] = results\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def parse_table_results(results_by_dataset, filename, output_dir=METRICS_DIR):\n",
    "    table_results = defaultdict(list)\n",
    "\n",
    "    for train_dataset, test_values in results_by_dataset.items():\n",
    "\n",
    "        for test_dataset, test_results in test_values.items():\n",
    "            table_results[\"train_dataset\"].append(\"all_datasets\" if train_dataset is None else train_dataset)\n",
    "            table_results[\"eval_dataset\"].append(\"all_datasets\" if test_dataset is None else test_dataset)\n",
    "\n",
    "            for metric, metric_value in test_results.items():\n",
    "                table_results[metric].append(metric_value)\n",
    "            \n",
    "    table_results = pd.DataFrame(table_results)\n",
    "    table_results.to_csv((f\"{output_dir}/{filename}.csv\"))\n",
    "    return table_results\n",
    "\n",
    "\n",
    "table_results = parse_table_results(results_by_dataset, output_dir=METRICS_DIR, filename=\"dev_lr_correlations\")\n",
    "table_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc60548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_model_coeffs(models, train_dataset): \n",
    "    clf = models[train_dataset]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(f\"Feature importance for train dataset: {train_dataset if train_dataset is not None else 'all_datasets'}\")\n",
    "    sns.barplot(y=features, x=clf.coef_, orient=\"h\")\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "for train_dataset in models.keys():\n",
    "    plot_model_coeffs(models, train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c1de8",
   "metadata": {},
   "source": [
    "### Leave-one-out (LOO) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Unique datasets\n",
    "_unique_datasets = list(train_df.dataset.unique())\n",
    "\n",
    "# Evaluation datasets\n",
    "# includes all_datasets (macro eval), as well as individual datasets\n",
    "_dev_orig_datasets = {None: dev_df}\n",
    "_dev_orig_datasets.update({dataset: dev_df[dev_df.dataset == dataset] for dataset in unique_datasets})\n",
    "\n",
    "_models = {}\n",
    "_results_by_dataset = {}\n",
    "for _dataset_name in _dev_orig_datasets.keys():\n",
    "    if _dataset_name is None: continue\n",
    "    \n",
    "    # Compute other dataset names except `_dataset_name`\n",
    "    _remaining_datasets = [k for k in _dev_orig_datasets.keys() if k != _dataset_name]\n",
    "    \n",
    "    # Select subset of trainin data that does not include `_dataset_name`\n",
    "    _train_remain_df = train_df[train_df.dataset.isin(_remaining_datasets)]\n",
    "    _train_remain_name = f\"all_except_{_dataset_name}\"\n",
    "    \n",
    "    print(\"Fitting model on\", _train_remain_name, f\"with {len(_train_remain_df)} examples (instead of {len(train_df)})\")\n",
    "\n",
    "    _model, _model_scalers, _valid_results = fit_model(_train_remain_df, LinearRegression)\n",
    "    _models[_train_remain_name] = _model\n",
    "    _results, _scores = eval_datasets(_model, _dev_orig_datasets, _model_scalers)\n",
    "    _results_by_dataset[_train_remain_name] = _results\n",
    "    \n",
    "\n",
    "parse_table_results(_results_by_dataset, \"dev_lr_loo_correlations\", METRICS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2465678",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for train_dataset in _models.keys():\n",
    "    plot_model_coeffs(_models, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aeb56b",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "We've seen there is a large correlation between the different metrics.. In particular, it might explain the coefficients, we see in the image above. In the presence of redundancy, the model may be [non-identifiable](https://en.wikipedia.org/wiki/Identifiability), i.e., have two or more parameterizations that are observationally equivalent.\n",
    "\n",
    "In this section of the notebook, we are interested in knowing whether there will be a set of orthogonal components that fully explain the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_datasets_with_pca(model, eval_datasets: dict, pca: dict):\n",
    "    eval_results = {}\n",
    "    eval_scores = {}\n",
    "    for dataset_name, dataset in eval_datasets.items():\n",
    "        X, y = dataset[features], dataset[target]\n",
    "\n",
    "        X_prec = pca.transform(X.copy())\n",
    "\n",
    "        scores = model.predict(X_prec)\n",
    "        eval_results[dataset_name] = {\n",
    "            \"mse\": metrics.mean_squared_error(y, scores),\n",
    "            \"r2\": metrics.r2_score(y, scores),\n",
    "            \"pearson\": pearsonr(scores, y)[0],\n",
    "            \"spearman\": spearmanr(scores, y)[0],\n",
    "        }\n",
    "        eval_scores[dataset_name] = scores\n",
    "    return eval_results, eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca5e900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = train_df.copy()\n",
    "print(\"Considering dataset with\", len(data), \"examples, spanning datasets:\", data.dataset.unique())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[features], data[target], test_size=0.20, random_state=78452, stratify=data[target])\n",
    "print(X_train.shape, X_test.shape)\n",
    "    \n",
    "# Preprocessing data (since LR may be sensitive to it)\n",
    "# X_train_prec, scalers = preprocess(X_train)\n",
    "# X_test_prec, _ = preprocess(X_test, scalers=scalers)\n",
    "\n",
    "\n",
    "# Iterate over several components\n",
    "eval_results = defaultdict(list)\n",
    "_pca_models = {}\n",
    "_pca = {}\n",
    "for n in range(2, 20):\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    print(\"Fitting PCA w/ n_components =\", n)\n",
    "    for seed in (123124, 1295532, 875843):\n",
    "        # Create estimator\n",
    "        pca = PCA(n_components=n, random_state=seed)\n",
    "        X_train_transf = pca.fit_transform(X_train.copy())\n",
    "        X_test_transf = pca.transform(X_test.copy())\n",
    "        # print(X_train_transf.shape, X_test_transf.shape)\n",
    "\n",
    "        # Fit LR on top of new representation\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train_transf, y_train)\n",
    "\n",
    "        scores = lr.predict(X_test_transf)\n",
    "        eval_results[\"n\"].append(n)\n",
    "        eval_results[\"seed\"].append(n)\n",
    "        eval_results[\"mse\"].append(metrics.mean_squared_error(y_test, scores))\n",
    "        eval_results[\"r2\"].append(metrics.r2_score(y_test, scores))\n",
    "        eval_results[\"pearson\"].append(pearsonr(scores, y_test)[0])\n",
    "        eval_results[\"spearman\"].append(spearmanr(scores, y_test)[0])\n",
    "        \n",
    "        _pca[(n, seed)] = pca\n",
    "        _pca_models[(n, seed)] = lr\n",
    "        \n",
    "eval_results = pd.DataFrame(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67dec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=eval_results, x=\"n\", y=\"mse\")\n",
    "plt.xlabel(\"N components (PCA)\")\n",
    "plt.title(\"MSE of fit in function of number of PCA components on dev set\")\n",
    "plt.xlim(0, 18)\n",
    "plt.ylim(0.08, 0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07374176",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components, seed = 10, 1295532\n",
    "model = _pca_models[(n_components, seed)]\n",
    "\n",
    "# Unique datasets\n",
    "unique_datasets = list(train_df.dataset.unique())\n",
    "\n",
    "# Evaluation datasets\n",
    "# includes all_datasets (macro eval), as well as individual datasets\n",
    "dev_orig_datasets = {None: dev_df}\n",
    "dev_orig_datasets.update({dataset: dev_df[dev_df.dataset == dataset] for dataset in unique_datasets})\n",
    "\n",
    "results_by_dataset = {}\n",
    "for dataset_name in dev_orig_datasets.keys():\n",
    "    print(\"Fitting model using\", \"all\" if dataset_name is None else dataset_name, \"datasets\")\n",
    "    results, scores = eval_datasets_with_pca(\n",
    "        model=model,\n",
    "        pca=_pca[(n_components, seed)], \n",
    "        eval_datasets=dev_orig_datasets,\n",
    "    )\n",
    "    \n",
    "    results_by_dataset[dataset_name] = results \n",
    "\n",
    "parse_table_results(results_by_dataset, f\"dev_pca_{n_components}+lr_correlations\", METRICS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94488c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce dimension to 2 with PCA\n",
    "pca = {i: lambda: make_pipeline(StandardScaler(), PCA(n_components=i, random_state=SEED)) for i in range(2, 15)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
