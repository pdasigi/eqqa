{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c200437",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Train and evaluate the baselines. Baselines are: \n",
    "\n",
    "- Linear Regression\n",
    "- L1 regression (Lasso Regression)\n",
    "- Decision Tree \n",
    "- Random Forests \n",
    "\n",
    "For every baseline above (except L1), we'll also consider a PCA version where we first reduce the dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf925ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read dataset with 31069 examples from ../outputs/metrics/proxy_metrics_20220426/train_all_datasets_metrics.csv.gz\n",
      "Read (holdout) dataset with 4009 examples from ../outputs/metrics/proxy_metrics_20220426/dev_all_datasets_metrics.csv.gz\n",
      "Features:\t ['exact_match', 'meteor', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'bleurt', 'bert-score', 'bleu1', 'bleu2', 'bleu3', 'bleu4', 'bleu-precision0', 'bleu-precision1', 'bleu-precision2', 'bleu-precision3', 'precision', 'recall', 'f1_score', 'csi', 'num_edits', 'edit_score'] \n",
      "\n",
      "Target:\t human_correctness\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "METRICS_DIR = \"../outputs/metrics/proxy_metrics_20220426\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# DATA LOADING\n",
    "# ---------------------------------------------------------------\n",
    "TRAIN_FILEPATH = f\"{METRICS_DIR}/train_all_datasets_metrics.csv.gz\"\n",
    "DEV_FILEPATH = f\"{METRICS_DIR}/dev_all_datasets_metrics.csv.gz\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILEPATH, index_col=0)\n",
    "print(\"Read dataset with\", len(train_df), \"examples from\", TRAIN_FILEPATH)\n",
    "\n",
    "dev_df = pd.read_csv(DEV_FILEPATH, index_col=0)\n",
    "print(\"Read (holdout) dataset with\", len(dev_df), \"examples from\", DEV_FILEPATH)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# FEATURES and TARGET \n",
    "# ---------------------------------------------------------------\n",
    "METRIC_COLS = train_df.select_dtypes(\"number\").columns\n",
    "features = list(METRIC_COLS[2:])\n",
    "\n",
    "# Target column will be the normalized human correctness\n",
    "target = METRIC_COLS[1]\n",
    "print(\"Features:\\t\", features, \"\\n\\nTarget:\\t\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb33bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# - [] Fit linear regression\n",
    "# - [] Fit L1 regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21c214",
   "metadata": {},
   "source": [
    "TODO \n",
    "- [ ] Compute Baselines (Avg metric)\n",
    "- [ ] Compute Linear Regression \n",
    "- [ ] L1 regression\n",
    "- [ ] Decision Tree\n",
    "- [ ] Random Forest\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5114415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ce3cf17",
   "metadata": {},
   "source": [
    "###  Average Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af31316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgBaseline:\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.mean(X, axis=1)\n",
    "        assert len(y_pred) == X.shape[0]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd86965",
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgBaseline().predict(dev_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6252217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import scipy.stats as st\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "\n",
    "EXPERIMENT_DIR = \"../outputs/experiment\"\n",
    "print(\"Persisting experiment results at\", EXPERIMENT_DIR)\n",
    "os.makedirs(EXPERIMENT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93809dc9",
   "metadata": {},
   "source": [
    "### Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f670c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, model_class, model_hparams, dataset, features, target, seed=81263):\n",
    "        self.model_class = model_class\n",
    "        self.model_hparams = model_hparams\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.seed = seed\n",
    "        \n",
    "    def load_data(self, data):\n",
    "        \"\"\"\"\"\"\n",
    "        data = data[data[\"dataset\"] == self.dataset].copy()\n",
    "        self.X_train = data[self.features]\n",
    "        self.y_train = data[self.target]\n",
    "        \n",
    "    def split(self, holdout_fraction=0.2):\n",
    "        \"\"\"\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X_train, self.y_train,\n",
    "            test_size=holdout_fraction, \n",
    "            random_state=self.seed, \n",
    "            stratify=self.y_train,\n",
    "        )\n",
    "        \n",
    "        self.X_train, self.X_test = X_train, X_test\n",
    "        self.y_train, self.y_test = y_train, y_test        \n",
    "        \n",
    "    def preprocess(self, with_std=True, with_pca=False, **kwargs):\n",
    "        \"\"\"\"\"\"\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.decomposition import PCA\n",
    "        \n",
    "        operations = []\n",
    "        \n",
    "        if with_std:\n",
    "            operations.append(StandardScaler())\n",
    "        if with_pca:\n",
    "            operations.append(PCA(random_state=self.seed, **kwargs))\n",
    "        \n",
    "        self.preproc_fn = make_pipeline(*operations)\n",
    "        self.preproc_fn.fit(self.X_train)\n",
    "        self.X_train = self.preproc_fn.transform(self.X_train)\n",
    "        self.X_test = self.preproc_fn.transform(self.X_test)\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\"\"\"\n",
    "        self.model = self.model_class(**self.model_hparams)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def evaluate(self, eval_dataset=None):\n",
    "        \"\"\"\"\"\"\n",
    "        import sklearn.metrics as m\n",
    "        import scipy.stats as st\n",
    "        \n",
    "        if eval_dataset is None:\n",
    "            print(\"Evaluating holdout dev set.\")\n",
    "            X_test, y_test = self.X_test, self.y_test\n",
    "        else:\n",
    "            X_test = eval_dataset[self.features]\n",
    "            y_test = eval_dataset[self.target]    \n",
    "            X_test = self.preproc_fn.transform(X_test)\n",
    "        \n",
    "        # Evaluation\n",
    "        scores = self.model.predict(self.X_test)\n",
    "        \n",
    "        return {\n",
    "            \"trained_on\": self.dataset,\n",
    "            \"mse\": m.mean_squared_error(y_pred=scores, y_true=y_test),\n",
    "            \"r2\": m.r2_score(y_pred=scores, y_true=y_test),\n",
    "            \"pearson\": st.pearsonr(scores, y_test)[0],\n",
    "            \"spearman\": st.spearmanr(scores, y_test)[0],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b02c793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating holdout dev set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trained_on': 'drop',\n",
       " 'mse': 0.07268463638902901,\n",
       " 'r2': 0.4327169015577803,\n",
       " 'pearson': 0.6643839861546849,\n",
       " 'spearman': 0.6749595512125868}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_pipeline = Pipeline(LinearRegression, {}, \"drop\", features, target)\n",
    "lr_pipeline.load_data(train_df)\n",
    "lr_pipeline.split(holdout_fraction=0.2)\n",
    "lr_pipeline.preprocess(with_std=True)\n",
    "lr_pipeline.fit()\n",
    "lr_pipeline.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d6de8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>reference</th>\n",
       "      <th>candidate</th>\n",
       "      <th>reference_tokens</th>\n",
       "      <th>candidate_tokens</th>\n",
       "      <th>human_correctness_original</th>\n",
       "      <th>human_correctness</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>meteor</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>...</th>\n",
       "      <th>bleu-precision2</th>\n",
       "      <th>bleu-precision3</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>csi</th>\n",
       "      <th>num_edits</th>\n",
       "      <th>edit_score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5033</th>\n",
       "      <td>006b6b4714b9e67fd29410d9bae9cdad</td>\n",
       "      <td>diabetes, hypertension</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>['diabetes', ',', 'hypertension']</td>\n",
       "      <td>['hypertension']</td>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>drop</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5034</th>\n",
       "      <td>009943eb797324320731ac41dcb30e4b</td>\n",
       "      <td>under the age of 18, 25 to 44, 45 to 64</td>\n",
       "      <td>25 to 44</td>\n",
       "      <td>['under', 'the', 'age', 'of', '18', ',', '25',...</td>\n",
       "      <td>['25', 'to', '44']</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>9</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>drop</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5035</th>\n",
       "      <td>011a8c8c2a833ca11db9246f11403a2a</td>\n",
       "      <td>6-yard touchdown pass</td>\n",
       "      <td>drew brees 6 - yard touchdown pass</td>\n",
       "      <td>['6', '-', 'yard', 'touchdown', 'pass']</td>\n",
       "      <td>['drew', 'brees', '6', '-', 'yard', 'touchdown...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551471</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>5</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>drop</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>0165c8d4a359cbbeedf4ce61ed6f1760</td>\n",
       "      <td>60 and 69, 70 to 79, 80 to 89, 90 to 99</td>\n",
       "      <td>Cunter</td>\n",
       "      <td>['60', 'and', '69', ',', '70', 'to', '79', ','...</td>\n",
       "      <td>['Cunter']</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>drop</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037</th>\n",
       "      <td>01b38514f642d6477fec2b1d45f4bf01</td>\n",
       "      <td>born outside the EU</td>\n",
       "      <td>outside the eu</td>\n",
       "      <td>['born', 'outside', 'the', 'EU']</td>\n",
       "      <td>['outside', 'the', 'eu']</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.754986</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>drop</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>fefc92db527b53e1a6a5b762a5deb6ae</td>\n",
       "      <td>a pump boat in Mususiasi</td>\n",
       "      <td>Semporna</td>\n",
       "      <td>['a', 'pump', 'boat', 'in', 'Mususiasi']</td>\n",
       "      <td>['Semporna']</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>drop</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716</th>\n",
       "      <td>ff52abd7769592ff86e4540fcf2d4b7f</td>\n",
       "      <td>Romania</td>\n",
       "      <td>treaty of bucharest</td>\n",
       "      <td>['Romania']</td>\n",
       "      <td>['treaty', 'of', 'bucharest']</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>drop</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5717</th>\n",
       "      <td>ff5363a23aeb6ff0cb8a920d12047185</td>\n",
       "      <td>the Algerian War of Independence</td>\n",
       "      <td>six - day war by israel ( 1967 ) , in nigeria ...</td>\n",
       "      <td>['the', 'Algerian', 'War', 'of', 'Independence']</td>\n",
       "      <td>['six', '-', 'day', 'war', 'by', 'israel', '('...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081301</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>76</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>drop</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5718</th>\n",
       "      <td>ff6a2ddb643665bbd25097c4e58246f5</td>\n",
       "      <td>Lance Berkman, Brad Ausmus, Chris Burke</td>\n",
       "      <td>Albert Pujols</td>\n",
       "      <td>['Lance', 'Berkman', ',', 'Brad', 'Ausmus', ',...</td>\n",
       "      <td>['Albert', 'Pujols']</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>drop</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5719</th>\n",
       "      <td>ffad823ee381d6722ceeaa9ca9cc49df</td>\n",
       "      <td>Pittsburgh Steelers, New England Patriots</td>\n",
       "      <td>chicago bears</td>\n",
       "      <td>['Pittsburgh', 'Steelers', ',', 'New', 'Englan...</td>\n",
       "      <td>['chicago', 'bears']</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>drop</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>687 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            example_id  \\\n",
       "5033  006b6b4714b9e67fd29410d9bae9cdad   \n",
       "5034  009943eb797324320731ac41dcb30e4b   \n",
       "5035  011a8c8c2a833ca11db9246f11403a2a   \n",
       "5036  0165c8d4a359cbbeedf4ce61ed6f1760   \n",
       "5037  01b38514f642d6477fec2b1d45f4bf01   \n",
       "...                                ...   \n",
       "5715  fefc92db527b53e1a6a5b762a5deb6ae   \n",
       "5716  ff52abd7769592ff86e4540fcf2d4b7f   \n",
       "5717  ff5363a23aeb6ff0cb8a920d12047185   \n",
       "5718  ff6a2ddb643665bbd25097c4e58246f5   \n",
       "5719  ffad823ee381d6722ceeaa9ca9cc49df   \n",
       "\n",
       "                                      reference  \\\n",
       "5033                     diabetes, hypertension   \n",
       "5034    under the age of 18, 25 to 44, 45 to 64   \n",
       "5035                      6-yard touchdown pass   \n",
       "5036    60 and 69, 70 to 79, 80 to 89, 90 to 99   \n",
       "5037                        born outside the EU   \n",
       "...                                         ...   \n",
       "5715                   a pump boat in Mususiasi   \n",
       "5716                                    Romania   \n",
       "5717           the Algerian War of Independence   \n",
       "5718    Lance Berkman, Brad Ausmus, Chris Burke   \n",
       "5719  Pittsburgh Steelers, New England Patriots   \n",
       "\n",
       "                                              candidate  \\\n",
       "5033                                       hypertension   \n",
       "5034                                           25 to 44   \n",
       "5035                 drew brees 6 - yard touchdown pass   \n",
       "5036                                             Cunter   \n",
       "5037                                     outside the eu   \n",
       "...                                                 ...   \n",
       "5715                                           Semporna   \n",
       "5716                                treaty of bucharest   \n",
       "5717  six - day war by israel ( 1967 ) , in nigeria ...   \n",
       "5718                                      Albert Pujols   \n",
       "5719                                      chicago bears   \n",
       "\n",
       "                                       reference_tokens  \\\n",
       "5033                  ['diabetes', ',', 'hypertension']   \n",
       "5034  ['under', 'the', 'age', 'of', '18', ',', '25',...   \n",
       "5035            ['6', '-', 'yard', 'touchdown', 'pass']   \n",
       "5036  ['60', 'and', '69', ',', '70', 'to', '79', ','...   \n",
       "5037                   ['born', 'outside', 'the', 'EU']   \n",
       "...                                                 ...   \n",
       "5715           ['a', 'pump', 'boat', 'in', 'Mususiasi']   \n",
       "5716                                        ['Romania']   \n",
       "5717   ['the', 'Algerian', 'War', 'of', 'Independence']   \n",
       "5718  ['Lance', 'Berkman', ',', 'Brad', 'Ausmus', ',...   \n",
       "5719  ['Pittsburgh', 'Steelers', ',', 'New', 'Englan...   \n",
       "\n",
       "                                       candidate_tokens  \\\n",
       "5033                                   ['hypertension']   \n",
       "5034                                 ['25', 'to', '44']   \n",
       "5035  ['drew', 'brees', '6', '-', 'yard', 'touchdown...   \n",
       "5036                                         ['Cunter']   \n",
       "5037                           ['outside', 'the', 'eu']   \n",
       "...                                                 ...   \n",
       "5715                                       ['Semporna']   \n",
       "5716                      ['treaty', 'of', 'bucharest']   \n",
       "5717  ['six', '-', 'day', 'war', 'by', 'israel', '('...   \n",
       "5718                               ['Albert', 'Pujols']   \n",
       "5719                               ['chicago', 'bears']   \n",
       "\n",
       "      human_correctness_original  human_correctness  exact_match    meteor  \\\n",
       "5033                           3               0.50          0.0  0.178571   \n",
       "5034                           2               0.25          0.0  0.125000   \n",
       "5035                           5               1.00          0.0  0.551471   \n",
       "5036                           1               0.00          0.0  0.000000   \n",
       "5037                           5               1.00          0.0  0.754986   \n",
       "...                          ...                ...          ...       ...   \n",
       "5715                           1               0.00          0.0  0.000000   \n",
       "5716                           1               0.00          0.0  0.000000   \n",
       "5717                           1               0.00          0.0  0.081301   \n",
       "5718                           1               0.00          0.0  0.000000   \n",
       "5719                           1               0.00          0.0  0.000000   \n",
       "\n",
       "        rouge1  ...  bleu-precision2  bleu-precision3  precision    recall  \\\n",
       "5033  0.666667  ...              0.0              0.0   1.000000  0.333333   \n",
       "5034  0.428571  ...              1.0              0.0   1.000000  0.230769   \n",
       "5035  0.800000  ...              0.6              0.5   0.714286  1.000000   \n",
       "5036  0.000000  ...              0.0              0.0   0.000000  0.000000   \n",
       "5037  0.857143  ...              0.0              0.0   0.666667  0.500000   \n",
       "...        ...  ...              ...              ...        ...       ...   \n",
       "5715  0.000000  ...              0.0              0.0   0.000000  0.000000   \n",
       "5716  0.000000  ...              0.0              0.0   0.000000  0.000000   \n",
       "5717  0.075472  ...              0.0              0.0   0.012821  0.200000   \n",
       "5718  0.000000  ...              0.0              0.0   0.000000  0.000000   \n",
       "5719  0.000000  ...              0.0              0.0   0.000000  0.000000   \n",
       "\n",
       "      f1_score       csi  num_edits  edit_score  dataset  split  \n",
       "5033  0.500000  0.333333          1    0.500000     drop  train  \n",
       "5034  0.375000  0.230769          9    0.818182     drop  train  \n",
       "5035  0.833333  0.714286          5    1.666667     drop  train  \n",
       "5036  0.000000  0.000000         12    1.000000     drop  train  \n",
       "5037  0.571429  0.400000          1    0.250000     drop  train  \n",
       "...        ...       ...        ...         ...      ...    ...  \n",
       "5715  0.000000  0.000000          5    1.000000     drop  train  \n",
       "5716  0.000000  0.000000          3    3.000000     drop  train  \n",
       "5717  0.024096  0.012195         76   15.200000     drop  train  \n",
       "5718  0.000000  0.000000          6    1.000000     drop  train  \n",
       "5719  0.000000  0.000000          5    1.000000     drop  train  \n",
       "\n",
       "[687 rows x 31 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.dataset == \"drop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"drop\"\n",
    "dataset = TRAIN_DATASETS[dataset_name]\n",
    "\n",
    "# preprocess dataset\n",
    "data = train_test_split(dataset[features], dataset[target], preprocessor, test_fraction=0.2, seed=SEED)\n",
    "\n",
    "# fit model\n",
    "model, model_results = fit_model(AvgBaseline, data)\n",
    "\n",
    "# evaluate in validation\n",
    "model_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbf7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset in TRAIN_DATASETS.items():\n",
    "    # Dataset preprocessor\n",
    "    preprocessor = scaler()\n",
    "    \n",
    "    data = train_test_split(dataset[features], dataset[target], preprocessor, test_fraction=0.2, seed=SEED)\n",
    "    model, model_results = fit_model(AvgBaseline, data, features=features[1:])\n",
    "    model_results[\"dataset\"] = dataset_name\n",
    "    model_results[\"seed\"] = SEED\n",
    "    model_results[\"test_fraction\"] = 0.2\n",
    "    model_results[\"preprocessing\"] = \"StandardScaler\"\n",
    "    results.append(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14a886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model_class, data: tuple, **kwargs) -> tuple:\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "    \n",
    "    # Estimator\n",
    "    model = model_class(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate in holdout set\n",
    "    scores = model.predict(X_test)\n",
    "    results = {\n",
    "        \"mse\": metrics.mean_squared_error(y_true=y_test, y_pred=scores),\n",
    "        \"r2\": metrics.r2_score(y_true=y_test, y_pred=scores),\n",
    "        \"pearson\": st.pearsonr(scores, y_test)[0],\n",
    "        \"spearman\": st.spearmanr(scores, y_test)[0],\n",
    "    }\n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "train_datasets = TRAIN_DATASETS\n",
    "preproc_fn = scaler\n",
    "test_fraction = 0.2\n",
    "seed = seed\n",
    "\n",
    "model_selection_results = []\n",
    "for dataset_name, dataset in train_datasets.items():\n",
    "    # Dataset preprocessor\n",
    "    preprocessor = preproc_fn()\n",
    "    \n",
    "    data = train_test_split(dataset[features], dataset[target], preprocessor, test_fraction=test_fraction, seed=SEED)\n",
    "    model, model_results = fit_model(LinearRegression, data)\n",
    "    model_results[\"dataset\"] = dataset_name\n",
    "    model_results[\"seed\"] = SEED\n",
    "    model_results[\"test_fraction\"] = test_fraction\n",
    "    model_results[\"preprocessing\"] = \"StandardScaler\"\n",
    "    results.append(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee3fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66742617",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_model(data, estimator, dataset=None):\n",
    "\n",
    "    if dataset is not None:\n",
    "        data = data[data[\"dataset\"] == dataset]\n",
    "    \n",
    "    print(\"Considering dataset with\", len(data), \"examples, spanning datasets:\", data.dataset.unique())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.20, random_state=78452, stratify=data[target])\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    \n",
    "    # Preprocessing data (since LR may be sensitive to it)\n",
    "    X_train_prec, scalers = preprocess(X_train)\n",
    "    X_test_prec, _ = preprocess(X_test, scalers=scalers)\n",
    "\n",
    "    # Create estimator\n",
    "    clf = estimator()\n",
    "    clf.fit(X_train_prec, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    scores = clf.predict(X_test_prec)\n",
    "    results = {\n",
    "        \"mse\": metrics.mean_squared_error(y_test, scores),\n",
    "        \"r2\": metrics.r2_score(y_test, scores),\n",
    "        \"pearson\": pearsonr(scores, y_test)[0],\n",
    "        \"spearman\": spearmanr(scores, y_test),\n",
    "    }\n",
    "    return clf, scalers, results\n",
    "\n",
    "\n",
    "def eval_datasets(model, eval_datasets: dict, scalers: dict):\n",
    "    eval_results = {}\n",
    "    eval_scores = {}\n",
    "    for dataset_name, dataset in eval_datasets.items():\n",
    "        X, y = dataset[features], dataset[target]\n",
    "\n",
    "        X_prec, _ = preprocess(X.copy(), scalers=scalers)\n",
    "\n",
    "        scores = model.predict(X_prec)\n",
    "        eval_results[dataset_name] = {\n",
    "            \"mse\": metrics.mean_squared_error(y, scores),\n",
    "            \"r2\": metrics.r2_score(y, scores),\n",
    "            \"pearson\": pearsonr(scores, y)[0],\n",
    "            \"spearman\": spearmanr(scores, y)[0],\n",
    "        }\n",
    "        eval_scores[dataset_name] = scores\n",
    "        \n",
    "    return eval_results, eval_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sanity check\n",
    "lr, lr_scalers, valid_results = fit_model(train_df, LinearRegression, dataset=\"narrativeqa\")\n",
    "valid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b913952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "    \n",
    "    \n",
    "model = AvgBaseline(features[1:])\n",
    "print(\"Using metrics:\", model._features)\n",
    "avg_metrics = model.predict(dev_df)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print(\"MSE:\", mean_squared_error(y_pred=avg_metrics, y_true=dev_df[target]))\n",
    "print(\"MAE:\", mean_absolute_error(y_pred=avg_metrics, y_true=dev_df[target]))\n",
    "print(\"Pearson:\", pearsonr(avg_metrics, dev_df[target])[0])\n",
    "print(\"Spearman:\", spearmanr(avg_metrics, dev_df[target])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320fef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print(\"MSE:\", mean_squared_error(y_pred=avg_metrics, y_true=dev_df[target]))\n",
    "print(\"MAE:\", mean_absolute_error(y_pred=avg_metrics, y_true=dev_df[target]))\n",
    "print(\"Pearson:\", pearsonr(avg_metrics, dev_df[target])[0])\n",
    "print(\"Spearman:\", spearmanr(avg_metrics, dev_df[target])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23f3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Unique datasets\n",
    "unique_datasets = list(train_df.dataset.unique())\n",
    "\n",
    "# Evaluation datasets\n",
    "# includes all_datasets (macro eval), as well as individual datasets\n",
    "dev_orig_datasets = {None: dev_df}\n",
    "dev_orig_datasets.update({dataset: dev_df[dev_df.dataset == dataset] for dataset in unique_datasets})\n",
    "\n",
    "models = {}\n",
    "results_by_dataset = {}\n",
    "for dataset_name in dev_orig_datasets.keys():\n",
    "    print(\"Fitting model using\", \"all\" if dataset_name is None else dataset_name, \"datasets\")\n",
    "    model, model_scalers, valid_results = fit_model(train_df, LinearRegression, dataset=dataset_name)\n",
    "    \n",
    "    models[dataset_name] = model\n",
    "    results, scores = eval_datasets(model, dev_orig_datasets, model_scalers)\n",
    "    \n",
    "    results_by_dataset[dataset_name] = results\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def parse_table_results(results_by_dataset, filename, output_dir=METRICS_DIR):\n",
    "    table_results = defaultdict(list)\n",
    "\n",
    "    for train_dataset, test_values in results_by_dataset.items():\n",
    "\n",
    "        for test_dataset, test_results in test_values.items():\n",
    "            table_results[\"train_dataset\"].append(\"all_datasets\" if train_dataset is None else train_dataset)\n",
    "            table_results[\"eval_dataset\"].append(\"all_datasets\" if test_dataset is None else test_dataset)\n",
    "\n",
    "            for metric, metric_value in test_results.items():\n",
    "                table_results[metric].append(metric_value)\n",
    "            \n",
    "    table_results = pd.DataFrame(table_results)\n",
    "    table_results.to_csv((f\"{output_dir}/{filename}.csv\"))\n",
    "    return table_results\n",
    "\n",
    "\n",
    "table_results = parse_table_results(results_by_dataset, output_dir=METRICS_DIR, filename=\"dev_lr_correlations\")\n",
    "table_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc60548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_model_coeffs(models, train_dataset): \n",
    "    clf = models[train_dataset]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(f\"Feature importance for train dataset: {train_dataset if train_dataset is not None else 'all_datasets'}\")\n",
    "    sns.barplot(y=features, x=clf.coef_, orient=\"h\")\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "for train_dataset in models.keys():\n",
    "    plot_model_coeffs(models, train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c1de8",
   "metadata": {},
   "source": [
    "### Leave-one-out (LOO) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Unique datasets\n",
    "_unique_datasets = list(train_df.dataset.unique())\n",
    "\n",
    "# Evaluation datasets\n",
    "# includes all_datasets (macro eval), as well as individual datasets\n",
    "_dev_orig_datasets = {None: dev_df}\n",
    "_dev_orig_datasets.update({dataset: dev_df[dev_df.dataset == dataset] for dataset in unique_datasets})\n",
    "\n",
    "_models = {}\n",
    "_results_by_dataset = {}\n",
    "for _dataset_name in _dev_orig_datasets.keys():\n",
    "    if _dataset_name is None: continue\n",
    "    \n",
    "    # Compute other dataset names except `_dataset_name`\n",
    "    _remaining_datasets = [k for k in _dev_orig_datasets.keys() if k != _dataset_name]\n",
    "    \n",
    "    # Select subset of trainin data that does not include `_dataset_name`\n",
    "    _train_remain_df = train_df[train_df.dataset.isin(_remaining_datasets)]\n",
    "    _train_remain_name = f\"all_except_{_dataset_name}\"\n",
    "    \n",
    "    print(\"Fitting model on\", _train_remain_name, f\"with {len(_train_remain_df)} examples (instead of {len(train_df)})\")\n",
    "\n",
    "    _model, _model_scalers, _valid_results = fit_model(_train_remain_df, LinearRegression)\n",
    "    _models[_train_remain_name] = _model\n",
    "    _results, _scores = eval_datasets(_model, _dev_orig_datasets, _model_scalers)\n",
    "    _results_by_dataset[_train_remain_name] = _results\n",
    "    \n",
    "\n",
    "parse_table_results(_results_by_dataset, \"dev_lr_loo_correlations\", METRICS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2465678",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for train_dataset in _models.keys():\n",
    "    plot_model_coeffs(_models, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aeb56b",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "We've seen there is a large correlation between the different metrics.. In particular, it might explain the coefficients, we see in the image above. In the presence of redundancy, the model may be [non-identifiable](https://en.wikipedia.org/wiki/Identifiability), i.e., have two or more parameterizations that are observationally equivalent.\n",
    "\n",
    "In this section of the notebook, we are interested in knowing whether there will be a set of orthogonal components that fully explain the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_datasets_with_pca(model, eval_datasets: dict, pca: dict):\n",
    "    eval_results = {}\n",
    "    eval_scores = {}\n",
    "    for dataset_name, dataset in eval_datasets.items():\n",
    "        X, y = dataset[features], dataset[target]\n",
    "\n",
    "        X_prec = pca.transform(X.copy())\n",
    "\n",
    "        scores = model.predict(X_prec)\n",
    "        eval_results[dataset_name] = {\n",
    "            \"mse\": metrics.mean_squared_error(y, scores),\n",
    "            \"r2\": metrics.r2_score(y, scores),\n",
    "            \"pearson\": pearsonr(scores, y)[0],\n",
    "            \"spearman\": spearmanr(scores, y)[0],\n",
    "        }\n",
    "        eval_scores[dataset_name] = scores\n",
    "    return eval_results, eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca5e900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = train_df.copy()\n",
    "print(\"Considering dataset with\", len(data), \"examples, spanning datasets:\", data.dataset.unique())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[features], data[target], test_size=0.20, random_state=78452, stratify=data[target])\n",
    "print(X_train.shape, X_test.shape)\n",
    "    \n",
    "# Preprocessing data (since LR may be sensitive to it)\n",
    "# X_train_prec, scalers = preprocess(X_train)\n",
    "# X_test_prec, _ = preprocess(X_test, scalers=scalers)\n",
    "\n",
    "\n",
    "# Iterate over several components\n",
    "eval_results = defaultdict(list)\n",
    "_pca_models = {}\n",
    "_pca = {}\n",
    "for n in range(2, 20):\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    print(\"Fitting PCA w/ n_components =\", n)\n",
    "    for seed in (123124, 1295532, 875843):\n",
    "        # Create estimator\n",
    "        pca = PCA(n_components=n, random_state=seed)\n",
    "        X_train_transf = pca.fit_transform(X_train.copy())\n",
    "        X_test_transf = pca.transform(X_test.copy())\n",
    "        # print(X_train_transf.shape, X_test_transf.shape)\n",
    "\n",
    "        # Fit LR on top of new representation\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train_transf, y_train)\n",
    "\n",
    "        scores = lr.predict(X_test_transf)\n",
    "        eval_results[\"n\"].append(n)\n",
    "        eval_results[\"seed\"].append(n)\n",
    "        eval_results[\"mse\"].append(metrics.mean_squared_error(y_test, scores))\n",
    "        eval_results[\"r2\"].append(metrics.r2_score(y_test, scores))\n",
    "        eval_results[\"pearson\"].append(pearsonr(scores, y_test)[0])\n",
    "        eval_results[\"spearman\"].append(spearmanr(scores, y_test)[0])\n",
    "        \n",
    "        _pca[(n, seed)] = pca\n",
    "        _pca_models[(n, seed)] = lr\n",
    "        \n",
    "eval_results = pd.DataFrame(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67dec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=eval_results, x=\"n\", y=\"mse\")\n",
    "plt.xlabel(\"N components (PCA)\")\n",
    "plt.title(\"MSE of fit in function of number of PCA components on dev set\")\n",
    "plt.xlim(0, 18)\n",
    "plt.ylim(0.08, 0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07374176",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components, seed = 10, 1295532\n",
    "model = _pca_models[(n_components, seed)]\n",
    "\n",
    "# Unique datasets\n",
    "unique_datasets = list(train_df.dataset.unique())\n",
    "\n",
    "# Evaluation datasets\n",
    "# includes all_datasets (macro eval), as well as individual datasets\n",
    "dev_orig_datasets = {None: dev_df}\n",
    "dev_orig_datasets.update({dataset: dev_df[dev_df.dataset == dataset] for dataset in unique_datasets})\n",
    "\n",
    "results_by_dataset = {}\n",
    "for dataset_name in dev_orig_datasets.keys():\n",
    "    print(\"Fitting model using\", \"all\" if dataset_name is None else dataset_name, \"datasets\")\n",
    "    results, scores = eval_datasets_with_pca(\n",
    "        model=model,\n",
    "        pca=_pca[(n_components, seed)], \n",
    "        eval_datasets=dev_orig_datasets,\n",
    "    )\n",
    "    \n",
    "    results_by_dataset[dataset_name] = results \n",
    "\n",
    "parse_table_results(results_by_dataset, f\"dev_pca_{n_components}+lr_correlations\", METRICS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94488c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce dimension to 2 with PCA\n",
    "pca = {i: lambda: make_pipeline(StandardScaler(), PCA(n_components=i, random_state=SEED)) for i in range(2, 15)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
